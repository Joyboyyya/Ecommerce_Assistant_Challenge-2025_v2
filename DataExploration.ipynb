{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc47f6fa",
      "metadata": {
        "id": "bc47f6fa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4824ad0",
      "metadata": {
        "id": "e4824ad0"
      },
      "outputs": [],
      "source": [
        "product_df = pd.read_csv(\"data\\Order_Data_Dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8195bd6",
      "metadata": {
        "id": "c8195bd6",
        "outputId": "a07e14da-80aa-4be7-e773-ed6927d474eb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Order_Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>Aging</th>\n",
              "      <th>Customer_Id</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Device_Type</th>\n",
              "      <th>Customer_Login_type</th>\n",
              "      <th>Product_Category</th>\n",
              "      <th>Product</th>\n",
              "      <th>Sales</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Discount</th>\n",
              "      <th>Profit</th>\n",
              "      <th>Shipping_Cost</th>\n",
              "      <th>Order_Priority</th>\n",
              "      <th>Payment_method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>10:56:33</td>\n",
              "      <td>8.0</td>\n",
              "      <td>37077</td>\n",
              "      <td>Female</td>\n",
              "      <td>Web</td>\n",
              "      <td>Member</td>\n",
              "      <td>Auto &amp; Accessories</td>\n",
              "      <td>Car Media Players</td>\n",
              "      <td>140.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>46.0</td>\n",
              "      <td>4.6</td>\n",
              "      <td>Medium</td>\n",
              "      <td>credit_card</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-07-24</td>\n",
              "      <td>20:41:37</td>\n",
              "      <td>2.0</td>\n",
              "      <td>59173</td>\n",
              "      <td>Female</td>\n",
              "      <td>Web</td>\n",
              "      <td>Member</td>\n",
              "      <td>Auto &amp; Accessories</td>\n",
              "      <td>Car Speakers</td>\n",
              "      <td>211.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>112.0</td>\n",
              "      <td>11.2</td>\n",
              "      <td>Medium</td>\n",
              "      <td>credit_card</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-11-08</td>\n",
              "      <td>08:38:49</td>\n",
              "      <td>8.0</td>\n",
              "      <td>41066</td>\n",
              "      <td>Female</td>\n",
              "      <td>Web</td>\n",
              "      <td>Member</td>\n",
              "      <td>Auto &amp; Accessories</td>\n",
              "      <td>Car Body Covers</td>\n",
              "      <td>117.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>31.2</td>\n",
              "      <td>3.1</td>\n",
              "      <td>Critical</td>\n",
              "      <td>credit_card</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-04-18</td>\n",
              "      <td>19:28:06</td>\n",
              "      <td>7.0</td>\n",
              "      <td>50741</td>\n",
              "      <td>Female</td>\n",
              "      <td>Web</td>\n",
              "      <td>Member</td>\n",
              "      <td>Auto &amp; Accessories</td>\n",
              "      <td>Car &amp; Bike Care</td>\n",
              "      <td>118.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>26.2</td>\n",
              "      <td>2.6</td>\n",
              "      <td>High</td>\n",
              "      <td>credit_card</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-08-13</td>\n",
              "      <td>21:18:39</td>\n",
              "      <td>9.0</td>\n",
              "      <td>53639</td>\n",
              "      <td>Female</td>\n",
              "      <td>Web</td>\n",
              "      <td>Member</td>\n",
              "      <td>Auto &amp; Accessories</td>\n",
              "      <td>Tyre</td>\n",
              "      <td>250.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>160.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>Critical</td>\n",
              "      <td>credit_card</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Order_Date      Time  Aging  Customer_Id  Gender Device_Type  \\\n",
              "0  2018-01-02  10:56:33    8.0        37077  Female         Web   \n",
              "1  2018-07-24  20:41:37    2.0        59173  Female         Web   \n",
              "2  2018-11-08  08:38:49    8.0        41066  Female         Web   \n",
              "3  2018-04-18  19:28:06    7.0        50741  Female         Web   \n",
              "4  2018-08-13  21:18:39    9.0        53639  Female         Web   \n",
              "\n",
              "  Customer_Login_type    Product_Category            Product  Sales  Quantity  \\\n",
              "0              Member  Auto & Accessories  Car Media Players  140.0       1.0   \n",
              "1              Member  Auto & Accessories       Car Speakers  211.0       1.0   \n",
              "2              Member  Auto & Accessories    Car Body Covers  117.0       5.0   \n",
              "3              Member  Auto & Accessories    Car & Bike Care  118.0       1.0   \n",
              "4              Member  Auto & Accessories               Tyre  250.0       1.0   \n",
              "\n",
              "   Discount  Profit  Shipping_Cost Order_Priority Payment_method  \n",
              "0       0.3    46.0            4.6         Medium    credit_card  \n",
              "1       0.3   112.0           11.2         Medium    credit_card  \n",
              "2       0.1    31.2            3.1       Critical    credit_card  \n",
              "3       0.3    26.2            2.6           High    credit_card  \n",
              "4       0.3   160.0           16.0       Critical    credit_card  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "product_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e24c99",
      "metadata": {
        "id": "a9e24c99",
        "outputId": "19c9a978-31ec-481e-a44e-fe87ccfb7c22"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Aging</th>\n",
              "      <th>Customer_Id</th>\n",
              "      <th>Sales</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Discount</th>\n",
              "      <th>Profit</th>\n",
              "      <th>Shipping_Cost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>51289.000000</td>\n",
              "      <td>51290.000000</td>\n",
              "      <td>51289.000000</td>\n",
              "      <td>51288.000000</td>\n",
              "      <td>51289.000000</td>\n",
              "      <td>51290.000000</td>\n",
              "      <td>51289.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.255035</td>\n",
              "      <td>58155.758764</td>\n",
              "      <td>152.340872</td>\n",
              "      <td>2.502983</td>\n",
              "      <td>0.303821</td>\n",
              "      <td>70.407226</td>\n",
              "      <td>7.041557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.959948</td>\n",
              "      <td>26032.215826</td>\n",
              "      <td>66.495419</td>\n",
              "      <td>1.511859</td>\n",
              "      <td>0.131027</td>\n",
              "      <td>48.729488</td>\n",
              "      <td>4.871745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>35831.250000</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>24.900000</td>\n",
              "      <td>2.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>61018.000000</td>\n",
              "      <td>133.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>59.900000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.000000</td>\n",
              "      <td>80736.250000</td>\n",
              "      <td>218.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>118.400000</td>\n",
              "      <td>11.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>10.500000</td>\n",
              "      <td>99999.000000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>167.500000</td>\n",
              "      <td>16.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Aging   Customer_Id         Sales      Quantity      Discount  \\\n",
              "count  51289.000000  51290.000000  51289.000000  51288.000000  51289.000000   \n",
              "mean       5.255035  58155.758764    152.340872      2.502983      0.303821   \n",
              "std        2.959948  26032.215826     66.495419      1.511859      0.131027   \n",
              "min        1.000000  10000.000000     33.000000      1.000000      0.100000   \n",
              "25%        3.000000  35831.250000     85.000000      1.000000      0.200000   \n",
              "50%        5.000000  61018.000000    133.000000      2.000000      0.300000   \n",
              "75%        8.000000  80736.250000    218.000000      4.000000      0.400000   \n",
              "max       10.500000  99999.000000    250.000000      5.000000      0.500000   \n",
              "\n",
              "             Profit  Shipping_Cost  \n",
              "count  51290.000000   51289.000000  \n",
              "mean      70.407226       7.041557  \n",
              "std       48.729488       4.871745  \n",
              "min        0.500000       0.100000  \n",
              "25%       24.900000       2.500000  \n",
              "50%       59.900000       6.000000  \n",
              "75%      118.400000      11.800000  \n",
              "max      167.500000      16.800000  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "product_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1afa9434",
      "metadata": {
        "id": "1afa9434",
        "outputId": "9a5b3bf3-96f8-44e9-ac51-41a82c148937"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Order_Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>Aging</th>\n",
              "      <th>Customer_Id</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Device_Type</th>\n",
              "      <th>Customer_Login_type</th>\n",
              "      <th>Product_Category</th>\n",
              "      <th>Product</th>\n",
              "      <th>Sales</th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Discount</th>\n",
              "      <th>Profit</th>\n",
              "      <th>Shipping_Cost</th>\n",
              "      <th>Order_Priority</th>\n",
              "      <th>Payment_method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2018-05-02</td>\n",
              "      <td>11:45:38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26058</td>\n",
              "      <td>Female</td>\n",
              "      <td>Web</td>\n",
              "      <td>Member</td>\n",
              "      <td>Auto &amp; Accessories</td>\n",
              "      <td>Car Media Players</td>\n",
              "      <td>140.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>55.8</td>\n",
              "      <td>5.6</td>\n",
              "      <td>High</td>\n",
              "      <td>credit_card</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Order_Date      Time  Aging  Customer_Id  Gender Device_Type  \\\n",
              "27  2018-05-02  11:45:38    NaN        26058  Female         Web   \n",
              "\n",
              "   Customer_Login_type    Product_Category            Product  Sales  \\\n",
              "27              Member  Auto & Accessories  Car Media Players  140.0   \n",
              "\n",
              "    Quantity  Discount  Profit  Shipping_Cost Order_Priority Payment_method  \n",
              "27       1.0       0.3    55.8            5.6           High    credit_card  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lambda x: product_df[product_df['Aging'].isna()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a65fb1a4",
      "metadata": {
        "id": "a65fb1a4",
        "outputId": "08ac2f89-3e5f-43d9-b8d0-5a96e273f9be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Order Data Dataset Overview:\n",
            "Number of rows: 51290\n",
            "Number of columns: 16\n",
            "\n",
            "Column names: ['Order_Date', 'Time', 'Aging', 'Customer_Id', 'Gender', 'Device_Type', 'Customer_Login_type', 'Product_Category', 'Product', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping_Cost', 'Order_Priority', 'Payment_method']\n",
            "\n",
            "Sample row from Order Data Dataset:\n",
            "Order_Date                     2018-01-02\n",
            "Time                             10:56:33\n",
            "Aging                                 8.0\n",
            "Customer_Id                         37077\n",
            "Gender                             Female\n",
            "Device_Type                           Web\n",
            "Customer_Login_type                Member\n",
            "Product_Category       Auto & Accessories\n",
            "Product                 Car Media Players\n",
            "Sales                               140.0\n",
            "Quantity                              1.0\n",
            "Discount                              0.3\n",
            "Profit                               46.0\n",
            "Shipping_Cost                         4.6\n",
            "Order_Priority                     Medium\n",
            "Payment_method                credit_card\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\n",
            "Product Information Dataset Overview:\n",
            "Number of rows: 5000\n",
            "Number of columns: 11\n",
            "\n",
            "Column names: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'store', 'categories', 'details', 'parent_asin']\n",
            "\n",
            "Sample row from Product Information Dataset:\n",
            "main_category                                   Musical Instruments\n",
            "title             Ernie Ball Mondo Slinky Nickelwound Electric G...\n",
            "average_rating                                                  4.8\n",
            "rating_number                                                100615\n",
            "features          ['Ernie Ball Slinkys are played by legends aro...\n",
            "description       ['Product Description', 'Ernie Ball Mondo Slin...\n",
            "price                                                          6.99\n",
            "store                                                    Ernie Ball\n",
            "categories        ['Musical Instruments', 'Instrument Accessorie...\n",
            "details           {\"Item Weight\": \"1.09 ounces\", \"Product Dimens...\n",
            "parent_asin                                              B0BSGM6CQ9\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\n",
            "Order Data Statistics:\n",
            "              Sales  Shipping_Cost        Profit\n",
            "count  51289.000000   51289.000000  51290.000000\n",
            "mean     152.340872       7.041557     70.407226\n",
            "std       66.495419       4.871745     48.729488\n",
            "min       33.000000       0.100000      0.500000\n",
            "25%       85.000000       2.500000     24.900000\n",
            "50%      133.000000       6.000000     59.900000\n",
            "75%      218.000000      11.800000    118.400000\n",
            "max      250.000000      16.800000    167.500000\n",
            "\n",
            "\n",
            "Product Data Statistics:\n",
            "       average_rating  rating_number        price\n",
            "count     5000.000000    5000.000000  3734.000000\n",
            "mean         4.486500    1876.229800    56.508026\n",
            "std          0.245218    3899.338363    83.447626\n",
            "min          3.100000     547.000000     1.990000\n",
            "25%          4.400000     698.750000    13.082500\n",
            "50%          4.500000     963.000000    25.925000\n",
            "75%          4.700000    1532.000000    59.990000\n",
            "max          5.000000  100615.000000  1328.000000\n",
            "\n",
            "\n",
            "Missing values in Order Data:\n",
            "Order_Date             0\n",
            "Time                   0\n",
            "Aging                  1\n",
            "Customer_Id            0\n",
            "Gender                 0\n",
            "Device_Type            0\n",
            "Customer_Login_type    0\n",
            "Product_Category       0\n",
            "Product                0\n",
            "Sales                  1\n",
            "Quantity               2\n",
            "Discount               1\n",
            "Profit                 0\n",
            "Shipping_Cost          1\n",
            "Order_Priority         2\n",
            "Payment_method         0\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "Missing values in Product Data:\n",
            "main_category       30\n",
            "title                0\n",
            "average_rating       0\n",
            "rating_number        0\n",
            "features             0\n",
            "description          0\n",
            "price             1266\n",
            "store                1\n",
            "categories           0\n",
            "details              0\n",
            "parent_asin          0\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "Order Priority Distribution:\n",
            "Order_Priority\n",
            "Medium      29433\n",
            "High        15499\n",
            "Critical     3932\n",
            "Low          2424\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Product Category Distribution:\n",
            "main_category\n",
            "Musical Instruments          4277\n",
            "All Electronics               195\n",
            "Industrial & Scientific        93\n",
            "Tools & Home Improvement       68\n",
            "Amazon Home                    60\n",
            "Home Audio & Theater           57\n",
            "Toys & Games                   40\n",
            "Computers                      33\n",
            "Cell Phones & Accessories      31\n",
            "AMAZON FASHION                 28\n",
            "Name: count, dtype: int64\n",
            "\n",
            "\n",
            "Top 5 Highest Rated Products:\n",
            "SimpTronic Tech MaxMaxi Condenser Sound Recording Microphone + Mic Shock Mount + Cable + Sponge Set - Rating: 5.0, Reviews: 1117, Price: $nan\n",
            "SimpTronic Tech SHENGGU Lavalier Lapel Microphone Omnidirectional Condenser Lapel Mic TRS 3.5mm Locking Screw for Sennheiser - Rating: 5.0, Reviews: 992, Price: $nan\n",
            "SimpTronic Tech Estone 1 Set 6 Pack Angled Plug Leads Patch Cables for Guitar Pedal Effect Color Coded - Rating: 5.0, Reviews: 992, Price: $nan\n",
            "Audioblast - 2 Units - 30 Foot - HQ-1 - Ultra Flexible - Dual Shielded (100%) - Guitar Instrument Effects Pedal Patch Cable w/Neutrik-Rean NYS224BG Gold ¼ inch (6.35mm) TS Plugs & Triple Boots - Rating: 5.0, Reviews: 839, Price: $nan\n",
            "MIPRO ACT-311/ACT-30T Single Channel Bodypack Wireless System with Lavaliere Microphone - Rating: 5.0, Reviews: 839, Price: $nan\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Load the datasets\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Basic information about the datasets\n",
        "print(\"Order Data Dataset Overview:\")\n",
        "print(f\"Number of rows: {order_data.shape[0]}\")\n",
        "print(f\"Number of columns: {order_data.shape[1]}\")\n",
        "print(\"\\nColumn names:\", list(order_data.columns))\n",
        "print(\"\\nSample row from Order Data Dataset:\")\n",
        "print(order_data.iloc[0])\n",
        "\n",
        "print(\"\\n\\nProduct Information Dataset Overview:\")\n",
        "print(f\"Number of rows: {product_data.shape[0]}\")\n",
        "print(f\"Number of columns: {product_data.shape[1]}\")\n",
        "print(\"\\nColumn names:\", list(product_data.columns))\n",
        "print(\"\\nSample row from Product Information Dataset:\")\n",
        "print(product_data.iloc[0])\n",
        "\n",
        "# Basic statistics for order data\n",
        "print(\"\\n\\nOrder Data Statistics:\")\n",
        "print(order_data[['Sales', 'Shipping_Cost', 'Profit']].describe())\n",
        "\n",
        "# Basic statistics for product data\n",
        "print(\"\\n\\nProduct Data Statistics:\")\n",
        "print(product_data[['average_rating', 'rating_number', 'price']].describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n\\nMissing values in Order Data:\")\n",
        "print(order_data.isnull().sum())\n",
        "\n",
        "print(\"\\n\\nMissing values in Product Data:\")\n",
        "print(product_data.isnull().sum())\n",
        "\n",
        "# Distribution of order priorities\n",
        "print(\"\\n\\nOrder Priority Distribution:\")\n",
        "print(order_data['Order_Priority'].value_counts())\n",
        "\n",
        "# Distribution of product categories\n",
        "print(\"\\n\\nProduct Category Distribution:\")\n",
        "print(product_data['main_category'].value_counts().head(10))  # Top 10 categories\n",
        "\n",
        "# Check for highest rated products\n",
        "print(\"\\n\\nTop 5 Highest Rated Products:\")\n",
        "top_rated = product_data.sort_values(by=['average_rating', 'rating_number'], ascending=False).head(5)\n",
        "for idx, row in top_rated.iterrows():\n",
        "    print(f\"{row['title']} - Rating: {row['average_rating']}, Reviews: {row['rating_number']}, Price: ${row['price']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c62619",
      "metadata": {
        "id": "c7c62619",
        "outputId": "f6dc3abe-4000-40f5-b7af-62f966a4289a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Order Data Dataset Overview:\n",
            "Number of rows: 51290\n",
            "Number of columns: 16\n",
            "\n",
            "Column names: ['Order_Date', 'Time', 'Aging', 'Customer_Id', 'Gender', 'Device_Type', 'Customer_Login_type', 'Product_Category', 'Product', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping_Cost', 'Order_Priority', 'Payment_method']\n",
            "\n",
            "Sample row from Order Data Dataset:\n",
            "Order_Date                     2018-01-02\n",
            "Time                             10:56:33\n",
            "Aging                                 8.0\n",
            "Customer_Id                         37077\n",
            "Gender                             Female\n",
            "Device_Type                           Web\n",
            "Customer_Login_type                Member\n",
            "Product_Category       Auto & Accessories\n",
            "Product                 Car Media Players\n",
            "Sales                               140.0\n",
            "Quantity                              1.0\n",
            "Discount                              0.3\n",
            "Profit                               46.0\n",
            "Shipping_Cost                         4.6\n",
            "Order_Priority                     Medium\n",
            "Payment_method                credit_card\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\n",
            "Product Information Dataset Overview:\n",
            "Number of rows: 5000\n",
            "Number of columns: 11\n",
            "\n",
            "Column names: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'store', 'categories', 'details', 'parent_asin']\n",
            "\n",
            "Sample row from Product Information Dataset:\n",
            "main_category                                   Musical Instruments\n",
            "title             Ernie Ball Mondo Slinky Nickelwound Electric G...\n",
            "average_rating                                                  4.8\n",
            "rating_number                                                100615\n",
            "features          ['Ernie Ball Slinkys are played by legends aro...\n",
            "description       ['Product Description', 'Ernie Ball Mondo Slin...\n",
            "price                                                          6.99\n",
            "store                                                    Ernie Ball\n",
            "categories        ['Musical Instruments', 'Instrument Accessorie...\n",
            "details           {\"Item Weight\": \"1.09 ounces\", \"Product Dimens...\n",
            "parent_asin                                              B0BSGM6CQ9\n",
            "Name: 0, dtype: object\n",
            "\n",
            "Order Data Column Types:\n",
            "Order_Date              object\n",
            "Time                    object\n",
            "Aging                  float64\n",
            "Customer_Id              int64\n",
            "Gender                  object\n",
            "Device_Type             object\n",
            "Customer_Login_type     object\n",
            "Product_Category        object\n",
            "Product                 object\n",
            "Sales                  float64\n",
            "Quantity               float64\n",
            "Discount               float64\n",
            "Profit                 float64\n",
            "Shipping_Cost          float64\n",
            "Order_Priority          object\n",
            "Payment_method          object\n",
            "dtype: object\n",
            "\n",
            "Product Data Column Types:\n",
            "main_category      object\n",
            "title              object\n",
            "average_rating    float64\n",
            "rating_number       int64\n",
            "features           object\n",
            "description        object\n",
            "price             float64\n",
            "store              object\n",
            "categories         object\n",
            "details            object\n",
            "parent_asin        object\n",
            "dtype: object\n",
            "\n",
            "Order Data Numeric Column Statistics:\n",
            "              Aging   Customer_Id         Sales      Quantity      Discount  \\\n",
            "count  51289.000000  51290.000000  51289.000000  51288.000000  51289.000000   \n",
            "mean       5.255035  58155.758764    152.340872      2.502983      0.303821   \n",
            "std        2.959948  26032.215826     66.495419      1.511859      0.131027   \n",
            "min        1.000000  10000.000000     33.000000      1.000000      0.100000   \n",
            "25%        3.000000  35831.250000     85.000000      1.000000      0.200000   \n",
            "50%        5.000000  61018.000000    133.000000      2.000000      0.300000   \n",
            "75%        8.000000  80736.250000    218.000000      4.000000      0.400000   \n",
            "max       10.500000  99999.000000    250.000000      5.000000      0.500000   \n",
            "\n",
            "             Profit  Shipping_Cost  \n",
            "count  51290.000000   51289.000000  \n",
            "mean      70.407226       7.041557  \n",
            "std       48.729488       4.871745  \n",
            "min        0.500000       0.100000  \n",
            "25%       24.900000       2.500000  \n",
            "50%       59.900000       6.000000  \n",
            "75%      118.400000      11.800000  \n",
            "max      167.500000      16.800000  \n",
            "\n",
            "Product Data Numeric Column Statistics:\n",
            "       average_rating  rating_number        price\n",
            "count     5000.000000    5000.000000  3734.000000\n",
            "mean         4.486500    1876.229800    56.508026\n",
            "std          0.245218    3899.338363    83.447626\n",
            "min          3.100000     547.000000     1.990000\n",
            "25%          4.400000     698.750000    13.082500\n",
            "50%          4.500000     963.000000    25.925000\n",
            "75%          4.700000    1532.000000    59.990000\n",
            "max          5.000000  100615.000000  1328.000000\n",
            "\n",
            "Missing Values in Order Data:\n",
            "Order_Date             0\n",
            "Time                   0\n",
            "Aging                  1\n",
            "Customer_Id            0\n",
            "Gender                 0\n",
            "Device_Type            0\n",
            "Customer_Login_type    0\n",
            "Product_Category       0\n",
            "Product                0\n",
            "Sales                  1\n",
            "Quantity               2\n",
            "Discount               1\n",
            "Profit                 0\n",
            "Shipping_Cost          1\n",
            "Order_Priority         2\n",
            "Payment_method         0\n",
            "dtype: int64\n",
            "\n",
            "Missing Values in Product Data:\n",
            "main_category       30\n",
            "title                0\n",
            "average_rating       0\n",
            "rating_number        0\n",
            "features             0\n",
            "description          0\n",
            "price             1266\n",
            "store                1\n",
            "categories           0\n",
            "details              0\n",
            "parent_asin          0\n",
            "dtype: int64\n",
            "\n",
            "Order Data - Order Priority Distribution:\n",
            "Order_Priority\n",
            "Medium      29433\n",
            "High        15499\n",
            "Critical     3932\n",
            "Low          2424\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Order Data - Product Category Distribution:\n",
            "Product_Category\n",
            "Fashion               25646\n",
            "Home & Furniture      15438\n",
            "Auto & Accessories     7505\n",
            "Electronic             2701\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Order Data - Payment Method Distribution:\n",
            "Payment_method\n",
            "credit_card    38137\n",
            "money_order     9629\n",
            "e_wallet        2789\n",
            "debit_card       734\n",
            "not_defined        1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Product Data - Main Category Distribution:\n",
            "main_category\n",
            "Musical Instruments          4277\n",
            "All Electronics               195\n",
            "Industrial & Scientific        93\n",
            "Tools & Home Improvement       68\n",
            "Amazon Home                    60\n",
            "Home Audio & Theater           57\n",
            "Toys & Games                   40\n",
            "Computers                      33\n",
            "Cell Phones & Accessories      31\n",
            "AMAZON FASHION                 28\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Analysis of Sales:\n",
            "Min: 33.0\n",
            "Q1 (25th percentile): 85.0\n",
            "Median: 133.0\n",
            "Q3 (75th percentile): 218.0\n",
            "Max: 250.0\n",
            "Mean: 152.34087231180175\n",
            "Standard Deviation: 66.49541941735208\n",
            "IQR (Interquartile Range): 133.0\n",
            "Number of potential outliers: 0 (0.00%)\n",
            "No outliers\n",
            "\n",
            "Analysis of Quantity:\n",
            "Min: 1.0\n",
            "Q1 (25th percentile): 1.0\n",
            "Median: 2.0\n",
            "Q3 (75th percentile): 4.0\n",
            "Max: 5.0\n",
            "Mean: 2.5029831539541414\n",
            "Standard Deviation: 1.5118586336411266\n",
            "IQR (Interquartile Range): 3.0\n",
            "Number of potential outliers: 0 (0.00%)\n",
            "No outliers\n",
            "\n",
            "Analysis of Discount:\n",
            "Min: 0.1\n",
            "Q1 (25th percentile): 0.2\n",
            "Median: 0.3\n",
            "Q3 (75th percentile): 0.4\n",
            "Max: 0.5\n",
            "Mean: 0.3038214821891634\n",
            "Standard Deviation: 0.1310266110766322\n",
            "IQR (Interquartile Range): 0.2\n",
            "Number of potential outliers: 0 (0.00%)\n",
            "No outliers\n",
            "\n",
            "Analysis of Profit:\n",
            "Min: 0.5\n",
            "Q1 (25th percentile): 24.9\n",
            "Median: 59.9\n",
            "Q3 (75th percentile): 118.4\n",
            "Max: 167.5\n",
            "Mean: 70.4072255800351\n",
            "Standard Deviation: 48.729488299512695\n",
            "IQR (Interquartile Range): 93.5\n",
            "Number of potential outliers: 0 (0.00%)\n",
            "No outliers\n",
            "\n",
            "Analysis of Shipping_Cost:\n",
            "Min: 0.1\n",
            "Q1 (25th percentile): 2.5\n",
            "Median: 6.0\n",
            "Q3 (75th percentile): 11.8\n",
            "Max: 16.8\n",
            "Mean: 7.0415566690713405\n",
            "Standard Deviation: 4.871744672835268\n",
            "IQR (Interquartile Range): 9.3\n",
            "Number of potential outliers: 0 (0.00%)\n",
            "No outliers\n",
            "\n",
            "Analysis of average_rating:\n",
            "Min: 3.1\n",
            "Q1 (25th percentile): 4.4\n",
            "Median: 4.5\n",
            "Q3 (75th percentile): 4.7\n",
            "Max: 5.0\n",
            "Mean: 4.4865\n",
            "Standard Deviation: 0.24521781410670607\n",
            "IQR (Interquartile Range): 0.2999999999999998\n",
            "Number of potential outliers: 160 (3.20%)\n",
            "Range of outliers: 3.1 to 3.9\n",
            "\n",
            "Analysis of rating_number:\n",
            "Min: 547\n",
            "Q1 (25th percentile): 698.75\n",
            "Median: 963.0\n",
            "Q3 (75th percentile): 1532.0\n",
            "Max: 100615\n",
            "Mean: 1876.2298\n",
            "Standard Deviation: 3899.3383632260156\n",
            "IQR (Interquartile Range): 833.25\n",
            "Number of potential outliers: 500 (10.00%)\n",
            "Range of outliers: 3821 to 100615\n",
            "\n",
            "Analysis of price:\n",
            "Min: 1.99\n",
            "Q1 (25th percentile): 13.0825\n",
            "Median: 25.924999999999997\n",
            "Q3 (75th percentile): 59.99\n",
            "Max: 1328.0\n",
            "Mean: 56.508026245313346\n",
            "Standard Deviation: 83.44762567017831\n",
            "IQR (Interquartile Range): 46.9075\n",
            "Number of potential outliers: 416 (11.14%)\n",
            "Range of outliers: 130.53 to 1328.0\n",
            "\n",
            "Order Data - Order Date Analysis:\n",
            "Date range: 2018-01-01 00:00:00 to 2018-12-30 00:00:00\n",
            "Orders by month:\n",
            "Order_Date\n",
            "2018-01    2519\n",
            "2018-02    2206\n",
            "2018-03    2899\n",
            "2018-04    3896\n",
            "2018-05    5417\n",
            "2018-06    4179\n",
            "2018-07    5321\n",
            "2018-08    4373\n",
            "2018-09    4851\n",
            "2018-10    4886\n",
            "2018-11    5679\n",
            "2018-12    5064\n",
            "Freq: M, Name: count, dtype: int64\n",
            "\n",
            "Top 5 Highest Rated Products (by average rating):\n",
            "SimpTronic Tech MaxMaxi Condenser Sound Recording Microphone + Mic Shock Mount + Cable + Sponge Set - Rating: 5.0, Reviews: 1117, Price: $nan\n",
            "SimpTronic Tech SHENGGU Lavalier Lapel Microphone Omnidirectional Condenser Lapel Mic TRS 3.5mm Locking Screw for Sennheiser - Rating: 5.0, Reviews: 992, Price: $nan\n",
            "SimpTronic Tech Estone 1 Set 6 Pack Angled Plug Leads Patch Cables for Guitar Pedal Effect Color Coded - Rating: 5.0, Reviews: 992, Price: $nan\n",
            "Audioblast - 2 Units - 30 Foot - HQ-1 - Ultra Flexible - Dual Shielded (100%) - Guitar Instrument Effects Pedal Patch Cable w/Neutrik-Rean NYS224BG Gold ¼ inch (6.35mm) TS Plugs & Triple Boots - Rating: 5.0, Reviews: 839, Price: $nan\n",
            "MIPRO ACT-311/ACT-30T Single Channel Bodypack Wireless System with Lavaliere Microphone - Rating: 5.0, Reviews: 839, Price: $nan\n",
            "\n",
            "Most Profitable Product Categories:\n",
            "Fashion: $2072623.9\n",
            "Home & Furniture: $880058.9\n",
            "Auto & Accessories: $484313.2\n",
            "Electronic: $174190.6\n",
            "\n",
            "Customer Analysis:\n",
            "Total unique customers: 38997\n",
            "\n",
            "Orders by Gender:\n",
            "Gender\n",
            "Male      28138\n",
            "Female    23152\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Orders by Device Type:\n",
            "Device_Type\n",
            "Web       47632\n",
            "Mobile     3658\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Orders by Login Type:\n",
            "Customer_Login_type\n",
            "Member          49097\n",
            "Guest            1993\n",
            "First SignUp      173\n",
            "New                27\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample Query 1: Customer ID 37077's last order:\n",
            "Order Date: 2018-01-02 00:00:00\n",
            "Product: Car Media Players\n",
            "Sales Amount: $140.0\n",
            "Shipping Cost: $4.6\n",
            "Order Priority: Medium\n",
            "\n",
            "Sample Query 2: 5 most recent Critical priority orders:\n",
            "8377. Date: 2018-12-30 00:00:00\n",
            "   Customer ID: 39758\n",
            "   Product: Suits\n",
            "   Sales: $109.0\n",
            "   Shipping Cost: $2.4\n",
            "\n",
            "11103. Date: 2018-12-30 00:00:00\n",
            "   Customer ID: 34789\n",
            "   Product: Shirts\n",
            "   Sales: $196.0\n",
            "   Shipping Cost: $9.6\n",
            "\n",
            "18001. Date: 2018-12-30 00:00:00\n",
            "   Customer ID: 59098\n",
            "   Product: LED\n",
            "   Sales: $192.0\n",
            "   Shipping Cost: $9.5\n",
            "\n",
            "20024. Date: 2018-12-30 00:00:00\n",
            "   Customer ID: 48215\n",
            "   Product: Samsung Mobile\n",
            "   Sales: $220.0\n",
            "   Shipping Cost: $13.3\n",
            "\n",
            "2039. Date: 2018-12-30 00:00:00\n",
            "   Customer ID: 26306\n",
            "   Product: Tyre\n",
            "   Sales: $250.0\n",
            "   Shipping Cost: $13.3\n",
            "\n",
            "\n",
            "Sample Query 3: Guitar products analysis:\n",
            "Total guitar products: 1731\n",
            "Average guitar product rating: 4.531369150779896\n",
            "Average guitar product price: $44.538390541571324\n",
            "\n",
            "Top 5 highest rated guitar products:\n",
            "- SimpTronic Tech Estone 1 Set 6 Pack Angled Plug Leads Patch Cables for Guitar Pedal Effect Color Coded\n",
            "  Rating: 5.0 stars (992 reviews)\n",
            "  Price: $nan\n",
            "\n",
            "- Audioblast - 2 Units - 30 Foot - HQ-1 - Ultra Flexible - Dual Shielded (100%) - Guitar Instrument Effects Pedal Patch Cable w/Neutrik-Rean NYS224BG Gold ¼ inch (6.35mm) TS Plugs & Triple Boots\n",
            "  Rating: 5.0 stars (839 reviews)\n",
            "  Price: $nan\n",
            "\n",
            "- Jim Dunlop Tortex Standard 1.14mm Purple Guitar Picks-36 Pack (418B1.14)\n",
            "  Rating: 4.9 stars (4367 reviews)\n",
            "  Price: $16.47\n",
            "\n",
            "- Buckle-Down Guitar Strap - MARVEL/Retro Comic Panels Black/Yellow - 2\" Wide - 29-54\" Length\n",
            "  Rating: 4.9 stars (1594 reviews)\n",
            "  Price: $24.95\n",
            "\n",
            "- String Swing Guitar Hanger Holder for Electric Acoustic and Bass Guitars Stand Accessories Home or Studio Wall - Musical Instruments Safe without Hard Cases Oak Hardwood CC01K-O 2-Pack\n",
            "  Rating: 4.9 stars (882 reviews)\n",
            "  Price: $28.98\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_15904\\4199805226.py:135: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  customer_orders['Order_Date'] = pd.to_datetime(customer_orders['Order_Date'])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the datasets\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Basic data exploration without visualizations\n",
        "print(\"Order Data Dataset Overview:\")\n",
        "print(f\"Number of rows: {order_data.shape[0]}\")\n",
        "print(f\"Number of columns: {order_data.shape[1]}\")\n",
        "print(\"\\nColumn names:\", list(order_data.columns))\n",
        "print(\"\\nSample row from Order Data Dataset:\")\n",
        "print(order_data.iloc[0])\n",
        "\n",
        "print(\"\\n\\nProduct Information Dataset Overview:\")\n",
        "print(f\"Number of rows: {product_data.shape[0]}\")\n",
        "print(f\"Number of columns: {product_data.shape[1]}\")\n",
        "print(\"\\nColumn names:\", list(product_data.columns))\n",
        "print(\"\\nSample row from Product Information Dataset:\")\n",
        "print(product_data.iloc[0])\n",
        "\n",
        "# Check datatypes of columns in Order Data\n",
        "print(\"\\nOrder Data Column Types:\")\n",
        "print(order_data.dtypes)\n",
        "\n",
        "# Check datatypes of columns in Product Data\n",
        "print(\"\\nProduct Data Column Types:\")\n",
        "print(product_data.dtypes)\n",
        "\n",
        "# Order Data statistics\n",
        "print(\"\\nOrder Data Numeric Column Statistics:\")\n",
        "print(order_data.describe())\n",
        "\n",
        "# Product Data statistics\n",
        "print(\"\\nProduct Data Numeric Column Statistics:\")\n",
        "print(product_data.describe())\n",
        "\n",
        "# Check for missing values in Order Data\n",
        "print(\"\\nMissing Values in Order Data:\")\n",
        "print(order_data.isnull().sum())\n",
        "\n",
        "# Check for missing values in Product Data\n",
        "print(\"\\nMissing Values in Product Data:\")\n",
        "print(product_data.isnull().sum())\n",
        "\n",
        "# Distribution of categorical columns in Order Data\n",
        "print(\"\\nOrder Data - Order Priority Distribution:\")\n",
        "print(order_data['Order_Priority'].value_counts())\n",
        "\n",
        "print(\"\\nOrder Data - Product Category Distribution:\")\n",
        "print(order_data['Product_Category'].value_counts().head(10))  # Show top 10\n",
        "\n",
        "print(\"\\nOrder Data - Payment Method Distribution:\")\n",
        "print(order_data['Payment_method'].value_counts())\n",
        "\n",
        "# Distribution of categorical columns in Product Data\n",
        "print(\"\\nProduct Data - Main Category Distribution:\")\n",
        "print(product_data['main_category'].value_counts().head(10))  # Show top 10\n",
        "\n",
        "# For numeric columns, calculate basic statistics including quartiles\n",
        "def analyze_numeric_column(dataframe, column_name):\n",
        "    if column_name in dataframe.columns and pd.api.types.is_numeric_dtype(dataframe[column_name]):\n",
        "        data = dataframe[column_name].dropna()\n",
        "        q1 = data.quantile(0.25)\n",
        "        q2 = data.quantile(0.50)  # median\n",
        "        q3 = data.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "\n",
        "        print(f\"\\nAnalysis of {column_name}:\")\n",
        "        print(f\"Min: {data.min()}\")\n",
        "        print(f\"Q1 (25th percentile): {q1}\")\n",
        "        print(f\"Median: {q2}\")\n",
        "        print(f\"Q3 (75th percentile): {q3}\")\n",
        "        print(f\"Max: {data.max()}\")\n",
        "        print(f\"Mean: {data.mean()}\")\n",
        "        print(f\"Standard Deviation: {data.std()}\")\n",
        "        print(f\"IQR (Interquartile Range): {iqr}\")\n",
        "\n",
        "        # Calculate potential outliers\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "\n",
        "        print(f\"Number of potential outliers: {len(outliers)} ({len(outliers)/len(data)*100:.2f}%)\")\n",
        "        print(f\"Range of outliers: {outliers.min()} to {outliers.max()}\" if len(outliers) > 0 else \"No outliers\")\n",
        "\n",
        "# Analyze key numeric columns in Order Data\n",
        "important_order_columns = ['Sales', 'Quantity', 'Discount', 'Profit', 'Shipping_Cost']\n",
        "for column in important_order_columns:\n",
        "    analyze_numeric_column(order_data, column)\n",
        "\n",
        "# Analyze key numeric columns in Product Data\n",
        "important_product_columns = ['average_rating', 'rating_number', 'price']\n",
        "for column in important_product_columns:\n",
        "    analyze_numeric_column(product_data, column)\n",
        "\n",
        "# Analyze date columns in Order Data\n",
        "print(\"\\nOrder Data - Order Date Analysis:\")\n",
        "if 'Order_Date' in order_data.columns:\n",
        "    order_data['Order_Date'] = pd.to_datetime(order_data['Order_Date'])\n",
        "    print(f\"Date range: {order_data['Order_Date'].min()} to {order_data['Order_Date'].max()}\")\n",
        "    print(\"Orders by month:\")\n",
        "    print(order_data['Order_Date'].dt.to_period('M').value_counts().sort_index())\n",
        "\n",
        "# Find top rated products\n",
        "print(\"\\nTop 5 Highest Rated Products (by average rating):\")\n",
        "top_rated = product_data.sort_values(by=['average_rating', 'rating_number'], ascending=[False, False]).head(5)\n",
        "for idx, row in top_rated.iterrows():\n",
        "    print(f\"{row['title']} - Rating: {row['average_rating']}, Reviews: {row['rating_number']}, Price: ${row['price']}\")\n",
        "\n",
        "# Find most profitable product categories\n",
        "print(\"\\nMost Profitable Product Categories:\")\n",
        "category_profit = order_data.groupby('Product_Category')['Profit'].sum().sort_values(ascending=False)\n",
        "for category, profit in category_profit.head(10).items():\n",
        "    print(f\"{category}: ${profit}\")\n",
        "\n",
        "# Customer analysis\n",
        "print(\"\\nCustomer Analysis:\")\n",
        "print(f\"Total unique customers: {order_data['Customer_Id'].nunique()}\")\n",
        "print(\"\\nOrders by Gender:\")\n",
        "print(order_data['Gender'].value_counts())\n",
        "print(\"\\nOrders by Device Type:\")\n",
        "print(order_data['Device_Type'].value_counts())\n",
        "print(\"\\nOrders by Login Type:\")\n",
        "print(order_data['Customer_Login_type'].value_counts())\n",
        "\n",
        "# Sample queries from the challenge\n",
        "print(\"\\nSample Query 1: Customer ID 37077's last order:\")\n",
        "customer_id = 37077\n",
        "customer_orders = order_data[order_data['Customer_Id'] == customer_id]\n",
        "if not customer_orders.empty:\n",
        "    # Convert to datetime and sort\n",
        "    customer_orders['Order_Date'] = pd.to_datetime(customer_orders['Order_Date'])\n",
        "    last_order = customer_orders.sort_values('Order_Date', ascending=False).iloc[0]\n",
        "    print(f\"Order Date: {last_order['Order_Date']}\")\n",
        "    print(f\"Product: {last_order['Product']}\")\n",
        "    print(f\"Sales Amount: ${last_order['Sales']}\")\n",
        "    print(f\"Shipping Cost: ${last_order['Shipping_Cost']}\")\n",
        "    print(f\"Order Priority: {last_order['Order_Priority']}\")\n",
        "else:\n",
        "    print(f\"No orders found for customer {customer_id}\")\n",
        "\n",
        "print(\"\\nSample Query 2: 5 most recent Critical priority orders:\")\n",
        "if 'Order_Priority' in order_data.columns and 'Order_Date' in order_data.columns:\n",
        "    # Convert to datetime\n",
        "    order_data['Order_Date'] = pd.to_datetime(order_data['Order_Date'])\n",
        "    critical_orders = order_data[order_data['Order_Priority'] == 'Critical']\n",
        "    recent_critical = critical_orders.sort_values('Order_Date', ascending=False).head(5)\n",
        "\n",
        "    for idx, order in recent_critical.iterrows():\n",
        "        print(f\"{idx + 1}. Date: {order['Order_Date']}\")\n",
        "        print(f\"   Customer ID: {order['Customer_Id']}\")\n",
        "        print(f\"   Product: {order['Product']}\")\n",
        "        print(f\"   Sales: ${order['Sales']}\")\n",
        "        print(f\"   Shipping Cost: ${order['Shipping_Cost']}\")\n",
        "        print()\n",
        "\n",
        "print(\"\\nSample Query 3: Guitar products analysis:\")\n",
        "guitar_products = product_data[\n",
        "    (product_data['title'].str.contains('guitar', case=False, na=False)) |\n",
        "    (product_data['categories'].str.contains('guitar', case=False, na=False))\n",
        "]\n",
        "print(f\"Total guitar products: {len(guitar_products)}\")\n",
        "print(f\"Average guitar product rating: {guitar_products['average_rating'].mean()}\")\n",
        "print(f\"Average guitar product price: ${guitar_products['price'].mean()}\")\n",
        "print(\"\\nTop 5 highest rated guitar products:\")\n",
        "top_guitars = guitar_products.sort_values(by=['average_rating', 'rating_number'], ascending=[False, False]).head(5)\n",
        "for idx, row in top_guitars.iterrows():\n",
        "    print(f\"- {row['title']}\")\n",
        "    print(f\"  Rating: {row['average_rating']} stars ({row['rating_number']} reviews)\")\n",
        "    print(f\"  Price: ${row['price']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a2e723",
      "metadata": {
        "id": "64a2e723",
        "outputId": "48069be0-473e-438e-bf86-6a6ef08226f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Order Data Dataset Overview:\n",
            "Number of rows: 51290\n",
            "Number of columns: 16\n",
            "\n",
            "Column names: ['Order_Date', 'Time', 'Aging', 'Customer_Id', 'Gender', 'Device_Type', 'Customer_Login_type', 'Product_Category', 'Product', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping_Cost', 'Order_Priority', 'Payment_method']\n",
            "\n",
            "Sample row from Order Data Dataset:\n",
            "Order_Date                     2018-01-02\n",
            "Time                             10:56:33\n",
            "Aging                                 8.0\n",
            "Customer_Id                         37077\n",
            "Gender                             Female\n",
            "Device_Type                           Web\n",
            "Customer_Login_type                Member\n",
            "Product_Category       Auto & Accessories\n",
            "Product                 Car Media Players\n",
            "Sales                               140.0\n",
            "Quantity                              1.0\n",
            "Discount                              0.3\n",
            "Profit                               46.0\n",
            "Shipping_Cost                         4.6\n",
            "Order_Priority                     Medium\n",
            "Payment_method                credit_card\n",
            "Name: 0, dtype: object\n",
            "\n",
            "\n",
            "Product Information Dataset Overview:\n",
            "Number of rows: 5000\n",
            "Number of columns: 11\n",
            "\n",
            "Column names: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'store', 'categories', 'details', 'parent_asin']\n",
            "\n",
            "Sample row from Product Information Dataset:\n",
            "main_category                                   Musical Instruments\n",
            "title             Ernie Ball Mondo Slinky Nickelwound Electric G...\n",
            "average_rating                                                  4.8\n",
            "rating_number                                                100615\n",
            "features          ['Ernie Ball Slinkys are played by legends aro...\n",
            "description       ['Product Description', 'Ernie Ball Mondo Slin...\n",
            "price                                                          6.99\n",
            "store                                                    Ernie Ball\n",
            "categories        ['Musical Instruments', 'Instrument Accessorie...\n",
            "details           {\"Item Weight\": \"1.09 ounces\", \"Product Dimens...\n",
            "parent_asin                                              B0BSGM6CQ9\n",
            "Name: 0, dtype: object\n",
            "\n",
            "Order Data Column Types:\n",
            "Order_Date              object\n",
            "Time                    object\n",
            "Aging                  float64\n",
            "Customer_Id              int64\n",
            "Gender                  object\n",
            "Device_Type             object\n",
            "Customer_Login_type     object\n",
            "Product_Category        object\n",
            "Product                 object\n",
            "Sales                  float64\n",
            "Quantity               float64\n",
            "Discount               float64\n",
            "Profit                 float64\n",
            "Shipping_Cost          float64\n",
            "Order_Priority          object\n",
            "Payment_method          object\n",
            "dtype: object\n",
            "\n",
            "Product Data Column Types:\n",
            "main_category      object\n",
            "title              object\n",
            "average_rating    float64\n",
            "rating_number       int64\n",
            "features           object\n",
            "description        object\n",
            "price             float64\n",
            "store              object\n",
            "categories         object\n",
            "details            object\n",
            "parent_asin        object\n",
            "dtype: object\n",
            "\n",
            "Order Data Numeric Column Statistics:\n",
            "              Aging   Customer_Id         Sales      Quantity      Discount  \\\n",
            "count  51289.000000  51290.000000  51289.000000  51288.000000  51289.000000   \n",
            "mean       5.255035  58155.758764    152.340872      2.502983      0.303821   \n",
            "std        2.959948  26032.215826     66.495419      1.511859      0.131027   \n",
            "min        1.000000  10000.000000     33.000000      1.000000      0.100000   \n",
            "25%        3.000000  35831.250000     85.000000      1.000000      0.200000   \n",
            "50%        5.000000  61018.000000    133.000000      2.000000      0.300000   \n",
            "75%        8.000000  80736.250000    218.000000      4.000000      0.400000   \n",
            "max       10.500000  99999.000000    250.000000      5.000000      0.500000   \n",
            "\n",
            "             Profit  Shipping_Cost  \n",
            "count  51290.000000   51289.000000  \n",
            "mean      70.407226       7.041557  \n",
            "std       48.729488       4.871745  \n",
            "min        0.500000       0.100000  \n",
            "25%       24.900000       2.500000  \n",
            "50%       59.900000       6.000000  \n",
            "75%      118.400000      11.800000  \n",
            "max      167.500000      16.800000  \n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name '_check_savefig_extra_args' from 'matplotlib.backend_bases' (c:\\Users\\goldr\\anaconda3\\envs\\myenv\\lib\\site-packages\\matplotlib\\backend_bases.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m numeric_order_cols \u001b[38;5;241m=\u001b[39m order_data\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m numeric_order_cols:\n\u001b[1;32m---> 49\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Handle outliers for better visualization\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     q1 \u001b[38;5;241m=\u001b[39m order_data[col]\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.25\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\goldr\\anaconda3\\envs\\myenv\\lib\\site-packages\\matplotlib\\pyplot.py:1027\u001b[0m, in \u001b[0;36mfigure\u001b[1;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "File \u001b[1;32mc:\\Users\\goldr\\anaconda3\\envs\\myenv\\lib\\site-packages\\matplotlib\\pyplot.py:549\u001b[0m, in \u001b[0;36mnew_figure_manager\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m         _api\u001b[38;5;241m.\u001b[39mwarn_external(\n\u001b[0;32m    544\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting a Matplotlib GUI outside of the main thread will likely \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfail.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# This function's signature is rewritten upon backend-load by switch_backend.\u001b[39;00m\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_figure_manager\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a new figure manager instance.\"\"\"\u001b[39;00m\n\u001b[0;32m    551\u001b[0m     _warn_if_gui_out_of_main_thread()\n",
            "File \u001b[1;32mc:\\Users\\goldr\\anaconda3\\envs\\myenv\\lib\\site-packages\\matplotlib\\pyplot.py:526\u001b[0m, in \u001b[0;36m_warn_if_gui_out_of_main_thread\u001b[1;34m()\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;66;03m# Make sure the repl display hook is installed in case we become interactive.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m     install_repl_displayhook()\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_warn_if_gui_out_of_main_thread\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    527\u001b[0m     warn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    528\u001b[0m     canvas_class \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mtype\u001b[39m[FigureCanvasBase], _get_backend_mod()\u001b[38;5;241m.\u001b[39mFigureCanvas)\n",
            "File \u001b[1;32mc:\\Users\\goldr\\anaconda3\\envs\\myenv\\lib\\site-packages\\matplotlib\\pyplot.py:358\u001b[0m, in \u001b[0;36m_get_backend_mod\u001b[1;34m()\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
            "File \u001b[1;32mc:\\Users\\goldr\\anaconda3\\envs\\myenv\\lib\\site-packages\\matplotlib\\pyplot.py:415\u001b[0m, in \u001b[0;36mswitch_backend\u001b[1;34m(newbackend)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 415\u001b[0m         rcParamsOrig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m candidate\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;66;03m# Switching to Agg should always succeed; if it doesn't, let the\u001b[39;00m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;66;03m# exception propagate out.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\goldr\\anaconda3\\envs\\myenv\\lib\\site-packages\\matplotlib\\backends\\registry.py:323\u001b[0m, in \u001b[0;36mload_backend_module\u001b[1;34m(self, backend)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mresolve_backend\u001b[39m(\u001b[38;5;28mself\u001b[39m, backend):\n\u001b[0;32m    320\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    Return the backend and GUI framework for the specified backend name.\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \n\u001b[1;32m--> 323\u001b[0m \u001b[38;5;124;03m    If the GUI framework is not yet known then it will be determined by loading the\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m    backend module and checking the ``FigureCanvas.required_interactive_framework``\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    attribute.\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    This function only loads entry points if they have not already been loaded and\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    the backend is not built-in and not of ``module://some.backend`` format.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    backend : str or None\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m        Name of backend, or None to use the default backend.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    backend : str\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m        The backend name.\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    framework : str or None\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m        The GUI framework, which will be None for a backend that is non-interactive.\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(backend, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\goldr\\anaconda3\\envs\\myenv\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\matplotlib_inline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_inline, config  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m      2\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\matplotlib_inline\\backend_inline.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m colors\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_agg\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_agg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasAgg\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pylab_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Gcf\n",
            "File \u001b[1;32mc:\\Users\\goldr\\anaconda3\\envs\\myenv\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m colors \u001b[38;5;28;01mas\u001b[39;00m mcolors\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     38\u001b[0m     _Backend, _check_savefig_extra_args, FigureCanvasBase, FigureManagerBase,\n\u001b[0;32m     39\u001b[0m     RendererBase)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfont_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m findfont, get_font\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mft2font\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (LOAD_FORCE_AUTOHINT, LOAD_NO_HINTING,\n\u001b[0;32m     42\u001b[0m                                 LOAD_DEFAULT, LOAD_NO_AUTOHINT)\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name '_check_savefig_extra_args' from 'matplotlib.backend_bases' (c:\\Users\\goldr\\anaconda3\\envs\\myenv\\lib\\site-packages\\matplotlib\\backend_bases.py)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set the style for prettier plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Create directory for visualizations\n",
        "os.makedirs('visualizations', exist_ok=True)\n",
        "\n",
        "# Load the datasets\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Basic data exploration with visualizations\n",
        "print(\"Order Data Dataset Overview:\")\n",
        "print(f\"Number of rows: {order_data.shape[0]}\")\n",
        "print(f\"Number of columns: {order_data.shape[1]}\")\n",
        "print(\"\\nColumn names:\", list(order_data.columns))\n",
        "print(\"\\nSample row from Order Data Dataset:\")\n",
        "print(order_data.iloc[0])\n",
        "\n",
        "print(\"\\n\\nProduct Information Dataset Overview:\")\n",
        "print(f\"Number of rows: {product_data.shape[0]}\")\n",
        "print(f\"Number of columns: {product_data.shape[1]}\")\n",
        "print(\"\\nColumn names:\", list(product_data.columns))\n",
        "print(\"\\nSample row from Product Information Dataset:\")\n",
        "print(product_data.iloc[0])\n",
        "\n",
        "# Check datatypes of columns in Order Data\n",
        "print(\"\\nOrder Data Column Types:\")\n",
        "print(order_data.dtypes)\n",
        "\n",
        "# Check datatypes of columns in Product Data\n",
        "print(\"\\nProduct Data Column Types:\")\n",
        "print(product_data.dtypes)\n",
        "\n",
        "# Order Data statistics\n",
        "print(\"\\nOrder Data Numeric Column Statistics:\")\n",
        "print(order_data.describe())\n",
        "\n",
        "# Visualize numeric columns in order data\n",
        "numeric_order_cols = order_data.select_dtypes(include=['int64', 'float64']).columns\n",
        "for col in numeric_order_cols:\n",
        "    plt.figure()\n",
        "    # Handle outliers for better visualization\n",
        "    q1 = order_data[col].quantile(0.25)\n",
        "    q3 = order_data[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    filtered_data = order_data[(order_data[col] >= lower_bound) & (order_data[col] <= upper_bound)]\n",
        "\n",
        "    sns.histplot(filtered_data[col], kde=True)\n",
        "    plt.title(f'Distribution of {col} (Outliers Removed)')\n",
        "    plt.savefig(f'visualizations/order_{col}_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Box plot\n",
        "    plt.figure()\n",
        "    sns.boxplot(y=order_data[col])\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "    plt.savefig(f'visualizations/order_{col}_boxplot.png')\n",
        "    plt.close()\n",
        "\n",
        "# Product Data statistics\n",
        "print(\"\\nProduct Data Numeric Column Statistics:\")\n",
        "print(product_data.describe())\n",
        "\n",
        "# Visualize numeric columns in product data\n",
        "numeric_product_cols = product_data.select_dtypes(include=['int64', 'float64']).columns\n",
        "for col in numeric_product_cols:\n",
        "    plt.figure()\n",
        "    # Handle outliers for better visualization\n",
        "    q1 = product_data[col].quantile(0.25)\n",
        "    q3 = product_data[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    filtered_data = product_data[(product_data[col] >= lower_bound) & (product_data[col] <= upper_bound)]\n",
        "\n",
        "    sns.histplot(filtered_data[col], kde=True)\n",
        "    plt.title(f'Distribution of {col} (Outliers Removed)')\n",
        "    plt.savefig(f'visualizations/product_{col}_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Box plot\n",
        "    plt.figure()\n",
        "    sns.boxplot(y=product_data[col])\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "    plt.savefig(f'visualizations/product_{col}_boxplot.png')\n",
        "    plt.close()\n",
        "\n",
        "# Check for missing values in Order Data\n",
        "print(\"\\nMissing Values in Order Data:\")\n",
        "missing_order = order_data.isnull().sum()\n",
        "print(missing_order)\n",
        "\n",
        "# Visualize missing values in Order Data\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(order_data.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
        "plt.title('Missing Values in Order Data')\n",
        "plt.savefig('visualizations/order_data_missing_values.png')\n",
        "plt.close()\n",
        "\n",
        "# Check for missing values in Product Data\n",
        "print(\"\\nMissing Values in Product Data:\")\n",
        "missing_product = product_data.isnull().sum()\n",
        "print(missing_product)\n",
        "\n",
        "# Visualize missing values in Product Data\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(product_data.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
        "plt.title('Missing Values in Product Data')\n",
        "plt.savefig('visualizations/product_data_missing_values.png')\n",
        "plt.close()\n",
        "\n",
        "# Distribution of categorical columns in Order Data\n",
        "print(\"\\nOrder Data - Order Priority Distribution:\")\n",
        "order_priority = order_data['Order_Priority'].value_counts()\n",
        "print(order_priority)\n",
        "\n",
        "# Visualize Order Priority\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=order_priority.index, y=order_priority.values)\n",
        "plt.title('Distribution of Order Priorities')\n",
        "plt.savefig('visualizations/order_priority_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Create a pie chart for Order Priority\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.pie(order_priority.values, labels=order_priority.index, autopct='%1.1f%%', startangle=90, shadow=True)\n",
        "plt.title('Order Priority Distribution')\n",
        "plt.axis('equal')\n",
        "plt.savefig('visualizations/order_priority_pie.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nOrder Data - Product Category Distribution:\")\n",
        "product_category_order = order_data['Product_Category'].value_counts().head(10)\n",
        "print(product_category_order)\n",
        "\n",
        "# Visualize Product Category in Order Data\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(x=product_category_order.values, y=product_category_order.index)\n",
        "plt.title('Top 10 Product Categories by Order Count')\n",
        "plt.xlabel('Number of Orders')\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/order_product_category_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nOrder Data - Payment Method Distribution:\")\n",
        "payment_method = order_data['Payment_method'].value_counts()\n",
        "print(payment_method)\n",
        "\n",
        "# Visualize Payment Method\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=payment_method.index, y=payment_method.values)\n",
        "plt.title('Distribution of Payment Methods')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/payment_method_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# Distribution of categorical columns in Product Data\n",
        "print(\"\\nProduct Data - Main Category Distribution:\")\n",
        "main_category = product_data['main_category'].value_counts().head(10)\n",
        "print(main_category)\n",
        "\n",
        "# Visualize Main Category\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.barplot(x=main_category.values, y=main_category.index)\n",
        "plt.title('Top 10 Main Categories in Product Data')\n",
        "plt.xlabel('Number of Products')\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/product_main_category_distribution.png')\n",
        "plt.close()\n",
        "\n",
        "# For numeric columns, calculate basic statistics including quartiles\n",
        "def analyze_numeric_column(dataframe, column_name):\n",
        "    if column_name in dataframe.columns and pd.api.types.is_numeric_dtype(dataframe[column_name]):\n",
        "        data = dataframe[column_name].dropna()\n",
        "        q1 = data.quantile(0.25)\n",
        "        q2 = data.quantile(0.50)  # median\n",
        "        q3 = data.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "\n",
        "        print(f\"\\nAnalysis of {column_name}:\")\n",
        "        print(f\"Min: {data.min()}\")\n",
        "        print(f\"Q1 (25th percentile): {q1}\")\n",
        "        print(f\"Median: {q2}\")\n",
        "        print(f\"Q3 (75th percentile): {q3}\")\n",
        "        print(f\"Max: {data.max()}\")\n",
        "        print(f\"Mean: {data.mean()}\")\n",
        "        print(f\"Standard Deviation: {data.std()}\")\n",
        "        print(f\"IQR (Interquartile Range): {iqr}\")\n",
        "\n",
        "        # Calculate potential outliers\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "\n",
        "        print(f\"Number of potential outliers: {len(outliers)} ({len(outliers)/len(data)*100:.2f}%)\")\n",
        "        print(f\"Range of outliers: {outliers.min()} to {outliers.max()}\" if len(outliers) > 0 else \"No outliers\")\n",
        "\n",
        "# Analyze key numeric columns in Order Data\n",
        "important_order_columns = ['Sales', 'Quantity', 'Discount', 'Profit', 'Shipping_Cost']\n",
        "for column in important_order_columns:\n",
        "    analyze_numeric_column(order_data, column)\n",
        "\n",
        "# Analyze key numeric columns in Product Data\n",
        "important_product_columns = ['average_rating', 'rating_number', 'price']\n",
        "for column in important_product_columns:\n",
        "    analyze_numeric_column(product_data, column)\n",
        "\n",
        "# Analyze date columns in Order Data\n",
        "print(\"\\nOrder Data - Order Date Analysis:\")\n",
        "if 'Order_Date' in order_data.columns:\n",
        "    order_data['Order_Date'] = pd.to_datetime(order_data['Order_Date'])\n",
        "    print(f\"Date range: {order_data['Order_Date'].min()} to {order_data['Order_Date'].max()}\")\n",
        "    orders_by_month = order_data['Order_Date'].dt.to_period('M').value_counts().sort_index()\n",
        "    print(\"Orders by month:\")\n",
        "    print(orders_by_month)\n",
        "\n",
        "    # Visualize Orders by Month\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    orders_by_month.plot(kind='bar')\n",
        "    plt.title('Orders by Month')\n",
        "    plt.xlabel('Month')\n",
        "    plt.ylabel('Number of Orders')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualizations/orders_by_month.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Visualize Orders by Day of Week\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    order_data['Order_Date'].dt.day_name().value_counts().plot(kind='bar')\n",
        "    plt.title('Orders by Day of Week')\n",
        "    plt.xlabel('Day of Week')\n",
        "    plt.ylabel('Number of Orders')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualizations/orders_by_day_of_week.png')\n",
        "    plt.close()\n",
        "\n",
        "# Find top rated products\n",
        "print(\"\\nTop 5 Highest Rated Products (by average rating):\")\n",
        "top_rated = product_data.sort_values(by=['average_rating', 'rating_number'], ascending=[False, False]).head(5)\n",
        "for idx, row in top_rated.iterrows():\n",
        "    print(f\"{row['title']} - Rating: {row['average_rating']}, Reviews: {row['rating_number']}, Price: ${row['price']}\")\n",
        "\n",
        "# Visualize Top Rated Products\n",
        "plt.figure(figsize=(14, 8))\n",
        "top_rated_plot = top_rated[['title', 'average_rating', 'rating_number']].copy()\n",
        "top_rated_plot['title'] = top_rated_plot['title'].str.slice(0, 30) + '...'  # Shorten titles for readability\n",
        "sns.barplot(x='average_rating', y='title', data=top_rated_plot)\n",
        "plt.title('Top 5 Highest Rated Products')\n",
        "plt.xlabel('Average Rating')\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/top_rated_products.png')\n",
        "plt.close()\n",
        "\n",
        "# Find most profitable product categories\n",
        "print(\"\\nMost Profitable Product Categories:\")\n",
        "category_profit = order_data.groupby('Product_Category')['Profit'].sum().sort_values(ascending=False)\n",
        "for category, profit in category_profit.head(10).items():\n",
        "    print(f\"{category}: ${profit}\")\n",
        "\n",
        "# Visualize Most Profitable Categories\n",
        "plt.figure(figsize=(14, 8))\n",
        "top_categories = category_profit.head(10)\n",
        "sns.barplot(x=top_categories.values, y=top_categories.index)\n",
        "plt.title('Top 10 Most Profitable Product Categories')\n",
        "plt.xlabel('Total Profit ($)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/most_profitable_categories.png')\n",
        "plt.close()\n",
        "\n",
        "# Customer analysis\n",
        "print(\"\\nCustomer Analysis:\")\n",
        "print(f\"Total unique customers: {order_data['Customer_Id'].nunique()}\")\n",
        "print(\"\\nOrders by Gender:\")\n",
        "gender_counts = order_data['Gender'].value_counts()\n",
        "print(gender_counts)\n",
        "\n",
        "# Visualize Gender Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=gender_counts.index, y=gender_counts.values)\n",
        "plt.title('Orders by Gender')\n",
        "plt.savefig('visualizations/orders_by_gender.png')\n",
        "plt.close()\n",
        "\n",
        "# Create a pie chart for Gender\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%', startangle=90, shadow=True)\n",
        "plt.title('Gender Distribution of Orders')\n",
        "plt.axis('equal')\n",
        "plt.savefig('visualizations/gender_distribution_pie.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nOrders by Device Type:\")\n",
        "device_counts = order_data['Device_Type'].value_counts()\n",
        "print(device_counts)\n",
        "\n",
        "# Visualize Device Type Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=device_counts.index, y=device_counts.values)\n",
        "plt.title('Orders by Device Type')\n",
        "plt.savefig('visualizations/orders_by_device_type.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nOrders by Login Type:\")\n",
        "login_counts = order_data['Customer_Login_type'].value_counts()\n",
        "print(login_counts)\n",
        "\n",
        "# Visualize Login Type Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=login_counts.index, y=login_counts.values)\n",
        "plt.title('Orders by Login Type')\n",
        "plt.savefig('visualizations/orders_by_login_type.png')\n",
        "plt.close()\n",
        "\n",
        "# Create correlation matrix for numeric columns in Order Data\n",
        "numeric_order_data = order_data.select_dtypes(include=['int64', 'float64'])\n",
        "if not numeric_order_data.empty:\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    correlation_matrix = numeric_order_data.corr()\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
        "    plt.title('Correlation Matrix of Order Data Numeric Columns')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualizations/order_data_correlation_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "# Create correlation matrix for numeric columns in Product Data\n",
        "numeric_product_data = product_data.select_dtypes(include=['int64', 'float64'])\n",
        "if not numeric_product_data.empty:\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    correlation_matrix = numeric_product_data.corr()\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
        "    plt.title('Correlation Matrix of Product Data Numeric Columns')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualizations/product_data_correlation_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "# Relationship between Price and Rating in Product Data\n",
        "if 'price' in product_data.columns and 'average_rating' in product_data.columns:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    # Filter out extreme outliers for better visualization\n",
        "    price_ratings = product_data[['price', 'average_rating', 'rating_number']].dropna()\n",
        "    q1_price = price_ratings['price'].quantile(0.25)\n",
        "    q3_price = price_ratings['price'].quantile(0.75)\n",
        "    iqr_price = q3_price - q1_price\n",
        "    filtered_data = price_ratings[(price_ratings['price'] >= q1_price - 1.5*iqr_price) &\n",
        "                                 (price_ratings['price'] <= q3_price + 1.5*iqr_price)]\n",
        "\n",
        "    # Use scatter plot with point size representing number of ratings\n",
        "    plt.scatter(filtered_data['price'], filtered_data['average_rating'],\n",
        "               s=filtered_data['rating_number'] / 1000, alpha=0.5)\n",
        "    plt.title('Relationship between Price and Rating')\n",
        "    plt.xlabel('Price ($)')\n",
        "    plt.ylabel('Average Rating')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualizations/price_vs_rating.png')\n",
        "    plt.close()\n",
        "\n",
        "# Relationship between Sales and Profit in Order Data\n",
        "if 'Sales' in order_data.columns and 'Profit' in order_data.columns:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    # Filter out extreme outliers for better visualization\n",
        "    sales_profit = order_data[['Sales', 'Profit']].dropna()\n",
        "    q1_sales = sales_profit['Sales'].quantile(0.25)\n",
        "    q3_sales = sales_profit['Sales'].quantile(0.75)\n",
        "    iqr_sales = q3_sales - q1_sales\n",
        "    filtered_data = sales_profit[(sales_profit['Sales'] >= q1_sales - 1.5*iqr_sales) &\n",
        "                               (sales_profit['Sales'] <= q3_sales + 1.5*iqr_sales)]\n",
        "\n",
        "    plt.scatter(filtered_data['Sales'], filtered_data['Profit'], alpha=0.5)\n",
        "    plt.title('Relationship between Sales and Profit')\n",
        "    plt.xlabel('Sales ($)')\n",
        "    plt.ylabel('Profit ($)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualizations/sales_vs_profit.png')\n",
        "    plt.close()\n",
        "\n",
        "# Sample queries from the challenge\n",
        "print(\"\\nSample Query 1: Customer ID 37077's last order:\")\n",
        "customer_id = 37077\n",
        "customer_orders = order_data[order_data['Customer_Id'] == customer_id]\n",
        "if not customer_orders.empty:\n",
        "    # Convert to datetime and sort\n",
        "    customer_orders['Order_Date'] = pd.to_datetime(customer_orders['Order_Date'])\n",
        "    last_order = customer_orders.sort_values('Order_Date', ascending=False).iloc[0]\n",
        "    print(f\"Order Date: {last_order['Order_Date']}\")\n",
        "    print(f\"Product: {last_order['Product']}\")\n",
        "    print(f\"Sales Amount: ${last_order['Sales']}\")\n",
        "    print(f\"Shipping Cost: ${last_order['Shipping_Cost']}\")\n",
        "    print(f\"Order Priority: {last_order['Order_Priority']}\")\n",
        "else:\n",
        "    print(f\"No orders found for customer {customer_id}\")\n",
        "\n",
        "print(\"\\nSample Query 2: 5 most recent Critical priority orders:\")\n",
        "if 'Order_Priority' in order_data.columns and 'Order_Date' in order_data.columns:\n",
        "    # Convert to datetime\n",
        "    order_data['Order_Date'] = pd.to_datetime(order_data['Order_Date'])\n",
        "    critical_orders = order_data[order_data['Order_Priority'] == 'Critical']\n",
        "    recent_critical = critical_orders.sort_values('Order_Date', ascending=False).head(5)\n",
        "\n",
        "    for idx, order in recent_critical.iterrows():\n",
        "        print(f\"{idx + 1}. Date: {order['Order_Date']}\")\n",
        "        print(f\"   Customer ID: {order['Customer_Id']}\")\n",
        "        print(f\"   Product: {order['Product']}\")\n",
        "        print(f\"   Sales: ${order['Sales']}\")\n",
        "        print(f\"   Shipping Cost: ${order['Shipping_Cost']}\")\n",
        "        print()\n",
        "\n",
        "print(\"\\nSample Query 3: Guitar products analysis:\")\n",
        "guitar_products = product_data[\n",
        "    (product_data['title'].str.contains('guitar', case=False, na=False)) |\n",
        "    (product_data['categories'].str.contains('guitar', case=False, na=False))\n",
        "]\n",
        "print(f\"Total guitar products: {len(guitar_products)}\")\n",
        "print(f\"Average guitar product rating: {guitar_products['average_rating'].mean()}\")\n",
        "print(f\"Average guitar product price: ${guitar_products['price'].mean()}\")\n",
        "\n",
        "# Visualize Guitar Product Ratings\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(guitar_products['average_rating'].dropna(), kde=True, bins=20)\n",
        "plt.title('Distribution of Guitar Product Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/guitar_product_ratings.png')\n",
        "plt.close()\n",
        "\n",
        "# Visualize Guitar Product Prices\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Handle outliers for better visualization\n",
        "q1 = guitar_products['price'].quantile(0.25)\n",
        "q3 = guitar_products['price'].quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "filtered_data = guitar_products[(guitar_products['price'] >= lower_bound) &\n",
        "                              (guitar_products['price'] <= upper_bound)]\n",
        "sns.histplot(filtered_data['price'].dropna(), kde=True, bins=20)\n",
        "plt.title('Distribution of Guitar Product Prices (Outliers Removed)')\n",
        "plt.xlabel('Price ($)')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/guitar_product_prices.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nTop 5 highest rated guitar products:\")\n",
        "top_guitars = guitar_products.sort_values(by=['average_rating', 'rating_number'], ascending=[False, False]).head(5)\n",
        "for idx, row in top_guitars.iterrows():\n",
        "    print(f\"- {row['title']}\")\n",
        "    print(f\"  Rating: {row['average_rating']} stars ({row['rating_number']} reviews)\")\n",
        "    print(f\"  Price: ${row['price']}\")\n",
        "    print()\n",
        "\n",
        "# Visualize Top Rated Guitar Products\n",
        "plt.figure(figsize=(14, 8))\n",
        "top_guitars_plot = top_guitars[['title', 'average_rating', 'rating_number']].copy()\n",
        "top_guitars_plot['title'] = top_guitars_plot['title'].str.slice(0, 30) + '...'  # Shorten titles for readability\n",
        "sns.barplot(x='average_rating', y='title', data=top_guitars_plot)\n",
        "plt.title('Top 5 Highest Rated Guitar Products')\n",
        "plt.xlabel('Average Rating')\n",
        "plt.tight_layout()\n",
        "plt.savefig('visualizations/top_rated_guitar_products.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nVisualization complete! All plots saved in the 'visualizations' folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d0a409",
      "metadata": {
        "id": "f1d0a409",
        "outputId": "d8b0ab44-e044-4c37-9bd4-d2e7f0a96d34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "DISTRIBUTION ANALYSIS FOR ORDER DATA\n",
            "==================================================\n",
            "\n",
            "-------------------- NUMERIC COLUMNS --------------------\n",
            "\n",
            "Column: Aging\n",
            "Number of non-null values: 51289\n",
            "Min: 1.0000, Max: 10.5000, Range: 9.5000\n",
            "Mean: 5.2550, Median: 5.0000\n",
            "Standard Deviation: 2.9599\n",
            "10th percentile: 1.0000, 25th percentile: 3.0000\n",
            "75th percentile: 8.0000, 90th percentile: 9.0000\n",
            "Interquartile Range (IQR): 5.0000\n",
            "Outliers: 0 (0.00% of data)\n",
            "Skewness: 0.0656 (Symmetric)\n",
            "Kurtosis: -1.2601 (Normal)\n",
            "\n",
            "Distribution (Text Histogram):\n",
            "Range: 1.0000 to 10.5000\n",
            "(0.99, 1.95] |  7467 ███████ 14.6%\n",
            "(1.95, 2.9] |  4815 ████ 9.4%\n",
            "(2.9, 3.85] |  5017 ████ 9.8%\n",
            "(3.85, 4.8] |  4875 ████ 9.5%\n",
            "(4.8, 5.75] |  4939 ████ 9.6%\n",
            "(5.75, 6.7] |  4891 ████ 9.5%\n",
            "(6.7, 7.65] |  4900 ████ 9.6%\n",
            "(7.65, 8.6] |  4820 ████ 9.4%\n",
            "(8.6, 9.55] |  4674 ████ 9.1%\n",
            "(9.55, 10.5] |  4891 ████ 9.5%\n",
            "\n",
            "Column: Customer_Id\n",
            "Number of non-null values: 51290\n",
            "Min: 10000.0000, Max: 99999.0000, Range: 89999.0000\n",
            "Mean: 58155.7588, Median: 61018.0000\n",
            "Standard Deviation: 26032.2158\n",
            "10th percentile: 20284.6000, 25th percentile: 35831.2500\n",
            "75th percentile: 80736.2500, 90th percentile: 92292.0000\n",
            "Interquartile Range (IQR): 44905.0000\n",
            "Outliers: 0 (0.00% of data)\n",
            "Skewness: -0.1795 (Symmetric)\n",
            "Kurtosis: -1.1787 (Normal)\n",
            "\n",
            "Distribution (Text Histogram):\n",
            "Range: 10000.0000 to 99999.0000\n",
            "(9910.001, 18999.9] |  4466 ████ 8.7%\n",
            "(18999.9, 27999.8] |  4477 ████ 8.7%\n",
            "(27999.8, 36999.7] |  4454 ████ 8.7%\n",
            "(36999.7, 45999.6] |  4580 ████ 8.9%\n",
            "(45999.6, 54999.5] |  4507 ████ 8.8%\n",
            "(54999.5, 63999.4] |  5129 █████ 10.0%\n",
            "(63999.4, 72999.3] |  5802 █████ 11.3%\n",
            "(72999.3, 81999.2] |  5852 █████ 11.4%\n",
            "(81999.2, 90999.1] |  6022 █████ 11.7%\n",
            "(90999.1, 99999.0] |  6001 █████ 11.7%\n",
            "\n",
            "Column: Sales\n",
            "Number of non-null values: 51289\n",
            "Min: 33.0000, Max: 250.0000, Range: 217.0000\n",
            "Mean: 152.3409, Median: 133.0000\n",
            "Standard Deviation: 66.4954\n",
            "10th percentile: 65.0000, 25th percentile: 85.0000\n",
            "75th percentile: 218.0000, 90th percentile: 228.0000\n",
            "Interquartile Range (IQR): 133.0000\n",
            "Outliers: 0 (0.00% of data)\n",
            "Skewness: -0.0878 (Symmetric)\n",
            "Kurtosis: -1.4401 (Normal)\n",
            "\n",
            "Distribution (Text Histogram):\n",
            "Range: 33.0000 to 250.0000\n",
            "(32.783, 54.7] |  2588 ██ 5.0%\n",
            "(54.7, 76.4] |  6465 ██████ 12.6%\n",
            "(76.4, 98.1] |  4097 ███ 8.0%\n",
            "(98.1, 119.8] |  6795 ██████ 13.2%\n",
            "(119.8, 141.5] |  6526 ██████ 12.7%\n",
            "(141.5, 163.2] |  2553 ██ 5.0%\n",
            "(163.2, 184.9] |     0  0.0%\n",
            "(184.9, 206.6] |  2777 ██ 5.4%\n",
            "(206.6, 228.3] | 15213 ██████████████ 29.7%\n",
            "(228.3, 250.0] |  4275 ████ 8.3%\n",
            "\n",
            "Column: Quantity\n",
            "Number of non-null values: 51288\n",
            "Min: 1.0000, Max: 5.0000, Range: 4.0000\n",
            "Mean: 2.5030, Median: 2.0000\n",
            "Standard Deviation: 1.5119\n",
            "10th percentile: 1.0000, 25th percentile: 1.0000\n",
            "75th percentile: 4.0000, 90th percentile: 5.0000\n",
            "Interquartile Range (IQR): 3.0000\n",
            "Outliers: 0 (0.00% of data)\n",
            "Skewness: 0.4642 (Symmetric)\n",
            "Kurtosis: -1.2829 (Normal)\n",
            "\n",
            "Distribution (Text Histogram):\n",
            "Range: 1.0000 to 5.0000\n",
            "(0.996, 1.4] | 20423 ███████████████████ 39.8%\n",
            "(1.4, 1.8] |     0  0.0%\n",
            "(1.8, 2.2] |  8223 ████████ 16.0%\n",
            "(2.2, 2.6] |     0  0.0%\n",
            "(2.6, 3.0] |  7266 ███████ 14.2%\n",
            "(3.0, 3.4] |     0  0.0%\n",
            "(3.4, 3.8] |     0  0.0%\n",
            "(3.8, 4.2] |  7174 ██████ 14.0%\n",
            "(4.2, 4.6] |     0  0.0%\n",
            "(4.6, 5.0] |  8202 ███████ 16.0%\n",
            "\n",
            "Column: Discount\n",
            "Number of non-null values: 51289\n",
            "Min: 0.1000, Max: 0.5000, Range: 0.4000\n",
            "Mean: 0.3038, Median: 0.3000\n",
            "Standard Deviation: 0.1310\n",
            "10th percentile: 0.1000, 25th percentile: 0.2000\n",
            "75th percentile: 0.4000, 90th percentile: 0.5000\n",
            "Interquartile Range (IQR): 0.2000\n",
            "Outliers: 0 (0.00% of data)\n",
            "Skewness: 0.0332 (Symmetric)\n",
            "Kurtosis: -1.1230 (Normal)\n",
            "\n",
            "Distribution (Text Histogram):\n",
            "Range: 0.1000 to 0.5000\n",
            "(0.0996, 0.14] |  7209 ███████ 14.1%\n",
            "(0.14, 0.18] |     0  0.0%\n",
            "(0.18, 0.22] | 12252 ███████████ 23.9%\n",
            "(0.22, 0.26] |     0  0.0%\n",
            "(0.26, 0.3] | 12402 ████████████ 24.2%\n",
            "(0.3, 0.34] |     0  0.0%\n",
            "(0.34, 0.38] |     0  0.0%\n",
            "(0.38, 0.42] | 10222 █████████ 19.9%\n",
            "(0.42, 0.46] |     0  0.0%\n",
            "(0.46, 0.5] |  9204 ████████ 17.9%\n",
            "\n",
            "Column: Profit\n",
            "Number of non-null values: 51290\n",
            "Min: 0.5000, Max: 167.5000, Range: 167.0000\n",
            "Mean: 70.4072, Median: 59.9000\n",
            "Standard Deviation: 48.7295\n",
            "10th percentile: 15.5000, 25th percentile: 24.9000\n",
            "75th percentile: 118.4000, 90th percentile: 135.8000\n",
            "Interquartile Range (IQR): 93.5000\n",
            "Outliers: 0 (0.00% of data)\n",
            "Skewness: 0.2610 (Symmetric)\n",
            "Kurtosis: -1.4648 (Normal)\n",
            "\n",
            "Distribution (Text Histogram):\n",
            "Range: 0.5000 to 167.5000\n",
            "(0.333, 17.2] |  7192 ███████ 14.0%\n",
            "(17.2, 33.9] | 11284 ███████████ 22.0%\n",
            "(33.9, 50.6] |  6070 █████ 11.8%\n",
            "(50.6, 67.3] |  2650 ██ 5.2%\n",
            "(67.3, 84.0] |  2515 ██ 4.9%\n",
            "(84.0, 100.7] |  2606 ██ 5.1%\n",
            "(100.7, 117.4] |  5743 █████ 11.2%\n",
            "(117.4, 134.1] |  7430 ███████ 14.5%\n",
            "(134.1, 150.8] |  4098 ███ 8.0%\n",
            "(150.8, 167.5] |  1702 █ 3.3%\n",
            "\n",
            "Column: Shipping_Cost\n",
            "Number of non-null values: 51289\n",
            "Min: 0.1000, Max: 16.8000, Range: 16.7000\n",
            "Mean: 7.0416, Median: 6.0000\n",
            "Standard Deviation: 4.8717\n",
            "10th percentile: 1.6000, 25th percentile: 2.5000\n",
            "75th percentile: 11.8000, 90th percentile: 13.6000\n",
            "Interquartile Range (IQR): 9.3000\n",
            "Outliers: 0 (0.00% of data)\n",
            "Skewness: 0.2625 (Symmetric)\n",
            "Kurtosis: -1.4622 (Normal)\n",
            "\n",
            "Distribution (Text Histogram):\n",
            "Range: 0.1000 to 16.8000\n",
            "(0.0833, 1.77] |  7192 ███████ 14.0%\n",
            "(1.77, 3.44] | 11979 ███████████ 23.4%\n",
            "(3.44, 5.11] |  5392 █████ 10.5%\n",
            "(5.11, 6.78] |  2633 ██ 5.1%\n",
            "(6.78, 8.45] |  2515 ██ 4.9%\n",
            "(8.45, 10.12] |  2876 ██ 5.6%\n",
            "(10.12, 11.79] |  5473 █████ 10.7%\n",
            "(11.79, 13.46] |  7763 ███████ 15.1%\n",
            "(13.46, 15.13] |  3764 ███ 7.3%\n",
            "(15.13, 16.8] |  1702 █ 3.3%\n",
            "\n",
            "-------------------- CATEGORICAL COLUMNS --------------------\n",
            "\n",
            "Column: Gender\n",
            "Number of non-null values: 51290\n",
            "Number of unique values: 2\n",
            "Value distribution:\n",
            "Male: 28138 █████████████████████ 54.9%\n",
            "Female: 23152 ██████████████████ 45.1%\n",
            "\n",
            "Column: Device_Type\n",
            "Number of non-null values: 51290\n",
            "Number of unique values: 2\n",
            "Value distribution:\n",
            "Web: 47632 █████████████████████████████████████ 92.9%\n",
            "Mobile:  3658 ██ 7.1%\n",
            "\n",
            "Column: Customer_Login_type\n",
            "Number of non-null values: 51290\n",
            "Number of unique values: 4\n",
            "Value distribution:\n",
            "Member: 49097 ██████████████████████████████████████ 95.7%\n",
            "Guest:  1993 █ 3.9%\n",
            "First SignUp:   173  0.3%\n",
            "New :    27  0.1%\n",
            "\n",
            "Column: Product_Category\n",
            "Number of non-null values: 51290\n",
            "Number of unique values: 4\n",
            "Value distribution:\n",
            "Fashion: 25646 ████████████████████ 50.0%\n",
            "Home & Furniture: 15438 ████████████ 30.1%\n",
            "Auto & Accessories:  7505 █████ 14.6%\n",
            "Electronic:  2701 ██ 5.3%\n",
            "\n",
            "Column: Product\n",
            "Number of non-null values: 51290\n",
            "Number of unique values: 42\n",
            "Top 10 most frequent values (out of 42):\n",
            "1. Suits: 2332 (4.5%)\n",
            "2. T - Shirts: 2332 (4.5%)\n",
            "3. Fossil Watch: 2332 (4.5%)\n",
            "4. Shirts: 2332 (4.5%)\n",
            "5. Jeans: 2332 (4.5%)\n",
            "6. Sports Wear: 2331 (4.5%)\n",
            "7. Titak watch: 2331 (4.5%)\n",
            "8. Sneakers: 2331 (4.5%)\n",
            "9. Formal Shoes: 2331 (4.5%)\n",
            "10. Running Shoes: 2331 (4.5%)\n",
            "\n",
            "Column: Order_Priority\n",
            "Number of non-null values: 51288\n",
            "Number of unique values: 4\n",
            "Value distribution:\n",
            "Medium: 29433 ██████████████████████ 57.4%\n",
            "High: 15499 ████████████ 30.2%\n",
            "Critical:  3932 ███ 7.7%\n",
            "Low:  2424 █ 4.7%\n",
            "\n",
            "Column: Payment_method\n",
            "Number of non-null values: 51290\n",
            "Number of unique values: 5\n",
            "Value distribution:\n",
            "credit_card: 38137 █████████████████████████████ 74.4%\n",
            "money_order:  9629 ███████ 18.8%\n",
            "e_wallet:  2789 ██ 5.4%\n",
            "debit_card:   734  1.4%\n",
            "not_defined:     1  0.0%\n",
            "\n",
            "-------------------- DATE COLUMNS --------------------\n",
            "\n",
            "Column: Order_Date\n",
            "Date analysis:\n",
            "Date range: 2018-01-01 to 2018-12-30\n",
            "Timespan: 363 days\n",
            "\n",
            "Distribution by month:\n",
            "2018-01: 2519\n",
            "2018-02: 2206\n",
            "2018-03: 2899\n",
            "2018-04: 3896\n",
            "2018-05: 5417\n",
            "2018-06: 4179\n",
            "2018-07: 5321\n",
            "2018-08: 4373\n",
            "2018-09: 4851\n",
            "2018-10: 4886\n",
            "2018-11: 5679\n",
            "2018-12: 5064\n",
            "\n",
            "Distribution by day of week:\n",
            "Tuesday: 7960 (15.5%)\n",
            "Monday: 7616 (14.8%)\n",
            "Wednesday: 7439 (14.5%)\n",
            "Thursday: 7188 (14.0%)\n",
            "Sunday: 7143 (13.9%)\n",
            "Friday: 7018 (13.7%)\n",
            "Saturday: 6926 (13.5%)\n",
            "\n",
            "Column: Time\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_15904\\733296925.py:109: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  dates = pd.to_datetime(dataframe[col])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time analysis:\n",
            "Earliest time: 00:00:06\n",
            "Latest time: 23:59:58\n",
            "\n",
            "Distribution by hour:\n",
            "00:00:  1238  2.4%\n",
            "01:00:   590  1.2%\n",
            "02:00:   251  0.5%\n",
            "03:00:   136  0.3%\n",
            "04:00:   106  0.2%\n",
            "05:00:    90  0.2%\n",
            "06:00:   244  0.5%\n",
            "07:00:   653  1.3%\n",
            "08:00:  1557 █ 3.0%\n",
            "09:00:  2468 █ 4.8%\n",
            "10:00:  3253 ██ 6.3%\n",
            "11:00:  3407 ██ 6.6%\n",
            "12:00:  3056 ██ 6.0%\n",
            "13:00:  3383 ██ 6.6%\n",
            "14:00:  3367 ██ 6.6%\n",
            "15:00:  3367 ██ 6.6%\n",
            "16:00:  3381 ██ 6.6%\n",
            "17:00:  3130 ██ 6.1%\n",
            "18:00:  2949 ██ 5.7%\n",
            "19:00:  3037 ██ 5.9%\n",
            "20:00:  3195 ██ 6.2%\n",
            "21:00:  3224 ██ 6.3%\n",
            "22:00:  3048 ██ 5.9%\n",
            "23:00:  2160 █ 4.2%\n",
            "\n",
            "==================================================\n",
            "DISTRIBUTION ANALYSIS FOR PRODUCT DATA\n",
            "==================================================\n",
            "\n",
            "-------------------- NUMERIC COLUMNS --------------------\n",
            "\n",
            "Column: average_rating\n",
            "Number of non-null values: 5000\n",
            "Min: 3.1000, Max: 5.0000, Range: 1.9000\n",
            "Mean: 4.4865, Median: 4.5000\n",
            "Standard Deviation: 0.2452\n",
            "10th percentile: 4.2000, 25th percentile: 4.4000\n",
            "75th percentile: 4.7000, 90th percentile: 4.7000\n",
            "Interquartile Range (IQR): 0.3000\n",
            "Outliers: 160 (3.20% of data)\n",
            "Skewness: -1.1766 (Highly skewed)\n",
            "Kurtosis: 1.9477 (Normal)\n",
            "\n",
            "Distribution (Text Histogram):\n",
            "Range: 3.1000 to 5.0000\n",
            "(3.098, 3.29] |     3  0.1%\n",
            "(3.29, 3.48] |     8  0.2%\n",
            "(3.48, 3.67] |    28  0.6%\n",
            "(3.67, 3.86] |    70  1.4%\n",
            "(3.86, 4.05] |   190 █ 3.8%\n",
            "(4.05, 4.24] |   479 ████ 9.6%\n",
            "(4.24, 4.43] |  1062 ██████████ 21.2%\n",
            "(4.43, 4.62] |  1744 █████████████████ 34.9%\n",
            "(4.62, 4.81] |  1382 █████████████ 27.6%\n",
            "(4.81, 5.0] |    34  0.7%\n",
            "\n",
            "Column: rating_number\n",
            "Number of non-null values: 5000\n",
            "Min: 547.0000, Max: 100615.0000, Range: 100068.0000\n",
            "Mean: 1876.2298, Median: 963.0000\n",
            "Standard Deviation: 3899.3384\n",
            "10th percentile: 598.0000, 25th percentile: 698.7500\n",
            "75th percentile: 1532.0000, 90th percentile: 2527.7000\n",
            "Interquartile Range (IQR): 833.2500\n",
            "Outliers: 500 (10.00% of data)\n",
            "Skewness: 10.2816 (Highly skewed)\n",
            "Kurtosis: 167.6507 (Heavy-tailed)\n",
            "\n",
            "Distribution (Text Histogram):\n",
            "Range: 547.0000 to 100615.0000\n",
            "(446.932, 10553.8] |  4887 ████████████████████████████████████████████████ 97.7%\n",
            "(10553.8, 20560.6] |    69  1.4%\n",
            "(20560.6, 30567.4] |    30  0.6%\n",
            "(30567.4, 40574.2] |     8  0.2%\n",
            "(40574.2, 50581.0] |     1  0.0%\n",
            "(50581.0, 60587.8] |     2  0.0%\n",
            "(60587.8, 70594.6] |     1  0.0%\n",
            "(70594.6, 80601.4] |     1  0.0%\n",
            "(80601.4, 90608.2] |     0  0.0%\n",
            "(90608.2, 100615.0] |     1  0.0%\n",
            "\n",
            "Column: price\n",
            "Number of non-null values: 3734\n",
            "Min: 1.9900, Max: 1328.0000, Range: 1326.0100\n",
            "Mean: 56.5080, Median: 25.9250\n",
            "Standard Deviation: 83.4476\n",
            "10th percentile: 8.9900, 25th percentile: 13.0825\n",
            "75th percentile: 59.9900, 90th percentile: 147.8970\n",
            "Interquartile Range (IQR): 46.9075\n",
            "Outliers: 416 (11.14% of data)\n",
            "Skewness: 4.2736 (Highly skewed)\n",
            "Kurtosis: 31.5470 (Heavy-tailed)\n",
            "\n",
            "Distribution (Text Histogram):\n",
            "Range: 1.9900 to 1328.0000\n",
            "(0.664, 134.591] |  3323 ████████████████████████████████████████████ 89.0%\n",
            "(134.591, 267.192] |   292 ███ 7.8%\n",
            "(267.192, 399.793] |    86 █ 2.3%\n",
            "(399.793, 532.394] |    13  0.3%\n",
            "(532.394, 664.995] |    14  0.4%\n",
            "(664.995, 797.596] |     4  0.1%\n",
            "(797.596, 930.197] |     0  0.0%\n",
            "(930.197, 1062.798] |     1  0.0%\n",
            "(1062.798, 1195.399] |     0  0.0%\n",
            "(1195.399, 1328.0] |     1  0.0%\n",
            "\n",
            "-------------------- CATEGORICAL COLUMNS --------------------\n",
            "\n",
            "Column: main_category\n",
            "Number of non-null values: 4970\n",
            "Number of unique values: 24\n",
            "Top 10 most frequent values (out of 24):\n",
            "1. Musical Instruments: 4277 (86.1%)\n",
            "2. All Electronics: 195 (3.9%)\n",
            "3. Industrial & Scientific: 93 (1.9%)\n",
            "4. Tools & Home Improvement: 68 (1.4%)\n",
            "5. Amazon Home: 60 (1.2%)\n",
            "6. Home Audio & Theater: 57 (1.1%)\n",
            "7. Toys & Games: 40 (0.8%)\n",
            "8. Computers: 33 (0.7%)\n",
            "9. Cell Phones & Accessories: 31 (0.6%)\n",
            "10. AMAZON FASHION: 28 (0.6%)\n",
            "\n",
            "Column: title\n",
            "Number of non-null values: 5000\n",
            "Number of unique values: 4831\n",
            "Top 10 most frequent values (out of 4831):\n",
            "1. Acoustic Panels Studio Foam Sound Proof Panels Nosie Dampening Foam Studio Music Equipment Acoustical Treatments Foam 12 Pack-12''12''1'': 8 (0.2%)\n",
            "2. M33 Ukulele Strap Upgrade Multicolor Hawaiian Jacquard Woven, Double J Hooks Clip On Ukulele Belt: 7 (0.1%)\n",
            "3. Hohner PBH7 Piedmont Blues Harmonica Set - 7-Pack: 6 (0.1%)\n",
            "4. Karaoke Microphone for Kids Adults, Wireless 4 in 1 Handheld Bluetooth Microphone with LED Lights, Portable Smartphone Speaker Boys Girls Singing Toys for Home KTV Outdoor Christmas Birthday Party: 5 (0.1%)\n",
            "5. Acoustic Foam Panels, 12Pack 2\" X 12\" X 12\" SoundProof Padding Foam Panels, Studio Foam Pyramid Tiles Sound Absorbing Dampening Foam Treatment Wall Panels: 5 (0.1%)\n",
            "6. GBGS Halloween White Strobe Light Super Bright 24 LEDs, Adjustable Speed, Flash Party Lighting for Kids, Children, Christmas, Birthday, Soft Eye-Caring: 5 (0.1%)\n",
            "7. Audio-Technica AT2020USB Cardioid Condenser USB Microphone (Discontinued),black: 4 (0.1%)\n",
            "8. Crosley NP1 Replacement Needle: 4 (0.1%)\n",
            "9. Audio-Technica AT95E Phonograph Cartridge for 1/2\" Mount Turntables: 4 (0.1%)\n",
            "10. Crescent 4/4 Full Size Student Violin Starter Kit, Black Color (Includes CrescentTM Digital E-Tuner): 4 (0.1%)\n",
            "\n",
            "Column: features\n",
            "Number of non-null values: 5000\n",
            "Number of unique values: 4515\n",
            "Top 10 most frequent values (out of 4515):\n",
            "1. []: 161 (3.2%)\n",
            "2. ['Good Looking - The Ukulele is beautiful and well-made. We have different colors for you to make choice.', 'High Quality - The Ukulele is generally made of high quality Basswood with advanced wood patternsand nylon strings. All these make the tone of Ukulele bright and smooth.', 'Easy to Play - The Ukulele is suitable for beginners, either kids or adults. If you would like to start playing Ukulele, it is the best choice for you.', 'Free Accessories - We offer you a fine package of free accessories: a set of 4 strings, a high quality Ukulele bag, and 2 finger picks.', 'Best Service - 30 days unconditional money back guarantee. Please locate your order on your Amazon account and click \"Ask Seller\" to get assistance. We provide 24 hours customer service.']: 13 (0.3%)\n",
            "3. ['♫SIGNAL STABILIZATION: Guitar cable AWG24 oxygen-free copper center conductor provides enhanced signal transmission for maximum signal transmission stability.', '♫LONG WORKING LIFE: Instrument cable is wrapped with aluminum foil to achieve super shielding. Guitar cable ensures fast signal transmission, pure tone, and increases the toughness of the guitar cable and prolongs the working life of the guitar cable.', '♫INSULATING OUTER LAYER: PVC insulating material, effectively reducing the electromagnetic interference of environmental noise, soft and durable. Lulu Home guitar cable insulating outer layer is more effective in ensuring the safety of using, and avoiding the danger while entertainment. Considering that the guitar cable often needs to be bent during carrying and use, Lulu Home guitar cable is made of PVC, which makes it softer and more durable than similar products.', '♫RELIABLE GOLD-PLATED: Higher conductivity and corrosion resistance. Damage to each guitar cable is usually a problem at the plug, and the quality of the plug directly affects the working life of the guitar cable. Lulu Home guitar cable plug is made of gold-plated material and has a high-hardness casing protection that is both durable and beautiful appearance. Heat shrink tubing will not be damaged.', '♫GUARANTEED FOR LIFE: You are not satisfied with our guitar cable, just send them back to us for a full refund in time. We hope to be better. Click the Add to Cart button now.']: 9 (0.2%)\n",
            "4. ['Acoustic environment is very important for Studios, Recording Studios, Vocal Booths, Control Rooms, Offices, Home Studios, Home Entertainment Theaters, Gaming Rooms, and Home Offices.Acoustic panels are good for sound by effectively controlling unwanted reverb and echo, Great for spot treating walls in your studio or office.', 'Whether you are podcasting and hosting a Webinar or video review producer,nobody likes to hear Echoes through your sounds.Our acoustic panels will help eliminate Echo in any room of your house.', 'By increasing noise absorption properties inside the room will help to reduce noise transmission through the walls and thus reduce noise outside the room.professional acoustic control reduce waves, reverb and flutter echoes in smaller to medium sized rooms.', 'Acoustic panels are compression packing and can expand quickly. About tape: Most people use adhesive spray though. I suggest using use 3M Command strips,4 per square (one in each corner),and it worked great !', 'NRC:0.5 Density:25kg/m3 Fire Retardant: CA Technical Bulletin 117.']: 9 (0.2%)\n",
            "5. ['Traditional reeds are known for their excellent response in all registers, allowing a pianissimo attack in even highest notes.', 'Extremely flexible, allowing the legato or staccato execution of large intervals while maintaining a richness of tone that gives body and clarity to the sound, which is a hallmark of Vandoren reeds.', 'Traditional reeds are available for all clarinets and saxophones in various strengths.', \"Every reed sealed in 'Flow Pack' to ensure freshness.\"]: 8 (0.2%)\n",
            "6. ['【TONGUE DRUM SET】- 1×musical steel drum used a unique mixture of alloying elements, 1×Alternate scale sticker, 2×Drumstick, 1×Tray(put drumsticks on it), 1×Bag, 1×Music book.', '【HIGH-GRADE STEEL MATERIAL】- Constructed of Steel-Titanium alloy, coated by protective spraying-paint against tarnishing, scuffs, and scratches. Handcrafted by artisans, it can produce a clean, ethereal, Buddha-like sound.', '【PROFESSIONAL MUSICAL DRUM】- 8-inch 11-note steel tongue drum has a total of eleven sounds in F tune. This makes it easier to learn and has a wider range of sounds and can play more music.', '【EASY TO LEARN & PLAY】- Beginners can learn how to play in ten minutes by using textbooks and scale stickers. You can play with the rubber topped mallets included or simply with your hands.', '【WIDE APPLICATION】- Great for personal meditation, yoga practice, zazen, music therapists, performances, religious activities, etc. Its ethereal sounds can purify your mind and soul and makes you achieve inner peace.']: 7 (0.1%)\n",
            "7. [\"Chord with comfort and bend strings with ease from this ‘C' shaped satin finished neck, 21 medium jumbo frets and modern 9.5 fingerboard radius\", 'Explore Jazz to Punk Rock sounds from the 3 single coils pickups, two tone controls, master volume and the 5 way pick up selector switch', 'Express yourself by creating sharp and flat pitch variations via the chrome Synchronized Tremolo indicative of the Stratocaster', 'Enjoy maximum playability with various string gauges via the adjustable truss rod and the adjustable saddles of the chrome Synchronized Tremolo', 'Enjoy peace of mind that the gloss polyurethane gloss finish will protect this beautiful body for many years']: 7 (0.1%)\n",
            "8. ['【TONGUE DRUM SET】- 1×musical steel drum used a unique mixture of alloying elements, 1×Alternate scale sticker, 2×Drumstick, 1×tray(put drumsticks on it), 1×Padded Travel Bag, 1×Music book.', '【HIGH-GRADE STEEL MATERIAL】- Constructed of Steel-Titanium alloy, coated by protective spraying-paint against tarnishing, scuffs, and scratches. Handcrafted by artisans, it can produce a clean, ethereal, Buddha-like sound.', '【PROFESSIONAL MUSICAL DRUM】- 10-inch 11-note steel tongue drum has a total of eleven sounds in D major. This makes it easier to learn and has a wider range of sounds and can play more music.', '【EASY TO LEARN & PLAY】- Beginners can learn how to play in ten minutes by using textbooks and scale stickers. You can play with the rubber topped mallets included or simply with your hands.', '【WIDE APPLICATION】- Great for personal meditation, yoga practice, zazen, music therapists, performances, religious activities, etc. Its ethereal sounds can purify your mind and soul and makes you achieve inner peace.']: 7 (0.1%)\n",
            "9. ['Male XLR to Female XLR', 'Balanced Microphone Cable', 'Ideal for use with microphones, active speakers, studio and stage applications and DMX lighting chains', 'Lengths: 1.6 / 3 / 6 / 10 / 20 / 30 / 50 / 65 Feet', 'Limited Lifetime Warranty']: 6 (0.1%)\n",
            "10. ['Genuine Crosley replacement needle', 'Diamond stylus needle', 'ABS plastic composition', 'Plays 3 Speeds - 33 1/3, 45 and 78 RPM Records', 'Needle provides thousands of rotations for extended playing time']: 6 (0.1%)\n",
            "\n",
            "Column: description\n",
            "Number of non-null values: 5000\n",
            "Number of unique values: 2644\n",
            "Top 10 most frequent values (out of 2644):\n",
            "1. []: 2152 (43.0%)\n",
            "2. ['1']: 11 (0.2%)\n",
            "3. ['Introduction', 'Apelila is an excellent and energetic brand in the US. Since we have dedicated to develop Apelila, our product lines are rich and diverse, for example, home furnishing, beauty tool, and musical instrument. At present, we are expanding the brand worldwide with our consistent high quality.', 'Features', '1. Good Looking.', 'The Ukulele is beautiful and well-made. We have different colors for you to make choice.', '2. High quality.', 'The Ukulele is generally made of high quality Basswood and nylon strings. All these make the tone of Ukulele bright and smooth .', '3. Easy to play.', 'The Ukulele is suitable for beginners, either children or adults. If you would like to start playing Ukulele, it is the best choice for you.', '4. Free accessories.', 'We offer you a fine package of free accessories: a set of 4 strings, a high quality Ukulele bag, and 2 finger picks.', 'Notice:', 'The strings are all in loose to keep from damage during the delivery. You have to tune them up before playing.']: 10 (0.2%)\n",
            "4. ['The most widely played reeds in the world, with a superiority proven over the years, these reeds suit all styles of music.']: 7 (0.1%)\n",
            "5. ['The Squier by Fender Bullet Strat with tremolo is an affordable and practical electric guitar designed for anyone’s budget. It has many of the ingredients that have propelled the Stratocaster to be one of the most iconic guitars ever made. This Squier is a perfect choice for a first guitar no matter who you are or what style of music you want to learn. Welcome to the Fender family!']: 7 (0.1%)\n",
            "6. [\"Dunlop's Primetone Sculpted Plectra will glide off your strings and bring out the true voice and clarity of your instrument. They're made from Ultex for maximum durability and tonal definition, with hand-burnished sculpted edges for fast, articulate runs and effortless strumming. Available in a variety of shapes and gauges with a low-profile grip or a smooth traditional surface.\"]: 5 (0.1%)\n",
            "7. ['Decorative:', 'soft, rich natural material experience, a variety of alternative modern colors, simple decorative modeling.', 'Thermal insulation:', 'special sound absorption mechanism, creating excellent insulation performance, thereby creating a very comfortable constant temperature space.', 'Flame retardancy:', 'polyester fiber fire retardant material, excellent fire retardant and fire retardant properties, CA Technical Bulletin 117 fire detection report.', 'Environmental protection:', 'polyester fiber close to the natural color and characteristics, national level testing institutions issued by the formaldehyde emission safety certification, the true sense of green environmental products.', 'structure:', 'really light and delicate.', 'Easily machinability:', 'the free cutting of the art knife, the collocation of various colors, the simple stitching, the simple processing of the edge angle, the perfect artistic picture and the different style can be easily embodied.', 'Stability:', 'good physical stability determines that it will not expand or shrink due to changes in temperature and humidity.', 'Impact resistance:', 'soft, natural texture and high elasticity will not break under the impact of huge external forces.']: 5 (0.1%)\n",
            "8. ['See below']: 5 (0.1%)\n",
            "9. ['About FUYXAN Ukulele', 'More than a simple musical instrument, they are an extension of you! FUYXAN Ukulele expects to give you an impressive music journey full of color and lets you be free to dance to the tune.', 'Specifications', 'Size: 21 inch\\xa0 String Material: Nylon Strings\\xa0 Body Material: Quality ABS Plastic\\xa0 Gear Heads: Metal\\xa0 Number of Frets: 15\\xa0 Colors for Option: Black, Pink, White, Blue, Yellow', 'About Tuning Ukulele', 'Are you finding your new ukulele always out of tune though you tune it again and again?\\xa0 Actually, it’s common for nylon strings since they are new and haven’t adapted to the stretching of tension. Therefore, it’s a little difficult for a new ukulele to hold its tune at the beginning. Please don’t worry, here are concrete steps for you to settle down a new ukulele.\\xa0\\xa0 Step 1, tune each string as close to pitch as possible. Starting from top to bottom, finally the order of notes should respectively be GCEA.\\xa0 Step 2, pull along the entire string carefully with two hands. Also be careful not to pull too hard on each string while stretching to avoid damage.\\xa0 Step 3 , repeat the previous process of tuning and stretching until each string stays in tune gradually.\\xa0 Step 4, play your ukulele as frequently as possible. It may take a few days or even few weeks before they settle down. Please be patient, since once that happens, you could play your ukulele at any time!\\xa0\\xa0 Note: To protect the strings from damage during shipping, all of them are in loose state. Please be patient to tune it before playing.']: 5 (0.1%)\n",
            "10. [\"With an attractive dark finish and included zippered case, Hohner's Piedmont Blues set is the perfect seven-piece starter set for those serious about taking up the harp. Seven great blues harps in an eye-catching matte black finish with gold trim. Click to enlarge. Includes zippered carrying case for your next gig. About the Piedmont Blues Set Piedmont Blues, or East Coast Blues, was born in the American Southeast during the 1900s. Out of its diverse influences arose some of Americas most well known blues harmonica players. What better way to recognize its ongoing influence than with a set of unique harps? The Piedmont Blues set presents the beginning harmonica player with an affordable way to sample playing in seven different keys (G, A, Bb, C, D, E, F). Each harmonica has an eye-catching matte black finish with gold trim. Packaging includes a cardboard sleeve with printed instructions and a key chart to get you started, and the entire set comes in a zippered carrying case for easy transport to your next gig.\"]: 5 (0.1%)\n",
            "\n",
            "Column: store\n",
            "Number of non-null values: 4999\n",
            "Number of unique values: 1573\n",
            "Top 10 most frequent values (out of 1573):\n",
            "1. Fender: 124 (2.5%)\n",
            "2. JIM DUNLOP: 103 (2.1%)\n",
            "3. YAMAHA: 88 (1.8%)\n",
            "4. Pyle: 86 (1.7%)\n",
            "5. Donner: 77 (1.5%)\n",
            "6. Neewer: 67 (1.3%)\n",
            "7. Behringer: 64 (1.3%)\n",
            "8. D'Addario: 61 (1.2%)\n",
            "9. Ernie Ball: 54 (1.1%)\n",
            "10. Musiclily: 43 (0.9%)\n",
            "\n",
            "Column: categories\n",
            "Number of non-null values: 5000\n",
            "Number of unique values: 368\n",
            "Top 10 most frequent values (out of 368):\n",
            "1. ['Musical Instruments', 'Instrument Accessories', 'Guitar & Bass Accessories', 'Picks & Pick Holders', 'Picks']: 168 (3.4%)\n",
            "2. ['Musical Instruments', 'Ukuleles, Mandolins & Banjos', 'Ukuleles']: 130 (2.6%)\n",
            "3. ['Musical Instruments', 'Microphones & Accessories', 'Accessories', 'Stands']: 118 (2.4%)\n",
            "4. ['Musical Instruments', 'Microphones & Accessories', 'Microphones', 'Condenser Microphones', 'Multipurpose']: 113 (2.3%)\n",
            "5. ['Musical Instruments', 'Microphones & Accessories', 'Microphones', 'Wireless Microphones & Systems', 'Handheld Wireless Microphones']: 112 (2.2%)\n",
            "6. ['Musical Instruments', 'Live Sound & Stage', 'Stage & Studio Cables', 'Instrument Cables']: 111 (2.2%)\n",
            "7. ['Musical Instruments', 'Studio Recording Equipment', 'Studio Environment', 'Acoustical Treatments']: 109 (2.2%)\n",
            "8. ['Musical Instruments', 'Live Sound & Stage', 'Stage & Studio Cables', 'Microphone Cables']: 94 (1.9%)\n",
            "9. ['Musical Instruments', 'Instrument Accessories', 'Guitar & Bass Accessories', 'Straps & Strap Locks', 'Straps']: 90 (1.8%)\n",
            "10. []: 83 (1.7%)\n",
            "\n",
            "Column: details\n",
            "Number of non-null values: 5000\n",
            "Number of unique values: 4953\n",
            "Top 10 most frequent values (out of 4953):\n",
            "1. {}: 26 (0.5%)\n",
            "2. {\"Item Weight\": \"2 pounds\", \"Product Dimensions\": \"9.5 x 4 x 2 inches\", \"Item model number\": \"PBH7\", \"Is Discontinued By Manufacturer\": \"No\", \"Date First Available\": \"February 16, 2007\", \"Color Name\": \"Gold\", \"Material Type\": \"Fabric\", \"Instrument Key\": \"G\", \"Size\": \"Multi-Instrument\", \"Color\": \"Gold\", \"Brand\": \"Hohner Accordions\", \"Material\": \"Fabric\", \"Style\": \"American\"}: 4 (0.1%)\n",
            "3. {\"Item Weight\": \"4.44 pounds\", \"Product Dimensions\": \"23 x 3 x 10 inches\", \"Item model number\": \"44BK\", \"Is Discontinued By Manufacturer\": \"No\", \"Date First Available\": \"December 10, 2005\", \"Back Material\": \"Maple\", \"Color Name\": \"Black\", \"Top Material\": \"Maple, Spruce\", \"Number of Strings\": \"4\", \"Material Type\": \"Maple Wood, Pear Wood\", \"Size\": \"4/4\", \"Brand\": \"Crescent\", \"Color\": \"Black\", \"Item Dimensions LxWxH\": \"23 x 3 x 10 inches\", \"Top Material Type\": \"Maple, Spruce\", \"Back Material Type\": \"Maple\", \"Finish Type\": \"Polished\", \"Instrument\": \"Violin\", \"Operation Mode\": \"Manual\"}: 3 (0.1%)\n",
            "4. {\"Item Weight\": \"1.5 pounds\", \"Product Dimensions\": \"9 x 1 x 12 inches\", \"Is Discontinued By Manufacturer\": \"No\", \"Date First Available\": \"November 15, 2012\"}: 3 (0.1%)\n",
            "5. {\"Brand\": \"Jeunesse Global\", \"Flavor\": \"Active\", \"Unit Count\": \"25.5 Fl Oz\", \"Item Weight\": \"12 Pounds\", \"Item Dimensions LxWxH\": \"8 x 12 x 8 inches\", \"Product Benefits\": \"Antioxidant\", \"Package Information\": \"Bottle\", \"Number of Items\": \"4\", \"Dosage Form\": \"Capsule\", \"Is Discontinued By Manufacturer\": \"No\", \"Product Dimensions\": \"8 x 12 x 8 inches; 12 Pounds\", \"Item model number\": \"MonavieActive\", \"Department\": \"Teens-Adults-Unisex\", \"Date First Available\": \"January 24, 2010\", \"Manufacturer\": \"Jeunesse Global\"}: 3 (0.1%)\n",
            "6. {\"Item Weight\": \"9.92 pounds\", \"Product Dimensions\": \"44.5 x 4 x 14.75 inches\", \"Item model number\": \"0310001506\", \"Is Discontinued By Manufacturer\": \"No\", \"Date First Available\": \"October 23, 2008\", \"Back Material\": \"Basswood\", \"Body Material\": \"Wood\", \"Color Name\": \"Black\", \"Fretboard Material\": \"Rosewood\", \"Guitar Pickup Configuration\": \"S\", \"Scale Length\": \"25.5 Inches\", \"String Gauge\": \"Light\", \"String Material\": \"Nickel\", \"Top Material\": \"Alder\", \"Neck Material Type\": \"Maple\", \"Number of Strings\": \"6\", \"Guitar Attribute\": \"Electric\", \"Guitar Bridge System\": \"Tremolo\", \"Material Type\": \"Bass Wood\", \"Instrument Key\": \"C\", \"Size\": \"Guitar\", \"Brand\": \"Fender\", \"Color\": \"Black\", \"Top Material Type\": \"Alder\", \"Back Material Type\": \"Basswood\", \"Fretboard Material Type\": \"Rosewood\", \"String Material Type\": \"Nickel\", \"Hand Orientation\": \"Right\"}: 2 (0.0%)\n",
            "7. {\"Brand\": \"Unknown\", \"Connector Type\": \"USB Type A\", \"Cable Type\": \"USB\", \"Compatible Devices\": \"Laptop, Personal Computer\", \"Connector Gender\": \"Male-to-Female\", \"Shape\": \"Round\", \"Unit Count\": \"1.0 Count\", \"Item Weight\": \"0.32 ounces\", \"Connectivity Technology\": \"USB\", \"Product Dimensions\": \"3 x 1 x 2 inches\", \"Item model number\": \"HDE-A29-NEW\", \"Is Discontinued By Manufacturer\": \"No\", \"Date First Available\": \"April 7, 2008\", \"Color Screen\": \"No\"}: 2 (0.0%)\n",
            "8. {\"Brand\": \"Al Cass\", \"Package Information\": \"Bottle\", \"Liquid Volume\": \"2 Fluid Ounces\", \"Item Weight\": \"2 Ounces\", \"Recommended Uses For Product\": \"Lubricant\", \"Flash Point\": \"144 Degrees Celsius\", \"Item Form\": \"Oil\", \"Product Dimensions\": \"5 x 2 x 2 inches\", \"Item model number\": \"341SG\", \"Is Discontinued By Manufacturer\": \"No\", \"Date First Available\": \"October 2, 2005\"}: 2 (0.0%)\n",
            "9. {\"Compatible Devices\": \"Amplifier\", \"Specific Uses For Product\": \"Microphone\", \"Connector Type\": \"XLR\", \"Connector Gender\": \"Male\", \"Color\": \"1/4 TRS to XLR Female(2Pack)\", \"Is Discontinued By Manufacturer\": \"No\", \"Product Dimensions\": \"5 x 4 x 2 inches; 0.64 Ounces\", \"Item model number\": \"XZT000865\", \"Date First Available\": \"January 2, 2019\", \"Manufacturer\": \"WYMECT\"}: 2 (0.0%)\n",
            "10. {\"Item Weight\": \"0.03 Pounds\", \"Product Dimensions\": \"2.2 x 1 x 12.6 inches\", \"Item model number\": \"701\", \"Is Discontinued By Manufacturer\": \"No\", \"Date First Available\": \"January 16, 2004\", \"Material Type\": \"Metal\", \"Brand\": \"Grover\", \"Material\": \"Metal\", \"Style\": \"Modern\", \"Item Dimensions LxWxH\": \"2.2 x 1 x 12.6 inches\"}: 2 (0.0%)\n",
            "\n",
            "Column: parent_asin\n",
            "Number of non-null values: 5000\n",
            "Number of unique values: 5000\n",
            "Top 10 most frequent values (out of 5000):\n",
            "1. B0BSGM6CQ9: 1 (0.0%)\n",
            "2. B0978Y8NTL: 1 (0.0%)\n",
            "3. B082FGB729: 1 (0.0%)\n",
            "4. B07SXGF77V: 1 (0.0%)\n",
            "5. B07RVB6SCL: 1 (0.0%)\n",
            "6. B01APVUCRY: 1 (0.0%)\n",
            "7. B00MRNXJ06: 1 (0.0%)\n",
            "8. B0BDFXQC71: 1 (0.0%)\n",
            "9. B07XDXKZRX: 1 (0.0%)\n",
            "10. B0B4G6MLSS: 1 (0.0%)\n",
            "\n",
            "==================================================\n",
            "CHALLENGE SAMPLE QUESTIONS ANALYSIS\n",
            "==================================================\n",
            "\n",
            "1. Top 5 highly-rated guitar products:\n",
            "- SimpTronic Tech Estone 1 Set 6 Pack Angled Plug Leads Patch Cables for Guitar Pedal Effect Color Coded\n",
            "  Rating: 5.0 stars (992 reviews)\n",
            "  Price: $nan\n",
            "\n",
            "- Audioblast - 2 Units - 30 Foot - HQ-1 - Ultra Flexible - Dual Shielded (100%) - Guitar Instrument Effects Pedal Patch Cable w/Neutrik-Rean NYS224BG Gold ¼ inch (6.35mm) TS Plugs & Triple Boots\n",
            "  Rating: 5.0 stars (839 reviews)\n",
            "  Price: $nan\n",
            "\n",
            "- Jim Dunlop Tortex Standard 1.14mm Purple Guitar Picks-36 Pack (418B1.14)\n",
            "  Rating: 4.9 stars (4367 reviews)\n",
            "  Price: $16.47\n",
            "\n",
            "- Buckle-Down Guitar Strap - MARVEL/Retro Comic Panels Black/Yellow - 2\" Wide - 29-54\" Length\n",
            "  Rating: 4.9 stars (1594 reviews)\n",
            "  Price: $24.95\n",
            "\n",
            "- String Swing Guitar Hanger Holder for Electric Acoustic and Bass Guitars Stand Accessories Home or Studio Wall - Musical Instruments Safe without Hard Cases Oak Hardwood CC01K-O 2-Pack\n",
            "  Rating: 4.9 stars (882 reviews)\n",
            "  Price: $28.98\n",
            "\n",
            "\n",
            "2. Products for thin guitar strings:\n",
            "- Elixir Strings 80/20 Bronze Acoustic Guitar Strings w NANOWEB Coating, Medium (.013-.056)\n",
            "  Rating: 4.8 stars (29394 reviews)\n",
            "  Price: $15.99\n",
            "\n",
            "- Elixir® Strings 16545 Acoustic Phosphor Bronze Guitar Strings with NANOWEB® Coating, 3 Pack, Light (.012-.053)\n",
            "  Rating: 4.8 stars (26373 reviews)\n",
            "  Price: $42.98\n",
            "\n",
            "- D'Addario Bass Guitar Strings - XL Nickel Bass Strings - EXL165SL - Perfect Intonation, Feel, Durability - For 4 String Bass Guitars - 45-105 Regular Light Top/Medium Bottom, Super Long Scale\n",
            "  Rating: 4.8 stars (8309 reviews)\n",
            "  Price: $25.56\n",
            "\n",
            "\n",
            "3. BOYA BYM1 Microphone for cello:\n",
            "- Boya BYM1 by Shotgun Video Microphone by-M1 Ultimate 3.5mm Lapel Mic Clip-On Video Recording Omnidirectional Condenser for iPhone Android Smartphone Mac Tablet DSLR Camcorder, Black\n",
            "  Rating: 4.0 stars (68708 reviews)\n",
            "  Price: $14.95\n",
            "  Features: ['Our lavalier microphones can create perfect videos and audio files on your smartphone and tablet. Pristine sound without effort, no matter where you are!', 'Pick up sounds from your environment easily. Ideal for YouTuber, Interviews, Livestreams, Podcasting, Demo videos, Voice dictation, Instagram Live & More.', 'Compatible with Apple/ iPhone, iPad, Android & Windows Smartphones, tablets. Also works with DSLR, camcorders, Gopro, camcorders etc. with standard 3.5 mm (1/8 inch)mic input.', 'It comes with a deluxe pouch to keep the microphone safe, a special wind muff, a durable lapel clip, the longest cord at 20feet (6m),LR44 battery and a 1/4” adapter.', 'For Smartphone: switch off the microphone, slide the ON/OFF up to OFF/Smartphone, the power is shut down.']\n",
            "  Description: ['Specification Transducer Type: Electret Condenser Polar Pattern: Omni-directional Frequency Range: 65Hz ~ 18KHz Signal / Noise: 74dB SPL Sensitivity: -30dB +/-3dB / 0dB=1V/Pa, 1kHz Output Impedance: 1000 Ohm or less Output Connection: 3.5mm (1/8\") 4-pole gold plug Battery Type: LR44 (included) Dimension: Microphone :18.00mmH x 8.30mmW x 8.30mm Audio Cable: 20.00ft (6.00m) Weight: Microphone : 2.50g;Power Module: 18.00g']\n",
            "\n",
            "This microphone appears to be primarily designed for voice recording and interviews, not specifically for musical instruments like cellos.\n",
            "- Boya BY-M1-PRO Premium Universal Omnidirectional Lavalier Microphone for Cameras, Smartphones, Tablets, Computers, Recorders & More, Black\n",
            "  Rating: 3.8 stars (2217 reviews)\n",
            "  Price: $nan\n",
            "  Features: ['Premium Lavalier Microphone with 3.5mm TRS/TRRS output for Cameras, Smartphone, Tablets, Computers, Mixers, Recorders and more', '3.5mm Headphone Output for Real-Time Monitoring with Smartphones, Tablets and Computers', 'Ideal for YouTube videos, Interviews, Vloggers, Social Media videos and Content Creators of all kinds', 'Omnidirectional pick-up pattern captures sound equally in every direction for rich, natural sounding, broadcast-quality sound', 'Foam Windscreen, Mic Clip, 1/4\" (6.45mm) Adapter, LR44 Battery, Cable Tie and Carry Pouch Included']\n",
            "  Description: []\n",
            "\n",
            "This microphone appears to be primarily designed for voice recording and interviews, not specifically for musical instruments like cellos.\n",
            "\n",
            "4. Customer ID 37077's last order:\n",
            "Order Date: 2018-01-02\n",
            "Product: Car Media Players\n",
            "Sales Amount: $140.0\n",
            "Shipping Cost: $4.6\n",
            "Order Priority: Medium\n",
            "\n",
            "5. 5 most recent Critical priority orders:\n",
            "8377. Date: 2018-12-30\n",
            "   Customer ID: 39758\n",
            "   Product: Suits\n",
            "   Sales: $109.0\n",
            "   Shipping Cost: $2.4\n",
            "\n",
            "11103. Date: 2018-12-30\n",
            "   Customer ID: 34789\n",
            "   Product: Shirts\n",
            "   Sales: $196.0\n",
            "   Shipping Cost: $9.6\n",
            "\n",
            "18001. Date: 2018-12-30\n",
            "   Customer ID: 59098\n",
            "   Product: LED\n",
            "   Sales: $192.0\n",
            "   Shipping Cost: $9.5\n",
            "\n",
            "20024. Date: 2018-12-30\n",
            "   Customer ID: 48215\n",
            "   Product: Samsung Mobile\n",
            "   Sales: $220.0\n",
            "   Shipping Cost: $13.3\n",
            "\n",
            "2039. Date: 2018-12-30\n",
            "   Customer ID: 26306\n",
            "   Product: Tyre\n",
            "   Sales: $250.0\n",
            "   Shipping Cost: $13.3\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_15904\\733296925.py:244: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  customer_orders['Order_Date'] = pd.to_datetime(customer_orders['Order_Date'])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "def analyze_distribution_by_type(dataframe, title):\n",
        "    \"\"\"\n",
        "    Analyze the distribution of each column according to its data type\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"DISTRIBUTION ANALYSIS FOR {title}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Get column types\n",
        "    numeric_cols = dataframe.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_cols = dataframe.select_dtypes(include=['object']).columns.tolist()\n",
        "    date_cols = [col for col in categorical_cols if 'date' in col.lower() or 'time' in col.lower()]\n",
        "    categorical_cols = [col for col in categorical_cols if col not in date_cols]\n",
        "\n",
        "    # Analyze numeric columns\n",
        "    print(f\"\\n{'-'*20} NUMERIC COLUMNS {'-'*20}\")\n",
        "    for col in numeric_cols:\n",
        "        data = dataframe[col].dropna()\n",
        "\n",
        "        # Calculate distribution statistics\n",
        "        min_val = data.min()\n",
        "        max_val = data.max()\n",
        "        mean_val = data.mean()\n",
        "        median_val = data.median()\n",
        "        std_val = data.std()\n",
        "\n",
        "        # Calculate percentiles\n",
        "        p10 = data.quantile(0.1)\n",
        "        q1 = data.quantile(0.25)\n",
        "        q3 = data.quantile(0.75)\n",
        "        p90 = data.quantile(0.9)\n",
        "        iqr = q3 - q1\n",
        "\n",
        "        # Identify outliers\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "        outlier_percent = len(outliers) / len(data) * 100\n",
        "\n",
        "        # Calculate distribution shape\n",
        "        skewness = data.skew()\n",
        "        kurtosis = data.kurt()\n",
        "\n",
        "        # Create a text-based histogram (10 bins)\n",
        "        hist_bins = pd.cut(data, bins=10).value_counts().sort_index()\n",
        "        total_count = hist_bins.sum()\n",
        "        max_bar_length = 50  # Maximum number of characters for the bar\n",
        "\n",
        "        print(f\"\\nColumn: {col}\")\n",
        "        print(f\"Number of non-null values: {len(data)}\")\n",
        "        print(f\"Min: {min_val:.4f}, Max: {max_val:.4f}, Range: {max_val - min_val:.4f}\")\n",
        "        print(f\"Mean: {mean_val:.4f}, Median: {median_val:.4f}\")\n",
        "        print(f\"Standard Deviation: {std_val:.4f}\")\n",
        "        print(f\"10th percentile: {p10:.4f}, 25th percentile: {q1:.4f}\")\n",
        "        print(f\"75th percentile: {q3:.4f}, 90th percentile: {p90:.4f}\")\n",
        "        print(f\"Interquartile Range (IQR): {iqr:.4f}\")\n",
        "        print(f\"Outliers: {len(outliers)} ({outlier_percent:.2f}% of data)\")\n",
        "        print(f\"Skewness: {skewness:.4f} ({'Symmetric' if abs(skewness) < 0.5 else 'Moderately skewed' if abs(skewness) < 1 else 'Highly skewed'})\")\n",
        "        print(f\"Kurtosis: {kurtosis:.4f} ({'Normal' if abs(kurtosis) < 3 else 'Heavy-tailed' if kurtosis > 3 else 'Light-tailed'})\")\n",
        "\n",
        "        print(\"\\nDistribution (Text Histogram):\")\n",
        "        print(f\"Range: {min_val:.4f} to {max_val:.4f}\")\n",
        "        for bin_range, count in hist_bins.items():\n",
        "            percentage = count / total_count\n",
        "            bar_length = int(percentage * max_bar_length)\n",
        "            bar = \"█\" * bar_length\n",
        "            print(f\"{bin_range} | {count:5d} {bar} {percentage:.1%}\")\n",
        "\n",
        "    # Analyze categorical columns\n",
        "    print(f\"\\n{'-'*20} CATEGORICAL COLUMNS {'-'*20}\")\n",
        "    for col in categorical_cols:\n",
        "        data = dataframe[col].dropna()\n",
        "        value_counts = data.value_counts()\n",
        "        total_count = len(data)\n",
        "\n",
        "        print(f\"\\nColumn: {col}\")\n",
        "        print(f\"Number of non-null values: {len(data)}\")\n",
        "        print(f\"Number of unique values: {data.nunique()}\")\n",
        "\n",
        "        # For columns with many unique values, show just the top ones\n",
        "        if data.nunique() > 10:\n",
        "            print(f\"Top 10 most frequent values (out of {data.nunique()}):\")\n",
        "            for i, (value, count) in enumerate(value_counts.head(10).items()):\n",
        "                percentage = count / total_count\n",
        "                print(f\"{i+1}. {value}: {count} ({percentage:.1%})\")\n",
        "        else:\n",
        "            print(\"Value distribution:\")\n",
        "            max_bar_length = 40  # Maximum number of characters for the bar\n",
        "            for value, count in value_counts.items():\n",
        "                percentage = count / total_count\n",
        "                bar_length = int(percentage * max_bar_length)\n",
        "                bar = \"█\" * bar_length\n",
        "                print(f\"{value}: {count:5d} {bar} {percentage:.1%}\")\n",
        "\n",
        "    # Analyze date columns\n",
        "    if date_cols:\n",
        "        print(f\"\\n{'-'*20} DATE COLUMNS {'-'*20}\")\n",
        "        for col in date_cols:\n",
        "            print(f\"\\nColumn: {col}\")\n",
        "            try:\n",
        "                # Try to convert to datetime\n",
        "                dates = pd.to_datetime(dataframe[col])\n",
        "\n",
        "                # Extract datetime components\n",
        "                if 'time' in col.lower():\n",
        "                    # For time analysis\n",
        "                    hour_counts = dates.dt.hour.value_counts().sort_index()\n",
        "                    total_hours = len(dates)\n",
        "\n",
        "                    print(f\"Time analysis:\")\n",
        "                    print(f\"Earliest time: {dates.min().time()}\")\n",
        "                    print(f\"Latest time: {dates.max().time()}\")\n",
        "\n",
        "                    print(\"\\nDistribution by hour:\")\n",
        "                    max_bar_length = 40\n",
        "                    for hour, count in hour_counts.items():\n",
        "                        percentage = count / total_hours\n",
        "                        bar_length = int(percentage * max_bar_length)\n",
        "                        bar = \"█\" * bar_length\n",
        "                        print(f\"{hour:02d}:00: {count:5d} {bar} {percentage:.1%}\")\n",
        "                else:\n",
        "                    # For date analysis\n",
        "                    print(f\"Date analysis:\")\n",
        "                    print(f\"Date range: {dates.min().date()} to {dates.max().date()}\")\n",
        "                    print(f\"Timespan: {(dates.max() - dates.min()).days} days\")\n",
        "\n",
        "                    # Distribution by month and year\n",
        "                    monthly_counts = dates.dt.to_period('M').value_counts().sort_index()\n",
        "                    print(\"\\nDistribution by month:\")\n",
        "                    for period, count in monthly_counts.items():\n",
        "                        print(f\"{period}: {count}\")\n",
        "\n",
        "                    # Distribution by day of week\n",
        "                    weekday_counts = dates.dt.day_name().value_counts()\n",
        "                    total_days = len(dates)\n",
        "\n",
        "                    print(\"\\nDistribution by day of week:\")\n",
        "                    for day, count in weekday_counts.items():\n",
        "                        percentage = count / total_days\n",
        "                        print(f\"{day}: {count} ({percentage:.1%})\")\n",
        "            except:\n",
        "                print(f\"Could not convert {col} to datetime format.\")\n",
        "                # Treat as categorical\n",
        "                data = dataframe[col].dropna()\n",
        "                value_counts = data.value_counts()\n",
        "                total_count = len(data)\n",
        "\n",
        "                print(f\"Treating as categorical column:\")\n",
        "                print(f\"Number of unique values: {data.nunique()}\")\n",
        "\n",
        "                if data.nunique() > 10:\n",
        "                    print(f\"Top 10 most frequent values (out of {data.nunique()}):\")\n",
        "                    for i, (value, count) in enumerate(value_counts.head(10).items()):\n",
        "                        percentage = count / total_count\n",
        "                        print(f\"{i+1}. {value}: {count} ({percentage:.1%})\")\n",
        "                else:\n",
        "                    print(\"Value distribution:\")\n",
        "                    for value, count in value_counts.items():\n",
        "                        percentage = count / total_count\n",
        "                        print(f\"{value}: {count} ({percentage:.1%})\")\n",
        "\n",
        "# Analyze both datasets\n",
        "analyze_distribution_by_type(order_data, \"ORDER DATA\")\n",
        "analyze_distribution_by_type(product_data, \"PRODUCT DATA\")\n",
        "\n",
        "# Additional analyses for specific questions from the challenge\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CHALLENGE SAMPLE QUESTIONS ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Top rated guitar products\n",
        "print(\"\\n1. Top 5 highly-rated guitar products:\")\n",
        "guitar_products = product_data[\n",
        "    (product_data['title'].str.contains('guitar', case=False, na=False)) |\n",
        "    (product_data['categories'].str.contains('guitar', case=False, na=False))\n",
        "]\n",
        "top_guitars = guitar_products.sort_values(by=['average_rating', 'rating_number'], ascending=[False, False]).head(5)\n",
        "for idx, row in top_guitars.iterrows():\n",
        "    print(f\"- {row['title']}\")\n",
        "    print(f\"  Rating: {row['average_rating']} stars ({row['rating_number']} reviews)\")\n",
        "    print(f\"  Price: ${row['price']}\")\n",
        "    print()\n",
        "\n",
        "# 2. Check for thin guitar strings\n",
        "print(\"\\n2. Products for thin guitar strings:\")\n",
        "thin_strings = product_data[\n",
        "    (product_data['title'].str.contains('guitar string', case=False, na=False)) &\n",
        "    (\n",
        "        (product_data['title'].str.contains('thin', case=False, na=False)) |\n",
        "        (product_data['title'].str.contains('light', case=False, na=False)) |\n",
        "        (product_data['features'].str.contains('thin', case=False, na=False)) |\n",
        "        (product_data['description'].str.contains('thin', case=False, na=False))\n",
        "    )\n",
        "].sort_values(by=['average_rating', 'rating_number'], ascending=[False, False]).head(3)\n",
        "\n",
        "for idx, row in thin_strings.iterrows():\n",
        "    print(f\"- {row['title']}\")\n",
        "    print(f\"  Rating: {row['average_rating']} stars ({row['rating_number']} reviews)\")\n",
        "    print(f\"  Price: ${row['price']}\")\n",
        "    print()\n",
        "\n",
        "# 3. Check for BOYA BYM1 Microphone\n",
        "print(\"\\n3. BOYA BYM1 Microphone for cello:\")\n",
        "boya_mic = product_data[\n",
        "    product_data['title'].str.contains('boya', case=False, na=False) &\n",
        "    (\n",
        "        product_data['title'].str.contains('bym1', case=False, na=False) |\n",
        "        product_data['title'].str.contains('by-m1', case=False, na=False) |\n",
        "        product_data['title'].str.contains('by m1', case=False, na=False)\n",
        "    )\n",
        "]\n",
        "\n",
        "if not boya_mic.empty:\n",
        "    for idx, row in boya_mic.iterrows():\n",
        "        print(f\"- {row['title']}\")\n",
        "        print(f\"  Rating: {row['average_rating']} stars ({row['rating_number']} reviews)\")\n",
        "        print(f\"  Price: ${row['price']}\")\n",
        "        print(f\"  Features: {row['features']}\")\n",
        "        print(f\"  Description: {row['description']}\")\n",
        "\n",
        "        # Check if good for cello\n",
        "        features_and_desc = str(row['features']).lower() + ' ' + str(row['description']).lower()\n",
        "        if 'cello' in features_and_desc or ('instrument' in features_and_desc and 'music' in features_and_desc):\n",
        "            print(\"\\nThis microphone appears to be suitable for cello recording.\")\n",
        "        else:\n",
        "            print(\"\\nThis microphone appears to be primarily designed for voice recording and interviews, not specifically for musical instruments like cellos.\")\n",
        "else:\n",
        "    print(\"BOYA BYM1 Microphone not found in dataset\")\n",
        "\n",
        "# 4. Customer order details\n",
        "print(\"\\n4. Customer ID 37077's last order:\")\n",
        "customer_id = 37077\n",
        "customer_orders = order_data[order_data['Customer_Id'] == customer_id]\n",
        "if not customer_orders.empty:\n",
        "    # Convert to datetime and sort\n",
        "    customer_orders['Order_Date'] = pd.to_datetime(customer_orders['Order_Date'])\n",
        "    last_order = customer_orders.sort_values('Order_Date', ascending=False).iloc[0]\n",
        "\n",
        "    print(f\"Order Date: {last_order['Order_Date'].strftime('%Y-%m-%d')}\")\n",
        "    print(f\"Product: {last_order['Product']}\")\n",
        "    print(f\"Sales Amount: ${last_order['Sales']}\")\n",
        "    print(f\"Shipping Cost: ${last_order['Shipping_Cost']}\")\n",
        "    print(f\"Order Priority: {last_order['Order_Priority']}\")\n",
        "else:\n",
        "    print(f\"No orders found for customer {customer_id}\")\n",
        "\n",
        "# 5. Recent high-priority orders\n",
        "print(\"\\n5. 5 most recent Critical priority orders:\")\n",
        "# Convert to datetime\n",
        "order_data['Order_Date'] = pd.to_datetime(order_data['Order_Date'])\n",
        "critical_orders = order_data[order_data['Order_Priority'] == 'Critical']\n",
        "recent_critical = critical_orders.sort_values('Order_Date', ascending=False).head(5)\n",
        "\n",
        "for idx, order in recent_critical.iterrows():\n",
        "    print(f\"{idx + 1}. Date: {order['Order_Date'].strftime('%Y-%m-%d')}\")\n",
        "    print(f\"   Customer ID: {order['Customer_Id']}\")\n",
        "    print(f\"   Product: {order['Product']}\")\n",
        "    print(f\"   Sales: ${order['Sales']}\")\n",
        "    print(f\"   Shipping Cost: ${order['Shipping_Cost']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a70969",
      "metadata": {
        "id": "d0a70969"
      },
      "source": [
        "# Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c82eed7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c82eed7a",
        "outputId": "8f5d7709-3f48-4aa3-9650-9381a68d5155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "INITIAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Missing Values:\n",
            "Aging             1\n",
            "Sales             1\n",
            "Quantity          2\n",
            "Discount          1\n",
            "Shipping_Cost     1\n",
            "Order_Priority    2\n",
            "dtype: int64\n",
            "Total missing values: 8\n",
            "\n",
            "Product Data Missing Values:\n",
            "main_category      30\n",
            "price            1266\n",
            "store               1\n",
            "dtype: int64\n",
            "Total missing values: 1297\n",
            "\n",
            "Imputing Order Data...\n",
            "Processing column: Aging\n",
            "  Filled Aging with median: 5.0\n",
            "Processing column: Sales\n",
            "  Filled Sales with median: 133.0\n",
            "Processing column: Quantity\n",
            "  Filled Quantity with median: 2.0\n",
            "Processing column: Discount\n",
            "  Filled Discount with median: 0.3\n",
            "Processing column: Shipping_Cost\n",
            "  Filled Shipping_Cost with median: 6.0\n",
            "Processing column: Order_Priority\n",
            "  Filled Order_Priority with mode: Medium\n",
            "\n",
            "Imputing Product Data...\n",
            "Processing column: main_category\n",
            "  Filled main_category with mode: Musical Instruments\n",
            "Processing column: price\n",
            "  Filled price with median: 25.924999999999997\n",
            "Processing column: store\n",
            "  Filled store with mode: Fender\n",
            "\n",
            "FINAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Remaining Missing Values:\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Order Data!\n",
            "\n",
            "Product Data Remaining Missing Values:\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Product Data!\n",
            "\n",
            "Saving clean datasets...\n",
            "Clean datasets saved as 'Order_Data_Dataset_Clean.csv' and 'Product_Information_Dataset_Clean.csv'\n",
            "\n",
            "Imputation complete! All datasets should have zero missing values.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])  # Only show columns with missing values\n",
        "print(f\"Total missing values: {order_data.isnull().sum().sum()}\")\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])  # Only show columns with missing values\n",
        "print(f\"Total missing values: {product_data.isnull().sum().sum()}\")\n",
        "\n",
        "# Simple imputation for Order Data\n",
        "print(\"\\nImputing Order Data...\")\n",
        "for column in order_data.columns:\n",
        "    # Skip columns that don't have missing values\n",
        "    if order_data[column].isnull().sum() == 0:\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing column: {column}\")\n",
        "\n",
        "    # Numeric columns\n",
        "    if pd.api.types.is_numeric_dtype(order_data[column]):\n",
        "        # Use median for all numeric columns\n",
        "        median_value = order_data[column].median()\n",
        "        order_data[column] = order_data[column].fillna(median_value)\n",
        "        print(f\"  Filled {column} with median: {median_value}\")\n",
        "\n",
        "    # Categorical/Object columns\n",
        "    else:\n",
        "        # Use mode (most frequent value)\n",
        "        if len(order_data[column].dropna()) > 0:\n",
        "            mode_value = order_data[column].mode()[0]\n",
        "            order_data[column] = order_data[column].fillna(mode_value)\n",
        "            print(f\"  Filled {column} with mode: {mode_value}\")\n",
        "        else:\n",
        "            # If all values are NaN, use a placeholder\n",
        "            order_data[column] = order_data[column].fillna(f\"Unknown {column}\")\n",
        "            print(f\"  Filled {column} with placeholder: 'Unknown {column}'\")\n",
        "\n",
        "# Simple imputation for Product Data\n",
        "print(\"\\nImputing Product Data...\")\n",
        "for column in product_data.columns:\n",
        "    # Skip columns that don't have missing values\n",
        "    if product_data[column].isnull().sum() == 0:\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing column: {column}\")\n",
        "\n",
        "    # Special handling for descriptive text fields\n",
        "    if column in ['description', 'features', 'details']:\n",
        "        placeholder = f\"No {column} available\"\n",
        "        product_data[column] = product_data[column].fillna(placeholder)\n",
        "        print(f\"  Filled {column} with placeholder: '{placeholder}'\")\n",
        "        continue\n",
        "\n",
        "    # Special handling for title\n",
        "    if column == 'title':\n",
        "        product_data[column] = product_data[column].fillna(\"Unnamed Product\")\n",
        "        print(f\"  Filled {column} with 'Unnamed Product'\")\n",
        "        continue\n",
        "\n",
        "    # Numeric columns\n",
        "    if pd.api.types.is_numeric_dtype(product_data[column]):\n",
        "        # Use median for all numeric columns\n",
        "        median_value = product_data[column].median()\n",
        "        product_data[column] = product_data[column].fillna(median_value)\n",
        "        print(f\"  Filled {column} with median: {median_value}\")\n",
        "\n",
        "    # Categorical/Object columns\n",
        "    else:\n",
        "        # Use mode (most frequent value)\n",
        "        if len(product_data[column].dropna()) > 0:\n",
        "            mode_value = product_data[column].mode()[0]\n",
        "            product_data[column] = product_data[column].fillna(mode_value)\n",
        "            print(f\"  Filled {column} with mode: {mode_value}\")\n",
        "        else:\n",
        "            # If all values are NaN, use a placeholder\n",
        "            product_data[column] = product_data[column].fillna(f\"Unknown {column}\")\n",
        "            print(f\"  Filled {column} with placeholder: 'Unknown {column}'\")\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "\n",
        "print(\"\\nOrder Data Remaining Missing Values:\")\n",
        "order_remaining = order_data.isnull().sum()\n",
        "print(order_remaining[order_remaining > 0])\n",
        "if order_data.isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Order Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {order_data.isnull().sum().sum()} missing values remain in Order Data\")\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values:\")\n",
        "product_remaining = product_data.isnull().sum()\n",
        "print(product_remaining[product_remaining > 0])\n",
        "if product_data.isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Product Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {product_data.isnull().sum().sum()} missing values remain in Product Data\")\n",
        "\n",
        "# Save the clean datasets\n",
        "print(\"\\nSaving clean datasets...\")\n",
        "order_data.to_csv('Order_Data_Dataset_Clean.csv', index=False)\n",
        "product_data.to_csv('Product_Information_Dataset_Clean.csv', index=False)\n",
        "print(\"Clean datasets saved as 'Order_Data_Dataset_Clean.csv' and 'Product_Information_Dataset_Clean.csv'\")\n",
        "\n",
        "print(\"\\nImputation complete! All datasets should have zero missing values.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the datasets\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Check missing values before imputation\n",
        "print(\"Missing values before imputation:\")\n",
        "print(\"\\nOrder Data:\")\n",
        "print(order_data.isnull().sum())\n",
        "print(\"\\nProduct Data:\")\n",
        "print(product_data.isnull().sum())\n",
        "\n",
        "# Impute missing values in Order Data\n",
        "def impute_order_data(df):\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # For numeric columns: use median for financial values, mean for others\n",
        "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    # Financial columns (use median to reduce impact of outliers)\n",
        "    financial_cols = ['Sales', 'Profit', 'Shipping_Cost', 'Discount']\n",
        "    financial_cols = [col for col in financial_cols if col in numeric_cols]\n",
        "\n",
        "    # Other numeric columns (use mean)\n",
        "    other_numeric_cols = [col for col in numeric_cols if col not in financial_cols]\n",
        "\n",
        "    # Apply imputation\n",
        "    if financial_cols:\n",
        "        median_imputer = SimpleImputer(strategy='median')\n",
        "        df_imputed[financial_cols] = median_imputer.fit_transform(df[financial_cols])\n",
        "\n",
        "    if other_numeric_cols:\n",
        "        mean_imputer = SimpleImputer(strategy='mean')\n",
        "        df_imputed[other_numeric_cols] = mean_imputer.fit_transform(df[other_numeric_cols])\n",
        "\n",
        "    # For categorical columns: use most frequent value\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    if len(categorical_cols) > 0:\n",
        "        mode_imputer = SimpleImputer(strategy='most_frequent')\n",
        "        df_imputed[categorical_cols] = mode_imputer.fit_transform(df[categorical_cols])\n",
        "\n",
        "    # For date columns: forward fill or use most recent date\n",
        "    date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
        "    for col in date_cols:\n",
        "        if col in df.columns and df[col].isnull().any():\n",
        "            # Try to convert to datetime first\n",
        "            try:\n",
        "                df_imputed[col] = pd.to_datetime(df_imputed[col])\n",
        "                # Forward fill (use previous value)\n",
        "                df_imputed[col] = df_imputed[col].fillna(method='ffill')\n",
        "                # If still NA at the beginning, use the next available date\n",
        "                df_imputed[col] = df_imputed[col].fillna(method='bfill')\n",
        "            except:\n",
        "                # If date conversion fails, treat as categorical\n",
        "                df_imputed[col] = df_imputed[col].fillna(df_imputed[col].mode()[0])\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# Impute missing values in Product Data\n",
        "def impute_product_data(df):\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # For product ratings: use median (less affected by outliers)\n",
        "    if 'average_rating' in df.columns:\n",
        "        df_imputed['average_rating'] = df_imputed['average_rating'].fillna(df_imputed['average_rating'].median())\n",
        "\n",
        "    if 'rating_number' in df.columns:\n",
        "        df_imputed['rating_number'] = df_imputed['rating_number'].fillna(df_imputed['rating_number'].median())\n",
        "\n",
        "    # For price: use median by category if possible, otherwise overall median\n",
        "    if 'price' in df.columns:\n",
        "        if 'main_category' in df.columns:\n",
        "            # Group by category and fill with category median\n",
        "            category_median = df.groupby('main_category')['price'].transform('median')\n",
        "            df_imputed['price'] = df_imputed['price'].fillna(category_median)\n",
        "            # If still missing, use overall median\n",
        "            df_imputed['price'] = df_imputed['price'].fillna(df_imputed['price'].median())\n",
        "        else:\n",
        "            df_imputed['price'] = df_imputed['price'].fillna(df_imputed['price'].median())\n",
        "\n",
        "    # For text fields: use special placeholders that make sense in context\n",
        "    if 'description' in df.columns:\n",
        "        df_imputed['description'] = df_imputed['description'].fillna(\"No description available\")\n",
        "\n",
        "    if 'features' in df.columns:\n",
        "        df_imputed['features'] = df_imputed['features'].fillna(\"Features not specified\")\n",
        "\n",
        "    if 'title' in df.columns:\n",
        "        # Title should never be missing for products, but if it is, use a placeholder\n",
        "        df_imputed['title'] = df_imputed['title'].fillna(\"Unnamed Product\")\n",
        "\n",
        "    # For other categorical fields, use most frequent value\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    categorical_cols = [col for col in categorical_cols if col not in ['title', 'description', 'features']]\n",
        "\n",
        "    if categorical_cols:\n",
        "        for col in categorical_cols:\n",
        "            df_imputed[col] = df_imputed[col].fillna(df_imputed[col].mode()[0])\n",
        "\n",
        "    # For remaining numeric columns, use median\n",
        "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    numeric_cols = [col for col in numeric_cols if col not in ['average_rating', 'rating_number', 'price']]\n",
        "\n",
        "    if numeric_cols:\n",
        "        for col in numeric_cols:\n",
        "            df_imputed[col] = df_imputed[col].fillna(df_imputed[col].median())\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# Apply imputation\n",
        "order_data_imputed = impute_order_data(order_data)\n",
        "product_data_imputed = impute_product_data(product_data)\n",
        "\n",
        "# Check missing values after imputation\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(\"\\nOrder Data:\")\n",
        "print(order_data_imputed.isnull().sum())\n",
        "print(\"\\nProduct Data:\")\n",
        "print(product_data_imputed.isnull().sum())\n",
        "\n",
        "# Save imputed datasets\n",
        "order_data_imputed.to_csv('Order_Data_Dataset_Imputed.csv', index=False)\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_Imputed.csv', index=False)\n",
        "\n",
        "print(\"\\nImputed datasets saved as 'Order_Data_Dataset_Imputed.csv' and 'Product_Information_Dataset_Imputed.csv'\")\n",
        "\n",
        "# Analyze imputation impact\n",
        "print(\"\\nImputation impact analysis:\")\n",
        "\n",
        "print(\"\\nOrder Data numeric columns (before vs after):\")\n",
        "numeric_order_cols = order_data.select_dtypes(include=['int64', 'float64']).columns\n",
        "for col in numeric_order_cols:\n",
        "    before_mean = order_data[col].mean()\n",
        "    after_mean = order_data_imputed[col].mean()\n",
        "    before_median = order_data[col].median()\n",
        "    after_median = order_data_imputed[col].median()\n",
        "    print(f\"{col}: Mean changed from {before_mean:.2f} to {after_mean:.2f}, Median from {before_median:.2f} to {after_median:.2f}\")\n",
        "\n",
        "print(\"\\nProduct Data numeric columns (before vs after):\")\n",
        "numeric_product_cols = product_data.select_dtypes(include=['int64', 'float64']).columns\n",
        "for col in numeric_product_cols:\n",
        "    before_mean = product_data[col].mean()\n",
        "    after_mean = product_data_imputed[col].mean()\n",
        "    before_median = product_data[col].median()\n",
        "    after_median = product_data_imputed[col].median()\n",
        "    print(f\"{col}: Mean changed from {before_mean:.2f} to {after_mean:.2f}, Median from {before_median:.2f} to {after_median:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gpv47mEtzA9",
        "outputId": "4801027f-d677-48d7-ba0b-5e874049d042"
      },
      "id": "9gpv47mEtzA9",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values before imputation:\n",
            "\n",
            "Order Data:\n",
            "Order_Date             0\n",
            "Time                   0\n",
            "Aging                  1\n",
            "Customer_Id            0\n",
            "Gender                 0\n",
            "Device_Type            0\n",
            "Customer_Login_type    0\n",
            "Product_Category       0\n",
            "Product                0\n",
            "Sales                  1\n",
            "Quantity               2\n",
            "Discount               1\n",
            "Profit                 0\n",
            "Shipping_Cost          1\n",
            "Order_Priority         2\n",
            "Payment_method         0\n",
            "dtype: int64\n",
            "\n",
            "Product Data:\n",
            "main_category       30\n",
            "title                0\n",
            "average_rating       0\n",
            "rating_number        0\n",
            "features             0\n",
            "description          0\n",
            "price             1266\n",
            "store                1\n",
            "categories           0\n",
            "details              0\n",
            "parent_asin          0\n",
            "dtype: int64\n",
            "\n",
            "Missing values after imputation:\n",
            "\n",
            "Order Data:\n",
            "Order_Date             0\n",
            "Time                   0\n",
            "Aging                  0\n",
            "Customer_Id            0\n",
            "Gender                 0\n",
            "Device_Type            0\n",
            "Customer_Login_type    0\n",
            "Product_Category       0\n",
            "Product                0\n",
            "Sales                  0\n",
            "Quantity               0\n",
            "Discount               0\n",
            "Profit                 0\n",
            "Shipping_Cost          0\n",
            "Order_Priority         0\n",
            "Payment_method         0\n",
            "dtype: int64\n",
            "\n",
            "Product Data:\n",
            "main_category     0\n",
            "title             0\n",
            "average_rating    0\n",
            "rating_number     0\n",
            "features          0\n",
            "description       0\n",
            "price             0\n",
            "store             0\n",
            "categories        0\n",
            "details           0\n",
            "parent_asin       0\n",
            "dtype: int64\n",
            "\n",
            "Imputed datasets saved as 'Order_Data_Dataset_Imputed.csv' and 'Product_Information_Dataset_Imputed.csv'\n",
            "\n",
            "Imputation impact analysis:\n",
            "\n",
            "Order Data numeric columns (before vs after):\n",
            "Aging: Mean changed from 5.26 to 5.26, Median from 5.00 to 5.00\n",
            "Customer_Id: Mean changed from 58155.76 to 58155.76, Median from 61018.00 to 61018.00\n",
            "Sales: Mean changed from 152.34 to 152.34, Median from 133.00 to 133.00\n",
            "Quantity: Mean changed from 2.50 to 2.50, Median from 2.00 to 2.00\n",
            "Discount: Mean changed from 0.30 to 0.30, Median from 0.30 to 0.30\n",
            "Profit: Mean changed from 70.41 to 70.41, Median from 59.90 to 59.90\n",
            "Shipping_Cost: Mean changed from 7.04 to 7.04, Median from 6.00 to 6.00\n",
            "\n",
            "Product Data numeric columns (before vs after):\n",
            "average_rating: Mean changed from 4.49 to 4.49, Median from 4.50 to 4.50\n",
            "rating_number: Mean changed from 1876.23 to 1876.23, Median from 963.00 to 963.00\n",
            "price: Mean changed from 56.51 to 48.82, Median from 25.92 to 25.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589fd80e",
      "metadata": {
        "id": "589fd80e",
        "outputId": "adf2f088-9ad2-4b6d-ec71-aad7962ce21e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Process one dataset at a time\n",
        "# Order Data\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "print(f\"Order Data initial missing values: {order_data.isnull().sum().sum()}\")\n",
        "\n",
        "# Fill numeric columns with median values\n",
        "for col in order_data.select_dtypes(include=['number']).columns:\n",
        "    order_data[col] = order_data[col].fillna(order_data[col].median())\n",
        "\n",
        "# Fill object columns with mode values\n",
        "for col in order_data.select_dtypes(include=['object']).columns:\n",
        "    order_data[col] = order_data[col].fillna(order_data[col].mode()[0] if not order_data[col].mode().empty else f\"Unknown {col}\")\n",
        "\n",
        "print(f\"Order Data remaining missing values: {order_data.isnull().sum().sum()}\")\n",
        "order_data.to_csv('Order_Data_Dataset_Clean.csv', index=False)\n",
        "print(\"Order Data saved\")\n",
        "\n",
        "# Clear memory\n",
        "del order_data\n",
        "\n",
        "# Product Data (separate process to reduce memory usage)\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "print(f\"Product Data initial missing values: {product_data.isnull().sum().sum()}\")\n",
        "\n",
        "# Fill numeric columns with median values\n",
        "for col in product_data.select_dtypes(include=['number']).columns:\n",
        "    product_data[col] = product_data[col].fillna(product_data[col].median())\n",
        "\n",
        "# Fill object columns with mode values\n",
        "for col in product_data.select_dtypes(include=['object']).columns:\n",
        "    if col in ['description', 'features', 'details']:\n",
        "        product_data[col] = product_data[col].fillna(f\"No {col} available\")\n",
        "    elif col == 'title':\n",
        "        product_data[col] = product_data[col].fillna(\"Unnamed Product\")\n",
        "    else:\n",
        "        product_data[col] = product_data[col].fillna(product_data[col].mode()[0] if not product_data[col].mode().empty else f\"Unknown {col}\")\n",
        "\n",
        "print(f\"Product Data remaining missing values: {product_data.isnull().sum().sum()}\")\n",
        "product_data.to_csv('Product_Information_Dataset_Clean.csv', index=False)\n",
        "print(\"Product Data saved\")\n",
        "\n",
        "print(\"Imputation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e715a18",
      "metadata": {
        "id": "3e715a18"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "print(f\"Total missing values: {order_data.isnull().sum().sum()}\")\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "print(f\"Total missing values: {product_data.isnull().sum().sum()}\")\n",
        "\n",
        "# ---- Order Data KNN Imputation ----\n",
        "print(\"\\nPerforming KNN imputation on Order Data...\")\n",
        "\n",
        "# Store categorical columns for later\n",
        "order_cat_cols = order_data.select_dtypes(include=['object']).columns.tolist()\n",
        "order_num_cols = order_data.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# Handle categorical columns first (we'll use mode imputation for these)\n",
        "for col in order_cat_cols:\n",
        "    if order_data[col].isnull().sum() > 0:\n",
        "        mode_value = order_data[col].mode()[0]\n",
        "        order_data[col] = order_data[col].fillna(mode_value)\n",
        "        print(f\"Filled categorical column {col} with mode: {mode_value}\")\n",
        "\n",
        "# For numeric data, use KNN imputation\n",
        "if order_data[order_num_cols].isnull().sum().sum() > 0:\n",
        "    # Create a copy of numeric columns only\n",
        "    order_numeric_data = order_data[order_num_cols].copy()\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    order_numeric_scaled = scaler.fit_transform(order_numeric_data.fillna(0))\n",
        "\n",
        "    # Apply KNN imputation (n_neighbors=5 is a common choice)\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    order_imputed_array = imputer.fit_transform(order_numeric_scaled)\n",
        "\n",
        "    # Convert back to dataframe and inverse transform to original scale\n",
        "    order_imputed_scaled = pd.DataFrame(order_imputed_array, columns=order_numeric_data.columns)\n",
        "    order_imputed = pd.DataFrame(scaler.inverse_transform(order_imputed_scaled),\n",
        "                                columns=order_numeric_data.columns)\n",
        "\n",
        "    # Update the original dataframe with imputed values\n",
        "    for col in order_num_cols:\n",
        "        order_data[col] = order_imputed[col].values\n",
        "\n",
        "    print(f\"KNN imputation completed for {len(order_num_cols)} numeric columns in Order Data\")\n",
        "\n",
        "# ---- Product Data KNN Imputation ----\n",
        "print(\"\\nPerforming KNN imputation on Product Data...\")\n",
        "\n",
        "# Store categorical columns for later\n",
        "product_cat_cols = product_data.select_dtypes(include=['object']).columns.tolist()\n",
        "product_num_cols = product_data.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# Handle special text columns first\n",
        "text_columns = ['description', 'features', 'details', 'title']\n",
        "for col in text_columns:\n",
        "    if col in product_data.columns and product_data[col].isnull().sum() > 0:\n",
        "        if col == 'title':\n",
        "            product_data[col] = product_data[col].fillna(\"Unnamed Product\")\n",
        "            print(f\"Filled {col} with 'Unnamed Product'\")\n",
        "        else:\n",
        "            placeholder = f\"No {col} available\"\n",
        "            product_data[col] = product_data[col].fillna(placeholder)\n",
        "            print(f\"Filled {col} with '{placeholder}'\")\n",
        "\n",
        "# Handle remaining categorical columns\n",
        "remaining_cat_cols = [col for col in product_cat_cols if col not in text_columns]\n",
        "for col in remaining_cat_cols:\n",
        "    if product_data[col].isnull().sum() > 0:\n",
        "        mode_value = product_data[col].mode()[0]\n",
        "        product_data[col] = product_data[col].fillna(mode_value)\n",
        "        print(f\"Filled categorical column {col} with mode: {mode_value}\")\n",
        "\n",
        "# For numeric data, use KNN imputation\n",
        "if product_data[product_num_cols].isnull().sum().sum() > 0:\n",
        "    # Create a copy of numeric columns only\n",
        "    product_numeric_data = product_data[product_num_cols].copy()\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    product_numeric_scaled = scaler.fit_transform(product_numeric_data.fillna(0))\n",
        "\n",
        "    # Apply KNN imputation (n_neighbors=5 is a common choice)\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    product_imputed_array = imputer.fit_transform(product_numeric_scaled)\n",
        "\n",
        "    # Convert back to dataframe and inverse transform to original scale\n",
        "    product_imputed_scaled = pd.DataFrame(product_imputed_array, columns=product_numeric_data.columns)\n",
        "    product_imputed = pd.DataFrame(scaler.inverse_transform(product_imputed_scaled),\n",
        "                                  columns=product_numeric_data.columns)\n",
        "\n",
        "    # Update the original dataframe with imputed values\n",
        "    for col in product_num_cols:\n",
        "        product_data[col] = product_imputed[col].values\n",
        "\n",
        "    print(f\"KNN imputation completed for {len(product_num_cols)} numeric columns in Product Data\")\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "\n",
        "print(\"\\nOrder Data Remaining Missing Values:\")\n",
        "order_remaining = order_data.isnull().sum()\n",
        "print(order_remaining[order_remaining > 0])\n",
        "if order_data.isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Order Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {order_data.isnull().sum().sum()} missing values remain in Order Data\")\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values:\")\n",
        "product_remaining = product_data.isnull().sum()\n",
        "print(product_remaining[product_remaining > 0])\n",
        "if product_data.isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Product Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {product_data.isnull().sum().sum()} missing values remain in Product Data\")\n",
        "\n",
        "# Save the clean datasets\n",
        "print(\"\\nSaving clean datasets...\")\n",
        "order_data.to_csv('Order_Data_Dataset_KNN_Clean.csv', index=False)\n",
        "product_data.to_csv('Product_Information_Dataset_KNN_Clean.csv', index=False)\n",
        "print(\"Clean datasets saved as 'Order_Data_Dataset_KNN_Clean.csv' and 'Product_Information_Dataset_KNN_Clean.csv'\")\n",
        "\n",
        "print(\"\\nKNN Imputation complete! All datasets should have zero missing values.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "print(f\"Total missing values: {order_data.isnull().sum().sum()}\")\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "print(f\"Total missing values: {product_data.isnull().sum().sum()}\")\n",
        "\n",
        "# ---- Order Data KNN Imputation ----\n",
        "print(\"\\nPerforming KNN imputation on Order Data...\")\n",
        "\n",
        "# Store categorical columns for later\n",
        "order_cat_cols = order_data.select_dtypes(include=['object']).columns.tolist()\n",
        "order_num_cols = order_data.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# Handle categorical columns first (we'll use mode imputation for these)\n",
        "for col in order_cat_cols:\n",
        "    if order_data[col].isnull().sum() > 0:\n",
        "        mode_value = order_data[col].mode()[0]\n",
        "        order_data[col] = order_data[col].fillna(mode_value)\n",
        "        print(f\"Filled categorical column {col} with mode: {mode_value}\")\n",
        "\n",
        "# For numeric data, use KNN imputation\n",
        "if order_data[order_num_cols].isnull().sum().sum() > 0:\n",
        "    # Create a copy of numeric columns only\n",
        "    order_numeric_data = order_data[order_num_cols].copy()\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    order_numeric_scaled = scaler.fit_transform(order_numeric_data.fillna(0))\n",
        "\n",
        "    # Apply KNN imputation (n_neighbors=5 is a common choice)\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    order_imputed_array = imputer.fit_transform(order_numeric_scaled)\n",
        "\n",
        "    # Convert back to dataframe and inverse transform to original scale\n",
        "    order_imputed_scaled = pd.DataFrame(order_imputed_array, columns=order_numeric_data.columns)\n",
        "    order_imputed = pd.DataFrame(scaler.inverse_transform(order_imputed_scaled),\n",
        "                                columns=order_numeric_data.columns)\n",
        "\n",
        "    # Update the original dataframe with imputed values\n",
        "    for col in order_num_cols:\n",
        "        order_data[col] = order_imputed[col].values\n",
        "\n",
        "    print(f\"KNN imputation completed for {len(order_num_cols)} numeric columns in Order Data\")\n",
        "\n",
        "# ---- Product Data KNN Imputation ----\n",
        "print(\"\\nPerforming KNN imputation on Product Data...\")\n",
        "\n",
        "# Store categorical columns for later\n",
        "product_cat_cols = product_data.select_dtypes(include=['object']).columns.tolist()\n",
        "product_num_cols = product_data.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# Handle special text columns first\n",
        "text_columns = ['description', 'features', 'details', 'title']\n",
        "for col in text_columns:\n",
        "    if col in product_data.columns and product_data[col].isnull().sum() > 0:\n",
        "        if col == 'title':\n",
        "            product_data[col] = product_data[col].fillna(\"Unnamed Product\")\n",
        "            print(f\"Filled {col} with 'Unnamed Product'\")\n",
        "        else:\n",
        "            placeholder = f\"No {col} available\"\n",
        "            product_data[col] = product_data[col].fillna(placeholder)\n",
        "            print(f\"Filled {col} with '{placeholder}'\")\n",
        "\n",
        "# Handle remaining categorical columns\n",
        "remaining_cat_cols = [col for col in product_cat_cols if col not in text_columns]\n",
        "for col in remaining_cat_cols:\n",
        "    if product_data[col].isnull().sum() > 0:\n",
        "        mode_value = product_data[col].mode()[0]\n",
        "        product_data[col] = product_data[col].fillna(mode_value)\n",
        "        print(f\"Filled categorical column {col} with mode: {mode_value}\")\n",
        "\n",
        "# For numeric data, use KNN imputation\n",
        "if product_data[product_num_cols].isnull().sum().sum() > 0:\n",
        "    # Create a copy of numeric columns only\n",
        "    product_numeric_data = product_data[product_num_cols].copy()\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    product_numeric_scaled = scaler.fit_transform(product_numeric_data.fillna(0))\n",
        "\n",
        "    # Apply KNN imputation (n_neighbors=5 is a common choice)\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    product_imputed_array = imputer.fit_transform(product_numeric_scaled)\n",
        "\n",
        "    # Convert back to dataframe and inverse transform to original scale\n",
        "    product_imputed_scaled = pd.DataFrame(product_imputed_array, columns=product_numeric_data.columns)\n",
        "    product_imputed = pd.DataFrame(scaler.inverse_transform(product_imputed_scaled),\n",
        "                                  columns=product_numeric_data.columns)\n",
        "\n",
        "    # Update the original dataframe with imputed values\n",
        "    for col in product_num_cols:\n",
        "        product_data[col] = product_imputed[col].values\n",
        "\n",
        "    print(f\"KNN imputation completed for {len(product_num_cols)} numeric columns in Product Data\")\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "\n",
        "print(\"\\nOrder Data Remaining Missing Values:\")\n",
        "order_remaining = order_data.isnull().sum()\n",
        "print(order_remaining[order_remaining > 0])\n",
        "if order_data.isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Order Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {order_data.isnull().sum().sum()} missing values remain in Order Data\")\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values:\")\n",
        "product_remaining = product_data.isnull().sum()\n",
        "print(product_remaining[product_remaining > 0])\n",
        "if product_data.isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Product Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {product_data.isnull().sum().sum()} missing values remain in Product Data\")\n",
        "\n",
        "# Save the clean datasets\n",
        "print(\"\\nSaving clean datasets...\")\n",
        "order_data.to_csv('Order_Data_Dataset_KNN_Clean.csv', index=False)\n",
        "product_data.to_csv('Product_Information_Dataset_KNN_Clean.csv', index=False)\n",
        "print(\"Clean datasets saved as 'Order_Data_Dataset_KNN_Clean.csv' and 'Product_Information_Dataset_KNN_Clean.csv'\")\n",
        "\n",
        "print(\"\\nKNN Imputation complete! All datasets should have zero missing values.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvTshd1UHkIA",
        "outputId": "d5d2f96a-ef05-427b-ee56-9bf5ceecdd65"
      },
      "id": "hvTshd1UHkIA",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "INITIAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Missing Values:\n",
            "Aging             1\n",
            "Sales             1\n",
            "Quantity          2\n",
            "Discount          1\n",
            "Shipping_Cost     1\n",
            "Order_Priority    2\n",
            "dtype: int64\n",
            "Total missing values: 8\n",
            "\n",
            "Product Data Missing Values:\n",
            "main_category      30\n",
            "price            1266\n",
            "store               1\n",
            "dtype: int64\n",
            "Total missing values: 1297\n",
            "\n",
            "Performing KNN imputation on Order Data...\n",
            "Filled categorical column Order_Priority with mode: Medium\n",
            "KNN imputation completed for 7 numeric columns in Order Data\n",
            "\n",
            "Performing KNN imputation on Product Data...\n",
            "Filled categorical column main_category with mode: Musical Instruments\n",
            "Filled categorical column store with mode: Fender\n",
            "KNN imputation completed for 3 numeric columns in Product Data\n",
            "\n",
            "FINAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Remaining Missing Values:\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Order Data!\n",
            "\n",
            "Product Data Remaining Missing Values:\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Product Data!\n",
            "\n",
            "Saving clean datasets...\n",
            "Clean datasets saved as 'Order_Data_Dataset_KNN_Clean.csv' and 'Product_Information_Dataset_KNN_Clean.csv'\n",
            "\n",
            "KNN Imputation complete! All datasets should have zero missing values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "\n",
        "# Function to perform MICE imputation with appropriate handling for numerical and categorical data\n",
        "def mice_imputation(df):\n",
        "    # Separate numerical and categorical columns\n",
        "    num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Handle numerical data with MICE\n",
        "    if num_cols:\n",
        "        # Create and fit the iterative imputer for numerical data\n",
        "        num_imputer = IterativeImputer(\n",
        "            estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            max_iter=10,\n",
        "            random_state=42\n",
        "        )\n",
        "        # Make a copy of the numerical data and impute\n",
        "        df_num = df[num_cols].copy()\n",
        "        df_num_imputed = pd.DataFrame(\n",
        "            num_imputer.fit_transform(df_num),\n",
        "            columns=num_cols\n",
        "        )\n",
        "\n",
        "        # Replace the original numerical columns with imputed values\n",
        "        for col in num_cols:\n",
        "            df[col] = df_num_imputed[col].values\n",
        "\n",
        "    # Handle categorical data\n",
        "    if cat_cols:\n",
        "        # For text columns like description, features, etc. use simple imputation\n",
        "        text_columns = ['description', 'features', 'details', 'title']\n",
        "        for col in [c for c in text_columns if c in cat_cols]:\n",
        "            if df[col].isnull().sum() > 0:\n",
        "                if col == 'title':\n",
        "                    df[col] = df[col].fillna(\"Unnamed Product\")\n",
        "                else:\n",
        "                    placeholder = f\"No {col} available\"\n",
        "                    df[col] = df[col].fillna(placeholder)\n",
        "                cat_cols.remove(col)  # Remove from cat_cols to skip MICE\n",
        "\n",
        "        # For remaining categorical columns, implement MICE with a classifier\n",
        "        if cat_cols:\n",
        "            # One hot encode or ordinal encode categorical data based on cardinality\n",
        "            cat_encoded = pd.DataFrame()\n",
        "            encoders = {}\n",
        "\n",
        "            for col in cat_cols:\n",
        "                # For high cardinality, use ordinal encoding\n",
        "                if df[col].nunique() > 10:\n",
        "                    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "                    encoded = encoder.fit_transform(df[[col]])\n",
        "                    cat_encoded[col] = encoded.flatten()\n",
        "                # For low cardinality, use one-hot encoding\n",
        "                else:\n",
        "                    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "                    encoded = encoder.fit_transform(df[[col]])\n",
        "                    encoded_cols = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
        "                    encoded_df = pd.DataFrame(encoded, columns=encoded_cols)\n",
        "                    cat_encoded = pd.concat([cat_encoded, encoded_df], axis=1)\n",
        "\n",
        "                encoders[col] = (encoder, 'ordinal' if df[col].nunique() > 10 else 'onehot')\n",
        "\n",
        "            # Combine with numerical features for MICE\n",
        "            if num_cols:\n",
        "                combined_data = pd.concat([df_num_imputed, cat_encoded], axis=1)\n",
        "            else:\n",
        "                combined_data = cat_encoded\n",
        "\n",
        "            # Now run MICE on this combined dataframe\n",
        "            combined_imputer = IterativeImputer(\n",
        "                estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "                max_iter=10,\n",
        "                random_state=42\n",
        "            )\n",
        "            combined_imputed = pd.DataFrame(\n",
        "                combined_imputer.fit_transform(combined_data),\n",
        "                columns=combined_data.columns\n",
        "            )\n",
        "\n",
        "            # Decode back to original categorical values and update the dataframe\n",
        "            for col in cat_cols:\n",
        "                encoder_type = encoders[col][1]\n",
        "                encoder = encoders[col][0]\n",
        "\n",
        "                if encoder_type == 'ordinal':\n",
        "                    # Get the column from imputed data\n",
        "                    imputed_values = combined_imputed[col].values.reshape(-1, 1)\n",
        "                    # Round to nearest integer for classification\n",
        "                    imputed_values = np.round(imputed_values).astype(int)\n",
        "                    # Convert back to original categories\n",
        "                    original_values = encoder.inverse_transform(imputed_values)\n",
        "                    df[col] = original_values\n",
        "                else:  # One-hot encoded\n",
        "                    # Get all columns for this category\n",
        "                    one_hot_cols = [c for c in combined_imputed.columns if c.startswith(f\"{col}_\")]\n",
        "                    # For each row, find the column with the highest value\n",
        "                    imputed_one_hot = combined_imputed[one_hot_cols].values\n",
        "                    # Get the index of max value for each row\n",
        "                    max_indices = np.argmax(imputed_one_hot, axis=1)\n",
        "                    # Convert indices back to original categories\n",
        "                    categories = encoder.categories_[0]\n",
        "                    original_values = np.array([categories[idx] for idx in max_indices])\n",
        "                    df[col] = original_values\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply MICE imputation to both datasets\n",
        "print(\"\\nPerforming MICE imputation on Order Data...\")\n",
        "order_data_imputed = mice_imputation(order_data.copy())\n",
        "\n",
        "print(\"\\nPerforming MICE imputation on Product Data...\")\n",
        "product_data_imputed = mice_imputation(product_data.copy())\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Remaining Missing Values:\")\n",
        "print(order_data_imputed.isnull().sum()[order_data_imputed.isnull().sum() > 0])\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values:\")\n",
        "print(product_data_imputed.isnull().sum()[product_data_imputed.isnull().sum() > 0])\n",
        "\n",
        "# Save the clean datasets\n",
        "print(\"\\nSaving clean datasets...\")\n",
        "order_data_imputed.to_csv('Order_Data_Dataset_MICE_Clean.csv', index=False)\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_MICE_Clean.csv', index=False)\n",
        "print(\"Clean datasets saved as 'Order_Data_Dataset_MICE_Clean.csv' and 'Product_Information_Dataset_MICE_Clean.csv'\")\n",
        "\n",
        "print(\"\\nMICE Imputation complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "id": "31b1M3FdHo6s",
        "outputId": "869e6ff6-1d2a-460c-f975-8dfea79c7e5e"
      },
      "id": "31b1M3FdHo6s",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "INITIAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Missing Values:\n",
            "Aging             1\n",
            "Sales             1\n",
            "Quantity          2\n",
            "Discount          1\n",
            "Shipping_Cost     1\n",
            "Order_Priority    2\n",
            "dtype: int64\n",
            "\n",
            "Product Data Missing Values:\n",
            "main_category      30\n",
            "price            1266\n",
            "store               1\n",
            "dtype: int64\n",
            "\n",
            "Performing MICE imputation on Order Data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-82903379f7e7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Apply MICE imputation to both datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPerforming MICE imputation on Order Data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0morder_data_imputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmice_imputation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPerforming MICE imputation on Product Data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-82903379f7e7>\u001b[0m in \u001b[0;36mmice_imputation\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# For low cardinality, use one-hot encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mencoded_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"{col}_{cat}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "\n",
        "# Function to perform MICE imputation with appropriate handling for numerical and categorical data\n",
        "def mice_imputation(df):\n",
        "    # Separate numerical and categorical columns\n",
        "    num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Handle numerical data with MICE\n",
        "    if num_cols:\n",
        "        # Create and fit the iterative imputer for numerical data\n",
        "        num_imputer = IterativeImputer(\n",
        "            estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            max_iter=10,\n",
        "            random_state=42\n",
        "        )\n",
        "        # Make a copy of the numerical data and impute\n",
        "        df_num = df[num_cols].copy()\n",
        "        df_num_imputed = pd.DataFrame(\n",
        "            num_imputer.fit_transform(df_num),\n",
        "            columns=num_cols\n",
        "        )\n",
        "\n",
        "        # Replace the original numerical columns with imputed values\n",
        "        for col in num_cols:\n",
        "            df[col] = df_num_imputed[col].values\n",
        "\n",
        "    # Handle categorical data\n",
        "    if cat_cols:\n",
        "        # For text columns like description, features, etc. use simple imputation\n",
        "        text_columns = ['description', 'features', 'details', 'title']\n",
        "        for col in [c for c in text_columns if c in cat_cols]:\n",
        "            if df[col].isnull().sum() > 0:\n",
        "                if col == 'title':\n",
        "                    df[col] = df[col].fillna(\"Unnamed Product\")\n",
        "                else:\n",
        "                    placeholder = f\"No {col} available\"\n",
        "                    df[col] = df[col].fillna(placeholder)\n",
        "                cat_cols.remove(col)  # Remove from cat_cols to skip MICE\n",
        "\n",
        "        # For remaining categorical columns, implement MICE with a classifier\n",
        "        if cat_cols:\n",
        "            # One hot encode or ordinal encode categorical data based on cardinality\n",
        "            cat_encoded = pd.DataFrame()\n",
        "            encoders = {}\n",
        "\n",
        "            for col in cat_cols:\n",
        "                # For high cardinality, use ordinal encoding\n",
        "                if df[col].nunique() > 10:\n",
        "                    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "                    encoded = encoder.fit_transform(df[[col]])\n",
        "                    cat_encoded[col] = encoded.flatten()\n",
        "                # For low cardinality, use one-hot encoding\n",
        "                else:\n",
        "                    # Fix: Use sparse_output instead of sparse\n",
        "                    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "                    encoded = encoder.fit_transform(df[[col]])\n",
        "                    encoded_cols = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
        "                    encoded_df = pd.DataFrame(encoded, columns=encoded_cols)\n",
        "                    cat_encoded = pd.concat([cat_encoded, encoded_df], axis=1)\n",
        "\n",
        "                encoders[col] = (encoder, 'ordinal' if df[col].nunique() > 10 else 'onehot')\n",
        "\n",
        "            # Combine with numerical features for MICE\n",
        "            if num_cols:\n",
        "                combined_data = pd.concat([df_num_imputed, cat_encoded], axis=1)\n",
        "            else:\n",
        "                combined_data = cat_encoded\n",
        "\n",
        "            # Now run MICE on this combined dataframe\n",
        "            combined_imputer = IterativeImputer(\n",
        "                estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "                max_iter=10,\n",
        "                random_state=42\n",
        "            )\n",
        "            combined_imputed = pd.DataFrame(\n",
        "                combined_imputer.fit_transform(combined_data),\n",
        "                columns=combined_data.columns\n",
        "            )\n",
        "\n",
        "            # Decode back to original categorical values and update the dataframe\n",
        "            for col in cat_cols:\n",
        "                encoder_type = encoders[col][1]\n",
        "                encoder = encoders[col][0]\n",
        "\n",
        "                if encoder_type == 'ordinal':\n",
        "                    # Get the column from imputed data\n",
        "                    imputed_values = combined_imputed[col].values.reshape(-1, 1)\n",
        "                    # Round to nearest integer for classification\n",
        "                    imputed_values = np.round(imputed_values).astype(int)\n",
        "                    # Convert back to original categories\n",
        "                    original_values = encoder.inverse_transform(imputed_values)\n",
        "                    df[col] = original_values\n",
        "                else:  # One-hot encoded\n",
        "                    # Get all columns for this category\n",
        "                    one_hot_cols = [c for c in combined_imputed.columns if c.startswith(f\"{col}_\")]\n",
        "                    # For each row, find the column with the highest value\n",
        "                    imputed_one_hot = combined_imputed[one_hot_cols].values\n",
        "                    # Get the index of max value for each row\n",
        "                    max_indices = np.argmax(imputed_one_hot, axis=1)\n",
        "                    # Convert indices back to original categories\n",
        "                    categories = encoder.categories_[0]\n",
        "                    original_values = np.array([categories[idx] for idx in max_indices])\n",
        "                    df[col] = original_values\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply MICE imputation to both datasets\n",
        "print(\"\\nPerforming MICE imputation on Order Data...\")\n",
        "order_data_imputed = mice_imputation(order_data.copy())\n",
        "\n",
        "print(\"\\nPerforming MICE imputation on Product Data...\")\n",
        "product_data_imputed = mice_imputation(product_data.copy())\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Remaining Missing Values:\")\n",
        "print(order_data_imputed.isnull().sum()[order_data_imputed.isnull().sum() > 0])\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values:\")\n",
        "print(product_data_imputed.isnull().sum()[product_data_imputed.isnull().sum() > 0])\n",
        "\n",
        "# Save the clean datasets\n",
        "print(\"\\nSaving clean datasets...\")\n",
        "order_data_imputed.to_csv('Order_Data_Dataset_MICE_Clean.csv', index=False)\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_MICE_Clean.csv', index=False)\n",
        "print(\"Clean datasets saved as 'Order_Data_Dataset_MICE_Clean.csv' and 'Product_Information_Dataset_MICE_Clean.csv'\")\n",
        "\n",
        "print(\"\\nMICE Imputation complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "oXnVfvEBLpqx",
        "outputId": "8620c36f-46ed-433c-85d5-6891fa7f42f6"
      },
      "id": "oXnVfvEBLpqx",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "INITIAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Missing Values:\n",
            "Aging             1\n",
            "Sales             1\n",
            "Quantity          2\n",
            "Discount          1\n",
            "Shipping_Cost     1\n",
            "Order_Priority    2\n",
            "dtype: int64\n",
            "\n",
            "Product Data Missing Values:\n",
            "main_category      30\n",
            "price            1266\n",
            "store               1\n",
            "dtype: int64\n",
            "\n",
            "Performing MICE imputation on Order Data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d7d1df1972ec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# Apply MICE imputation to both datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPerforming MICE imputation on Order Data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0morder_data_imputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmice_imputation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPerforming MICE imputation on Product Data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-d7d1df1972ec>\u001b[0m in \u001b[0;36mmice_imputation\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;31m# Convert back to original categories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0moriginal_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimputed_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# One-hot encoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0;31m# Get all columns for this category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m         \"\"\"\n\u001b[0;32m-> 4524\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5266\u001b[0m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5267\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5268\u001b[0m         if (\n\u001b[1;32m   5269\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 if (\n\u001b[1;32m    608\u001b[0m                     \u001b[0mobject_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;31m# Caller is responsible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "\n",
        "# Function to perform simplified MICE imputation\n",
        "def simplified_mice_imputation(df):\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # Separate numerical and categorical columns\n",
        "    num_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()\n",
        "    cat_cols = df_imputed.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Handle text columns with simple imputation first\n",
        "    text_columns = ['description', 'features', 'details', 'title']\n",
        "    for col in [c for c in text_columns if c in cat_cols]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            if col == 'title':\n",
        "                df_imputed[col] = df_imputed[col].fillna(\"Unnamed Product\")\n",
        "            else:\n",
        "                placeholder = f\"No {col} available\"\n",
        "                df_imputed[col] = df_imputed[col].fillna(placeholder)\n",
        "\n",
        "    # Handle remaining categorical columns with mode imputation\n",
        "    for col in [c for c in cat_cols if c not in text_columns]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            mode_value = df_imputed[col].mode()[0]\n",
        "            df_imputed[col] = df_imputed[col].fillna(mode_value)\n",
        "            print(f\"Imputed missing values in '{col}' with mode: {mode_value}\")\n",
        "\n",
        "    # Handle numerical columns with MICE\n",
        "    if num_cols and df_imputed[num_cols].isnull().sum().sum() > 0:\n",
        "        # Create a numerical-only dataframe\n",
        "        df_num = df_imputed[num_cols].copy()\n",
        "\n",
        "        # First use SimpleImputer for initial values (needed for MICE to work properly)\n",
        "        initial_imputer = SimpleImputer(strategy='mean')\n",
        "        df_num_initial = pd.DataFrame(\n",
        "            initial_imputer.fit_transform(df_num),\n",
        "            columns=num_cols\n",
        "        )\n",
        "\n",
        "        # Now apply MICE to the initially imputed data\n",
        "        try:\n",
        "            mice_imputer = IterativeImputer(\n",
        "                estimator=RandomForestRegressor(n_estimators=50, random_state=42),\n",
        "                max_iter=5,\n",
        "                random_state=42,\n",
        "                skip_complete=True  # Skip columns with no missing values\n",
        "            )\n",
        "\n",
        "            # Fit and transform the data\n",
        "            df_num_imputed = pd.DataFrame(\n",
        "                mice_imputer.fit_transform(df_num_initial),\n",
        "                columns=num_cols\n",
        "            )\n",
        "\n",
        "            # Replace only the originally missing values\n",
        "            for col in num_cols:\n",
        "                missing_mask = df[col].isnull()\n",
        "                if missing_mask.any():\n",
        "                    # Only replace the missing values\n",
        "                    df_imputed.loc[missing_mask, col] = df_num_imputed.loc[missing_mask, col].values\n",
        "                    print(f\"MICE imputation completed for {missing_mask.sum()} missing values in '{col}'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in MICE imputation: {str(e)}\")\n",
        "            print(\"Falling back to simple mean imputation for numerical data\")\n",
        "\n",
        "            # Fallback to simple mean imputation\n",
        "            for col in num_cols:\n",
        "                if df_imputed[col].isnull().sum() > 0:\n",
        "                    mean_value = df_imputed[col].mean()\n",
        "                    df_imputed[col] = df_imputed[col].fillna(mean_value)\n",
        "                    print(f\"Imputed missing values in '{col}' with mean: {mean_value:.2f}\")\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# Apply simplified MICE imputation to both datasets\n",
        "print(\"\\nPerforming simplified MICE imputation on Order Data...\")\n",
        "order_data_imputed = simplified_mice_imputation(order_data)\n",
        "\n",
        "print(\"\\nPerforming simplified MICE imputation on Product Data...\")\n",
        "product_data_imputed = simplified_mice_imputation(product_data)\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Remaining Missing Values:\")\n",
        "remaining_order = order_data_imputed.isnull().sum()\n",
        "print(remaining_order[remaining_order > 0])\n",
        "if order_data_imputed.isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Order Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {order_data_imputed.isnull().sum().sum()} missing values remain in Order Data\")\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values:\")\n",
        "remaining_product = product_data_imputed.isnull().sum()\n",
        "print(remaining_product[remaining_product > 0])\n",
        "if product_data_imputed.isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Product Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {product_data_imputed.isnull().sum().sum()} missing values remain in Product Data\")\n",
        "\n",
        "# Save the clean datasets\n",
        "print(\"\\nSaving clean datasets...\")\n",
        "order_data_imputed.to_csv('Order_Data_Dataset_MICE_Clean.csv', index=False)\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_MICE_Clean.csv', index=False)\n",
        "print(\"Clean datasets saved!\")\n",
        "\n",
        "print(\"\\nSimplified MICE Imputation complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK9gIPZhOcjG",
        "outputId": "e43bb54b-a525-4fa6-b231-367dba1a019b"
      },
      "id": "bK9gIPZhOcjG",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "INITIAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Missing Values:\n",
            "Aging             1\n",
            "Sales             1\n",
            "Quantity          2\n",
            "Discount          1\n",
            "Shipping_Cost     1\n",
            "Order_Priority    2\n",
            "dtype: int64\n",
            "\n",
            "Product Data Missing Values:\n",
            "main_category      30\n",
            "price            1266\n",
            "store               1\n",
            "dtype: int64\n",
            "\n",
            "Performing simplified MICE imputation on Order Data...\n",
            "Imputed missing values in 'Order_Priority' with mode: Medium\n",
            "MICE imputation completed for 1 missing values in 'Aging'\n",
            "MICE imputation completed for 1 missing values in 'Sales'\n",
            "MICE imputation completed for 2 missing values in 'Quantity'\n",
            "MICE imputation completed for 1 missing values in 'Discount'\n",
            "MICE imputation completed for 1 missing values in 'Shipping_Cost'\n",
            "\n",
            "Performing simplified MICE imputation on Product Data...\n",
            "Imputed missing values in 'main_category' with mode: Musical Instruments\n",
            "Imputed missing values in 'store' with mode: Fender\n",
            "MICE imputation completed for 1266 missing values in 'price'\n",
            "\n",
            "FINAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Remaining Missing Values:\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Order Data!\n",
            "\n",
            "Product Data Remaining Missing Values:\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Product Data!\n",
            "\n",
            "Saving clean datasets...\n",
            "Clean datasets saved!\n",
            "\n",
            "Simplified MICE Imputation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "\n",
        "# Create imputation tracking columns\n",
        "def add_imputation_tracking(df):\n",
        "    \"\"\"Add columns to track imputed values\"\"\"\n",
        "    tracking_df = pd.DataFrame(index=df.index)\n",
        "\n",
        "    # For each column with missing values, create a tracking column\n",
        "    for col in df.columns:\n",
        "        if df[col].isnull().any():\n",
        "            # Create a tracking column with suffix '_imputed'\n",
        "            tracking_col = f\"{col}_imputed\"\n",
        "            tracking_df[tracking_col] = df[col].isnull()\n",
        "\n",
        "    # Concatenate with original dataframe\n",
        "    return pd.concat([df, tracking_df], axis=1)\n",
        "\n",
        "# Function to perform MICE imputation with tracking\n",
        "def mice_imputation_with_tracking(df):\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # First add tracking columns before any imputation\n",
        "    df_imputed = add_imputation_tracking(df_imputed)\n",
        "\n",
        "    # Separate numerical and categorical columns (original columns only, not tracking columns)\n",
        "    original_cols = df.columns.tolist()\n",
        "    num_cols = df_imputed[original_cols].select_dtypes(include=['number']).columns.tolist()\n",
        "    cat_cols = df_imputed[original_cols].select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Handle text columns with simple imputation first\n",
        "    text_columns = ['description', 'features', 'details', 'title']\n",
        "    for col in [c for c in text_columns if c in cat_cols]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            if col == 'title':\n",
        "                df_imputed[col] = df_imputed[col].fillna(\"Unnamed Product\")\n",
        "            else:\n",
        "                placeholder = f\"No {col} available\"\n",
        "                df_imputed[col] = df_imputed[col].fillna(placeholder)\n",
        "            print(f\"Imputed missing values in '{col}' with placeholder text\")\n",
        "\n",
        "    # Handle remaining categorical columns with mode imputation\n",
        "    for col in [c for c in cat_cols if c not in text_columns]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            mode_value = df_imputed[col].mode()[0]\n",
        "            df_imputed[col] = df_imputed[col].fillna(mode_value)\n",
        "            print(f\"Imputed missing values in '{col}' with mode: {mode_value}\")\n",
        "\n",
        "    # Handle numerical columns with MICE\n",
        "    if num_cols and df_imputed[num_cols].isnull().sum().sum() > 0:\n",
        "        # Create a numerical-only dataframe\n",
        "        df_num = df_imputed[num_cols].copy()\n",
        "\n",
        "        # First use SimpleImputer for initial values (needed for MICE to work properly)\n",
        "        initial_imputer = SimpleImputer(strategy='mean')\n",
        "        df_num_initial = pd.DataFrame(\n",
        "            initial_imputer.fit_transform(df_num),\n",
        "            columns=num_cols\n",
        "        )\n",
        "\n",
        "        # Now apply MICE to the initially imputed data\n",
        "        try:\n",
        "            mice_imputer = IterativeImputer(\n",
        "                estimator=RandomForestRegressor(n_estimators=50, random_state=42),\n",
        "                max_iter=10,\n",
        "                random_state=42,\n",
        "                skip_complete=True  # Skip columns with no missing values\n",
        "            )\n",
        "\n",
        "            # Fit and transform the data\n",
        "            df_num_imputed = pd.DataFrame(\n",
        "                mice_imputer.fit_transform(df_num_initial),\n",
        "                columns=num_cols\n",
        "            )\n",
        "\n",
        "            # Replace only the originally missing values\n",
        "            for col in num_cols:\n",
        "                missing_mask = df[col].isnull()\n",
        "                if missing_mask.any():\n",
        "                    # Only replace the missing values\n",
        "                    df_imputed.loc[missing_mask, col] = df_num_imputed.loc[missing_mask, col].values\n",
        "                    print(f\"MICE imputation completed for {missing_mask.sum()} missing values in '{col}'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in MICE imputation: {str(e)}\")\n",
        "            print(\"Falling back to simple mean imputation for numerical data\")\n",
        "\n",
        "            # Fallback to simple mean imputation\n",
        "            for col in num_cols:\n",
        "                if df_imputed[col].isnull().sum() > 0:\n",
        "                    mean_value = df_imputed[col].mean()\n",
        "                    df_imputed[col] = df_imputed[col].fillna(mean_value)\n",
        "                    print(f\"Imputed missing values in '{col}' with mean: {mean_value:.2f}\")\n",
        "\n",
        "    # Add a summary column for tracking\n",
        "    df_imputed['imputed_columns'] = ''\n",
        "\n",
        "    # Find all tracking columns\n",
        "    tracking_cols = [col for col in df_imputed.columns if col.endswith('_imputed')]\n",
        "\n",
        "    # For each row, create a list of imputed columns\n",
        "    for idx in df_imputed.index:\n",
        "        imputed_cols = []\n",
        "        for tracking_col in tracking_cols:\n",
        "            if df_imputed.loc[idx, tracking_col]:\n",
        "                # Extract original column name by removing '_imputed' suffix\n",
        "                original_col = tracking_col.replace('_imputed', '')\n",
        "                imputed_cols.append(original_col)\n",
        "\n",
        "        # Update the summary column\n",
        "        if imputed_cols:\n",
        "            df_imputed.loc[idx, 'imputed_columns'] = ', '.join(imputed_cols)\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# Apply MICE imputation with tracking to both datasets\n",
        "print(\"\\nPerforming MICE imputation with tracking on Order Data...\")\n",
        "order_data_imputed = mice_imputation_with_tracking(order_data)\n",
        "\n",
        "print(\"\\nPerforming MICE imputation with tracking on Product Data...\")\n",
        "product_data_imputed = mice_imputation_with_tracking(product_data)\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Remaining Missing Values (original columns):\")\n",
        "remaining_order = order_data_imputed[order_data.columns].isnull().sum()\n",
        "print(remaining_order[remaining_order > 0])\n",
        "if order_data_imputed[order_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Order Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {order_data_imputed[order_data.columns].isnull().sum().sum()} missing values remain in Order Data\")\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values (original columns):\")\n",
        "remaining_product = product_data_imputed[product_data.columns].isnull().sum()\n",
        "print(remaining_product[remaining_product > 0])\n",
        "if product_data_imputed[product_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Product Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {product_data_imputed[product_data.columns].isnull().sum().sum()} missing values remain in Product Data\")\n",
        "\n",
        "# Display information about the tracking columns\n",
        "print(\"\\nIMPUTATION TRACKING SUMMARY:\")\n",
        "print(\"\\nOrder Data Imputation Counts:\")\n",
        "order_tracking_cols = [col for col in order_data_imputed.columns if col.endswith('_imputed')]\n",
        "for col in order_tracking_cols:\n",
        "    count = order_data_imputed[col].sum()\n",
        "    original_col = col.replace('_imputed', '')\n",
        "    print(f\"- {original_col}: {count} values imputed\")\n",
        "\n",
        "print(\"\\nProduct Data Imputation Counts:\")\n",
        "product_tracking_cols = [col for col in product_data_imputed.columns if col.endswith('_imputed')]\n",
        "for col in product_tracking_cols:\n",
        "    count = product_data_imputed[col].sum()\n",
        "    original_col = col.replace('_imputed', '')\n",
        "    print(f\"- {original_col}: {count} values imputed\")\n",
        "\n",
        "# Save the clean datasets with tracking information\n",
        "print(\"\\nSaving clean datasets with imputation tracking...\")\n",
        "order_data_imputed.to_csv('Order_Data_Dataset_MICE_Tracked.csv', index=False)\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_MICE_Tracked.csv', index=False)\n",
        "print(\"Clean datasets saved with tracking information!\")\n",
        "\n",
        "print(\"\\nMICE Imputation with tracking complete!\")"
      ],
      "metadata": {
        "id": "I285x8ooSdJ7"
      },
      "id": "I285x8ooSdJ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "\n",
        "# Function to perform MICE imputation with tracking\n",
        "def mice_imputation_with_tracking(df):\n",
        "    # Store original missing value locations before imputation\n",
        "    missing_tracker = {}\n",
        "    for col in df.columns:\n",
        "        if df[col].isnull().any():\n",
        "            missing_tracker[col] = df[col].isnull()\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # Separate numerical and categorical columns\n",
        "    num_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()\n",
        "    cat_cols = df_imputed.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Handle text columns with simple imputation first\n",
        "    text_columns = ['description', 'features', 'details', 'title']\n",
        "    for col in [c for c in text_columns if c in cat_cols]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            if col == 'title':\n",
        "                df_imputed[col] = df_imputed[col].fillna(\"Unnamed Product\")\n",
        "            else:\n",
        "                placeholder = f\"No {col} available\"\n",
        "                df_imputed[col] = df_imputed[col].fillna(placeholder)\n",
        "            print(f\"Imputed missing values in '{col}' with placeholder text\")\n",
        "\n",
        "    # Handle remaining categorical columns with mode imputation\n",
        "    for col in [c for c in cat_cols if c not in text_columns]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            mode_value = df_imputed[col].mode()[0]\n",
        "            df_imputed[col] = df_imputed[col].fillna(mode_value)\n",
        "            print(f\"Imputed missing values in '{col}' with mode: {mode_value}\")\n",
        "\n",
        "    # Handle numerical columns with MICE\n",
        "    if num_cols and df_imputed[num_cols].isnull().sum().sum() > 0:\n",
        "        # Create a numerical-only dataframe\n",
        "        df_num = df_imputed[num_cols].copy()\n",
        "\n",
        "        # First use SimpleImputer for initial values (needed for MICE to work properly)\n",
        "        initial_imputer = SimpleImputer(strategy='mean')\n",
        "        df_num_initial = pd.DataFrame(\n",
        "            initial_imputer.fit_transform(df_num),\n",
        "            columns=num_cols\n",
        "        )\n",
        "\n",
        "        # Now apply MICE to the initially imputed data\n",
        "        try:\n",
        "            mice_imputer = IterativeImputer(\n",
        "                estimator=RandomForestRegressor(n_estimators=50, random_state=42),\n",
        "                max_iter=10,\n",
        "                random_state=42,\n",
        "                skip_complete=True  # Skip columns with no missing values\n",
        "            )\n",
        "\n",
        "            # Fit and transform the data\n",
        "            df_num_imputed = pd.DataFrame(\n",
        "                mice_imputer.fit_transform(df_num_initial),\n",
        "                columns=num_cols\n",
        "            )\n",
        "\n",
        "            # Replace only the originally missing values\n",
        "            for col in num_cols:\n",
        "                missing_mask = df[col].isnull()\n",
        "                if missing_mask.any():\n",
        "                    # Only replace the missing values\n",
        "                    df_imputed.loc[missing_mask, col] = df_num_imputed.loc[missing_mask, col].values\n",
        "                    print(f\"MICE imputation completed for {missing_mask.sum()} missing values in '{col}'\")\n",
        "\n",
        "                    # Special handling for Quantity column - convert to integer\n",
        "                    if col == 'Quantity':\n",
        "                        df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "                        print(\"Quantity column rounded to integers\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in MICE imputation: {str(e)}\")\n",
        "            print(\"Falling back to simple mean imputation for numerical data\")\n",
        "\n",
        "            # Fallback to simple mean imputation\n",
        "            for col in num_cols:\n",
        "                if df_imputed[col].isnull().sum() > 0:\n",
        "                    mean_value = df_imputed[col].mean()\n",
        "                    df_imputed[col] = df_imputed[col].fillna(mean_value)\n",
        "                    print(f\"Imputed missing values in '{col}' with mean: {mean_value:.2f}\")\n",
        "\n",
        "                    # Special handling for Quantity column in fallback - convert to integer\n",
        "                    if col == 'Quantity':\n",
        "                        df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "                        print(\"Quantity column rounded to integers (fallback)\")\n",
        "\n",
        "    # Post-processing: ensure Quantity is integer type regardless of imputation method\n",
        "    if 'Quantity' in df_imputed.columns:\n",
        "        df_imputed['Quantity'] = df_imputed['Quantity'].round().astype(int)\n",
        "        print(\"Final check: Quantity column converted to integers\")\n",
        "\n",
        "    # Add a single column with imputed column names\n",
        "    df_imputed['imputed_columns'] = ''\n",
        "\n",
        "    # For each row, create a list of imputed columns\n",
        "    for idx in df_imputed.index:\n",
        "        imputed_cols = []\n",
        "        for col, missing_mask in missing_tracker.items():\n",
        "            if missing_mask[idx]:\n",
        "                imputed_cols.append(col)\n",
        "\n",
        "        # Update the summary column\n",
        "        if imputed_cols:\n",
        "            df_imputed.loc[idx, 'imputed_columns'] = ', '.join(imputed_cols)\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# Apply MICE imputation with tracking to both datasets\n",
        "print(\"\\nPerforming MICE imputation with tracking on Order Data...\")\n",
        "order_data_imputed = mice_imputation_with_tracking(order_data)\n",
        "\n",
        "print(\"\\nPerforming MICE imputation with tracking on Product Data...\")\n",
        "product_data_imputed = mice_imputation_with_tracking(product_data)\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Remaining Missing Values (original columns):\")\n",
        "remaining_order = order_data_imputed[order_data.columns].isnull().sum()\n",
        "print(remaining_order[remaining_order > 0])\n",
        "if order_data_imputed[order_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Order Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {order_data_imputed[order_data.columns].isnull().sum().sum()} missing values remain in Order Data\")\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values (original columns):\")\n",
        "remaining_product = product_data_imputed[product_data.columns].isnull().sum()\n",
        "print(remaining_product[remaining_product > 0])\n",
        "if product_data_imputed[product_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Product Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {product_data_imputed[product_data.columns].isnull().sum().sum()} missing values remain in Product Data\")\n",
        "\n",
        "# Display information about the imputed values\n",
        "print(\"\\nIMPUTATION TRACKING SUMMARY:\")\n",
        "print(\"\\nOrder Data Imputation Counts:\")\n",
        "for col in order_missing.index[order_missing > 0]:\n",
        "    count = (order_data[col].isnull()).sum()\n",
        "    print(f\"- {col}: {count} values imputed\")\n",
        "\n",
        "print(\"\\nProduct Data Imputation Counts:\")\n",
        "for col in product_missing.index[product_missing > 0]:\n",
        "    count = (product_data[col].isnull()).sum()\n",
        "    print(f\"- {col}: {count} values imputed\")\n",
        "\n",
        "# Check for any decimal values in Quantity column\n",
        "if 'Quantity' in order_data_imputed.columns:\n",
        "    non_integer_mask = order_data_imputed['Quantity'] != order_data_imputed['Quantity'].astype(int)\n",
        "    non_integer_count = non_integer_mask.sum()\n",
        "    if non_integer_count > 0:\n",
        "        print(f\"\\nWARNING: Found {non_integer_count} non-integer values in Quantity column after imputation!\")\n",
        "        print(\"Converting these to integers now...\")\n",
        "        order_data_imputed['Quantity'] = order_data_imputed['Quantity'].round().astype(int)\n",
        "    else:\n",
        "        print(\"\\nAll Quantity values are proper integers.\")\n",
        "\n",
        "# Sample imputation tracking data\n",
        "print(\"\\nSample rows with imputed values:\")\n",
        "order_imputed_rows = order_data_imputed[order_data_imputed['imputed_columns'] != ''].head(3)\n",
        "if not order_imputed_rows.empty:\n",
        "    print(\"\\nOrder Data sample imputed rows:\")\n",
        "    for idx, row in order_imputed_rows.iterrows():\n",
        "        print(f\"Row {idx}: Imputed columns: {row['imputed_columns']}\")\n",
        "\n",
        "product_imputed_rows = product_data_imputed[product_data_imputed['imputed_columns'] != ''].head(3)\n",
        "if not product_imputed_rows.empty:\n",
        "    print(\"\\nProduct Data sample imputed rows:\")\n",
        "    for idx, row in product_imputed_rows.iterrows():\n",
        "        print(f\"Row {idx}: Imputed columns: {row['imputed_columns']}\")\n",
        "\n",
        "# Save the clean datasets with imputation tracking\n",
        "print(\"\\nSaving clean datasets with imputation tracking...\")\n",
        "order_data_imputed.to_csv('Order_Data_Dataset_MICE_Tracked.csv', index=False)\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_MICE_Tracked.csv', index=False)\n",
        "print(\"Clean datasets saved with tracking information!\")\n",
        "\n",
        "print(\"\\nMICE Imputation with tracking complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiUufb5b_i5F",
        "outputId": "1e4904cc-479d-4160-8966-9ef03e200142"
      },
      "id": "kiUufb5b_i5F",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "INITIAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Missing Values:\n",
            "Aging             1\n",
            "Sales             1\n",
            "Quantity          2\n",
            "Discount          1\n",
            "Shipping_Cost     1\n",
            "Order_Priority    2\n",
            "dtype: int64\n",
            "\n",
            "Product Data Missing Values:\n",
            "main_category      30\n",
            "price            1266\n",
            "store               1\n",
            "dtype: int64\n",
            "\n",
            "Performing MICE imputation with tracking on Order Data...\n",
            "Imputed missing values in 'Order_Priority' with mode: Medium\n",
            "MICE imputation completed for 1 missing values in 'Aging'\n",
            "MICE imputation completed for 1 missing values in 'Sales'\n",
            "MICE imputation completed for 2 missing values in 'Quantity'\n",
            "Quantity column rounded to integers\n",
            "MICE imputation completed for 1 missing values in 'Discount'\n",
            "MICE imputation completed for 1 missing values in 'Shipping_Cost'\n",
            "Final check: Quantity column converted to integers\n",
            "\n",
            "Performing MICE imputation with tracking on Product Data...\n",
            "Imputed missing values in 'main_category' with mode: Musical Instruments\n",
            "Imputed missing values in 'store' with mode: Fender\n",
            "MICE imputation completed for 1266 missing values in 'price'\n",
            "\n",
            "FINAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Remaining Missing Values (original columns):\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Order Data!\n",
            "\n",
            "Product Data Remaining Missing Values (original columns):\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Product Data!\n",
            "\n",
            "IMPUTATION TRACKING SUMMARY:\n",
            "\n",
            "Order Data Imputation Counts:\n",
            "- Aging: 1 values imputed\n",
            "- Sales: 1 values imputed\n",
            "- Quantity: 2 values imputed\n",
            "- Discount: 1 values imputed\n",
            "- Shipping_Cost: 1 values imputed\n",
            "- Order_Priority: 2 values imputed\n",
            "\n",
            "Product Data Imputation Counts:\n",
            "- main_category: 30 values imputed\n",
            "- price: 1266 values imputed\n",
            "- store: 1 values imputed\n",
            "\n",
            "All Quantity values are proper integers.\n",
            "\n",
            "Sample rows with imputed values:\n",
            "\n",
            "Order Data sample imputed rows:\n",
            "Row 27: Imputed columns: Aging\n",
            "Row 95: Imputed columns: Quantity\n",
            "Row 211: Imputed columns: Discount\n",
            "\n",
            "Product Data sample imputed rows:\n",
            "Row 18: Imputed columns: price\n",
            "Row 51: Imputed columns: main_category\n",
            "Row 56: Imputed columns: price\n",
            "\n",
            "Saving clean datasets with imputation tracking...\n",
            "Clean datasets saved with tracking information!\n",
            "\n",
            "MICE Imputation with tracking complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "\n",
        "# Function to perform enhanced imputation with tracking\n",
        "def enhanced_imputation_with_tracking(df):\n",
        "    # Store original missing value locations before imputation\n",
        "    missing_tracker = {}\n",
        "    for col in df.columns:\n",
        "        if df[col].isnull().any():\n",
        "            missing_tracker[col] = df[col].isnull()\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # Identify columns that should be integers\n",
        "    integer_columns = []\n",
        "    if 'Quantity' in df_imputed.columns:\n",
        "        integer_columns.append('Quantity')\n",
        "    if 'Aging' in df_imputed.columns:\n",
        "        integer_columns.append('Aging')\n",
        "\n",
        "    # Separate numerical and categorical columns\n",
        "    num_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()\n",
        "    cat_cols = df_imputed.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Handle text columns with simple imputation first\n",
        "    text_columns = ['description', 'features', 'details', 'title']\n",
        "    for col in [c for c in text_columns if c in cat_cols]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            if col == 'title':\n",
        "                df_imputed[col] = df_imputed[col].fillna(\"Unnamed Product\")\n",
        "            else:\n",
        "                placeholder = f\"No {col} available\"\n",
        "                df_imputed[col] = df_imputed[col].fillna(placeholder)\n",
        "            print(f\"Imputed missing values in '{col}' with placeholder text\")\n",
        "\n",
        "    # Handle remaining categorical columns with mode imputation\n",
        "    for col in [c for c in cat_cols if c not in text_columns]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            mode_value = df_imputed[col].mode()[0]\n",
        "            df_imputed[col] = df_imputed[col].fillna(mode_value)\n",
        "            print(f\"Imputed missing values in '{col}' with mode: {mode_value}\")\n",
        "\n",
        "    # Special handling for price column\n",
        "    if 'price' in df_imputed.columns and df_imputed['price'].isnull().any():\n",
        "        print(\"Applying specialized imputation for price column...\")\n",
        "\n",
        "        # Create a mask for missing prices\n",
        "        price_missing_mask = df_imputed['price'].isnull()\n",
        "        price_missing_count = price_missing_mask.sum()\n",
        "\n",
        "        # Get rows with missing prices\n",
        "        missing_price_rows = df_imputed[price_missing_mask]\n",
        "\n",
        "        # 1. Try to impute price based on similar products in the same category if available\n",
        "        if 'main_category' in df_imputed.columns and 'title' in df_imputed.columns:\n",
        "            # For each row with missing price\n",
        "            for idx, row in missing_price_rows.iterrows():\n",
        "                category = row.get('main_category')\n",
        "\n",
        "                if pd.notna(category):\n",
        "                    # Find similar products in the same category with non-null prices\n",
        "                    similar_products = df_imputed[\n",
        "                        (df_imputed['main_category'] == category) &\n",
        "                        (~df_imputed['price'].isnull())\n",
        "                    ]\n",
        "\n",
        "                    if len(similar_products) > 0:\n",
        "                        # Calculate mean price for this category\n",
        "                        category_mean = similar_products['price'].mean()\n",
        "                        # Add a small random variation (±15%)\n",
        "                        variation = np.random.uniform(0.85, 1.15)\n",
        "                        imputed_price = round(category_mean * variation, 2)\n",
        "                        df_imputed.loc[idx, 'price'] = imputed_price\n",
        "                    else:\n",
        "                        # If no products in the same category, use global mean with larger variation\n",
        "                        global_mean = df_imputed['price'].dropna().mean()\n",
        "                        variation = np.random.uniform(0.7, 1.3)  # Wider variation\n",
        "                        imputed_price = round(global_mean * variation, 2)\n",
        "                        df_imputed.loc[idx, 'price'] = imputed_price\n",
        "                else:\n",
        "                    # Use global mean with wide variation for products without category\n",
        "                    global_mean = df_imputed['price'].dropna().mean()\n",
        "                    variation = np.random.uniform(0.7, 1.3)\n",
        "                    imputed_price = round(global_mean * variation, 2)\n",
        "                    df_imputed.loc[idx, 'price'] = imputed_price\n",
        "\n",
        "            print(f\"Completed specialized imputation for {price_missing_count} missing prices\")\n",
        "\n",
        "    # Handle remaining numerical columns with MICE\n",
        "    if num_cols and df_imputed[num_cols].isnull().sum().sum() > 0:\n",
        "        # Special handling for price already done, remove it from num_cols if it exists\n",
        "        num_cols_for_mice = [col for col in num_cols if col != 'price']\n",
        "\n",
        "        if num_cols_for_mice:\n",
        "            # Create a numerical-only dataframe\n",
        "            df_num = df_imputed[num_cols_for_mice].copy()\n",
        "\n",
        "            # First use SimpleImputer for initial values\n",
        "            initial_imputer = SimpleImputer(strategy='mean')\n",
        "            df_num_initial = pd.DataFrame(\n",
        "                initial_imputer.fit_transform(df_num),\n",
        "                columns=num_cols_for_mice\n",
        "            )\n",
        "\n",
        "            # Now apply MICE to the initially imputed data\n",
        "            try:\n",
        "                mice_imputer = IterativeImputer(\n",
        "                    estimator=RandomForestRegressor(n_estimators=50, random_state=42),\n",
        "                    max_iter=10,\n",
        "                    random_state=42,\n",
        "                    skip_complete=True\n",
        "                )\n",
        "\n",
        "                # Fit and transform the data\n",
        "                df_num_imputed = pd.DataFrame(\n",
        "                    mice_imputer.fit_transform(df_num_initial),\n",
        "                    columns=num_cols_for_mice\n",
        "                )\n",
        "\n",
        "                # Replace only the originally missing values\n",
        "                for col in num_cols_for_mice:\n",
        "                    missing_mask = df[col].isnull()\n",
        "                    if missing_mask.any():\n",
        "                        # Only replace the missing values\n",
        "                        df_imputed.loc[missing_mask, col] = df_num_imputed.loc[missing_mask, col].values\n",
        "                        print(f\"MICE imputation completed for {missing_mask.sum()} missing values in '{col}'\")\n",
        "\n",
        "                        # Special handling for integer columns\n",
        "                        if col in integer_columns:\n",
        "                            df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "                            print(f\"{col} column rounded to integers\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in MICE imputation: {str(e)}\")\n",
        "                print(\"Falling back to simple mean imputation for numerical data\")\n",
        "\n",
        "                # Fallback to simple mean imputation\n",
        "                for col in num_cols_for_mice:\n",
        "                    if df_imputed[col].isnull().sum() > 0:\n",
        "                        mean_value = df_imputed[col].mean()\n",
        "                        df_imputed[col] = df_imputed[col].fillna(mean_value)\n",
        "                        print(f\"Imputed missing values in '{col}' with mean: {mean_value:.2f}\")\n",
        "\n",
        "                        # Special handling for integer columns in fallback\n",
        "                        if col in integer_columns:\n",
        "                            df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "                            print(f\"{col} column rounded to integers (fallback)\")\n",
        "\n",
        "    # Post-processing: ensure integer columns are properly converted\n",
        "    for col in integer_columns:\n",
        "        if col in df_imputed.columns:\n",
        "            df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "            print(f\"Final check: {col} column converted to integers\")\n",
        "\n",
        "    # Add a single column with imputed column names\n",
        "    df_imputed['imputed_columns'] = ''\n",
        "\n",
        "    # For each row, create a list of imputed columns\n",
        "    for idx in df_imputed.index:\n",
        "        imputed_cols = []\n",
        "        for col, missing_mask in missing_tracker.items():\n",
        "            if missing_mask[idx]:\n",
        "                imputed_cols.append(col)\n",
        "\n",
        "        # Update the summary column\n",
        "        if imputed_cols:\n",
        "            df_imputed.loc[idx, 'imputed_columns'] = ', '.join(imputed_cols)\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# Apply enhanced imputation with tracking to both datasets\n",
        "print(\"\\nPerforming enhanced imputation with tracking on Order Data...\")\n",
        "order_data_imputed = enhanced_imputation_with_tracking(order_data)\n",
        "\n",
        "print(\"\\nPerforming enhanced imputation with tracking on Product Data...\")\n",
        "product_data_imputed = enhanced_imputation_with_tracking(product_data)\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Remaining Missing Values (original columns):\")\n",
        "remaining_order = order_data_imputed[order_data.columns].isnull().sum()\n",
        "print(remaining_order[remaining_order > 0])\n",
        "if order_data_imputed[order_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Order Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {order_data_imputed[order_data.columns].isnull().sum().sum()} missing values remain in Order Data\")\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values (original columns):\")\n",
        "remaining_product = product_data_imputed[product_data.columns].isnull().sum()\n",
        "print(remaining_product[remaining_product > 0])\n",
        "if product_data_imputed[product_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Product Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {product_data_imputed[product_data.columns].isnull().sum().sum()} missing values remain in Product Data\")\n",
        "\n",
        "# Check for any non-integer values in Aging and Quantity columns\n",
        "for col in ['Quantity', 'Aging']:\n",
        "    if col in order_data_imputed.columns:\n",
        "        non_integer_mask = order_data_imputed[col] != order_data_imputed[col].astype(int)\n",
        "        non_integer_count = non_integer_mask.sum()\n",
        "        if non_integer_count > 0:\n",
        "            print(f\"\\nWARNING: Found {non_integer_count} non-integer values in {col} column after imputation!\")\n",
        "            print(\"Converting these to integers now...\")\n",
        "            order_data_imputed[col] = order_data_imputed[col].round().astype(int)\n",
        "        else:\n",
        "            print(f\"\\nAll {col} values are proper integers.\")\n",
        "\n",
        "# Check price distribution after imputation\n",
        "if 'price' in product_data_imputed.columns:\n",
        "    original_prices = product_data['price'].dropna()\n",
        "    imputed_prices = product_data_imputed.loc[product_data['price'].isnull(), 'price']\n",
        "\n",
        "    print(\"\\nPRICE IMPUTATION STATISTICS:\")\n",
        "    print(f\"Original prices - Mean: {original_prices.mean():.2f}, Min: {original_prices.min():.2f}, Max: {original_prices.max():.2f}\")\n",
        "    print(f\"Imputed prices - Mean: {imputed_prices.mean():.2f}, Min: {imputed_prices.min():.2f}, Max: {imputed_prices.max():.2f}\")\n",
        "\n",
        "    # Count number of unique imputed prices\n",
        "    unique_imputed_prices = imputed_prices.nunique()\n",
        "    print(f\"Number of unique imputed prices: {unique_imputed_prices}\")\n",
        "\n",
        "    # Price variation check\n",
        "    if unique_imputed_prices < 10:\n",
        "        print(\"WARNING: Low variation in imputed prices detected.\")\n",
        "    else:\n",
        "        print(\"GOOD: Imputed prices show good variation.\")\n",
        "\n",
        "# Save the clean datasets with imputation tracking\n",
        "print(\"\\nSaving clean datasets with imputation tracking...\")\n",
        "order_data_imputed.to_csv('Order_Data_Dataset_Enhanced_Imputed.csv', index=False)\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_Enhanced_Imputed.csv', index=False)\n",
        "print(\"Clean datasets saved with tracking information!\")\n",
        "\n",
        "print(\"\\nEnhanced imputation with tracking complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t4u-rB6_lVp",
        "outputId": "b43581e9-1260-49ab-9a5e-20d5d8f18b90"
      },
      "id": "_t4u-rB6_lVp",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "INITIAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Missing Values:\n",
            "Aging             1\n",
            "Sales             1\n",
            "Quantity          2\n",
            "Discount          1\n",
            "Shipping_Cost     1\n",
            "Order_Priority    2\n",
            "dtype: int64\n",
            "\n",
            "Product Data Missing Values:\n",
            "main_category      30\n",
            "price            1266\n",
            "store               1\n",
            "dtype: int64\n",
            "\n",
            "Performing enhanced imputation with tracking on Order Data...\n",
            "Imputed missing values in 'Order_Priority' with mode: Medium\n",
            "MICE imputation completed for 1 missing values in 'Aging'\n",
            "Aging column rounded to integers\n",
            "MICE imputation completed for 1 missing values in 'Sales'\n",
            "MICE imputation completed for 2 missing values in 'Quantity'\n",
            "Quantity column rounded to integers\n",
            "MICE imputation completed for 1 missing values in 'Discount'\n",
            "MICE imputation completed for 1 missing values in 'Shipping_Cost'\n",
            "Final check: Quantity column converted to integers\n",
            "Final check: Aging column converted to integers\n",
            "\n",
            "Performing enhanced imputation with tracking on Product Data...\n",
            "Imputed missing values in 'main_category' with mode: Musical Instruments\n",
            "Imputed missing values in 'store' with mode: Fender\n",
            "Applying specialized imputation for price column...\n",
            "Completed specialized imputation for 1266 missing prices\n",
            "\n",
            "FINAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Remaining Missing Values (original columns):\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Order Data!\n",
            "\n",
            "Product Data Remaining Missing Values (original columns):\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Product Data!\n",
            "\n",
            "All Quantity values are proper integers.\n",
            "\n",
            "All Aging values are proper integers.\n",
            "\n",
            "PRICE IMPUTATION STATISTICS:\n",
            "Original prices - Mean: 56.51, Min: 1.99, Max: 1328.00\n",
            "Imputed prices - Mean: 56.06, Min: 10.57, Max: 92.24\n",
            "Number of unique imputed prices: 953\n",
            "GOOD: Imputed prices show good variation.\n",
            "\n",
            "Saving clean datasets with imputation tracking...\n",
            "Clean datasets saved with tracking information!\n",
            "\n",
            "Enhanced imputation with tracking complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "\n",
        "# Function to perform enhanced imputation with tracking\n",
        "def enhanced_imputation_with_tracking(df):\n",
        "    # Store original missing value locations before imputation\n",
        "    missing_tracker = {}\n",
        "    for col in df.columns:\n",
        "        if df[col].isnull().any():\n",
        "            missing_tracker[col] = df[col].isnull()\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # Identify columns that should be integers\n",
        "    integer_columns = []\n",
        "    if 'Quantity' in df_imputed.columns:\n",
        "        integer_columns.append('Quantity')\n",
        "    if 'Aging' in df_imputed.columns:\n",
        "        integer_columns.append('Aging')\n",
        "    if 'Sales' in df_imputed.columns:\n",
        "        integer_columns.append('Sales')\n",
        "\n",
        "    # Separate numerical and categorical columns\n",
        "    num_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()\n",
        "    cat_cols = df_imputed.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Handle text columns with simple imputation first\n",
        "    text_columns = ['description', 'features', 'details', 'title']\n",
        "    for col in [c for c in text_columns if c in cat_cols]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            if col == 'title':\n",
        "                df_imputed[col] = df_imputed[col].fillna(\"Unnamed Product\")\n",
        "            else:\n",
        "                placeholder = f\"No {col} available\"\n",
        "                df_imputed[col] = df_imputed[col].fillna(placeholder)\n",
        "            print(f\"Imputed missing values in '{col}' with placeholder text\")\n",
        "\n",
        "    # Handle remaining categorical columns with mode imputation\n",
        "    for col in [c for c in cat_cols if c not in text_columns]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            mode_value = df_imputed[col].mode()[0]\n",
        "            df_imputed[col] = df_imputed[col].fillna(mode_value)\n",
        "            print(f\"Imputed missing values in '{col}' with mode: {mode_value}\")\n",
        "\n",
        "    # Special KNN handling for price column\n",
        "    if 'price' in df_imputed.columns and df_imputed['price'].isnull().any():\n",
        "        print(\"Applying KNN imputation for price column...\")\n",
        "\n",
        "        # Create a mask for missing prices\n",
        "        price_missing_mask = df_imputed['price'].isnull()\n",
        "        price_missing_count = price_missing_mask.sum()\n",
        "\n",
        "        try:\n",
        "            # Prepare features for KNN\n",
        "            features_for_knn = []\n",
        "\n",
        "            # Use numeric features if available\n",
        "            numeric_features = ['average_rating', 'rating_number']\n",
        "            for feature in numeric_features:\n",
        "                if feature in df_imputed.columns:\n",
        "                    features_for_knn.append(feature)\n",
        "\n",
        "            # If there are categorical features that might correlate with price, encode them\n",
        "            categorical_features = []\n",
        "            if 'main_category' in df_imputed.columns:\n",
        "                # Create dummy variables for main_category\n",
        "                category_dummies = pd.get_dummies(df_imputed['main_category'], prefix='cat')\n",
        "                df_imputed = pd.concat([df_imputed, category_dummies], axis=1)\n",
        "                categorical_features.extend(category_dummies.columns.tolist())\n",
        "\n",
        "            # If store is available and might correlate with price\n",
        "            if 'store' in df_imputed.columns:\n",
        "                # Create dummy variables for store\n",
        "                store_dummies = pd.get_dummies(df_imputed['store'], prefix='store')\n",
        "                df_imputed = pd.concat([df_imputed, store_dummies], axis=1)\n",
        "                categorical_features.extend(store_dummies.columns.tolist())\n",
        "\n",
        "            # Combine all features for KNN\n",
        "            all_features = features_for_knn + categorical_features\n",
        "\n",
        "            if all_features:\n",
        "                # Select features and scale them\n",
        "                X = df_imputed[all_features].copy()\n",
        "                scaler = StandardScaler()\n",
        "                X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "                # Use KNN imputation specifically for price\n",
        "                knn_imputer = KNNImputer(n_neighbors=21, weights='distance')\n",
        "\n",
        "                # Create a dataframe with scaled features and price\n",
        "                knn_df = pd.DataFrame(X_scaled, columns=all_features)\n",
        "                knn_df['price'] = df_imputed['price']\n",
        "\n",
        "                # Impute price\n",
        "                knn_imputed = knn_imputer.fit_transform(knn_df)\n",
        "\n",
        "                # Update only missing prices\n",
        "                df_imputed.loc[price_missing_mask, 'price'] = pd.DataFrame(knn_imputed, columns=list(all_features) + ['price']).loc[price_missing_mask, 'price'].values\n",
        "\n",
        "                print(f\"KNN imputation completed for {price_missing_count} missing prices\")\n",
        "\n",
        "                # Ensure consistency for prices by product title if needed\n",
        "                if 'title' in df_imputed.columns:\n",
        "                    # Group by title and use the median imputed price for duplicate products\n",
        "                    title_groups = df_imputed.groupby('title')\n",
        "\n",
        "                    for title, group in title_groups:\n",
        "                        if len(group) > 1 and group['price'].std() > 0:\n",
        "                            # Check if this title had any imputed prices\n",
        "                            if any(price_missing_mask[group.index]):\n",
        "                                # Find median price for this title\n",
        "                                median_price = group['price'].median()\n",
        "\n",
        "                                # Update all imputed prices for this title to be the same\n",
        "                                df_imputed.loc[group.index & price_missing_mask, 'price'] = median_price\n",
        "                                print(f\"Standardized price for multiple instances of product: {title}\")\n",
        "\n",
        "            else:\n",
        "                # Fallback to mean imputation with some variation if no features available\n",
        "                mean_price = df_imputed['price'].dropna().mean()\n",
        "                for idx in df_imputed[price_missing_mask].index:\n",
        "                    variation = np.random.uniform(0.85, 1.15)\n",
        "                    df_imputed.loc[idx, 'price'] = round(mean_price * variation, 2)\n",
        "                print(f\"No suitable features found for KNN. Used mean imputation with variation for {price_missing_count} missing prices\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in KNN price imputation: {str(e)}\")\n",
        "            print(\"Falling back to mean imputation for prices\")\n",
        "\n",
        "            # Fallback to mean imputation\n",
        "            mean_price = df_imputed['price'].dropna().mean()\n",
        "            df_imputed.loc[price_missing_mask, 'price'] = mean_price\n",
        "            print(f\"Imputed {price_missing_count} missing prices with mean: {mean_price:.2f}\")\n",
        "\n",
        "    # Handle remaining numerical columns with MICE\n",
        "    num_cols_for_mice = [col for col in num_cols if col != 'price']\n",
        "\n",
        "    if num_cols_for_mice and df_imputed[num_cols_for_mice].isnull().sum().sum() > 0:\n",
        "        # Create a numerical-only dataframe\n",
        "        df_num = df_imputed[num_cols_for_mice].copy()\n",
        "\n",
        "        # First use SimpleImputer for initial values\n",
        "        initial_imputer = SimpleImputer(strategy='mean')\n",
        "        df_num_initial = pd.DataFrame(\n",
        "            initial_imputer.fit_transform(df_num),\n",
        "            columns=num_cols_for_mice\n",
        "        )\n",
        "\n",
        "        # Now apply MICE to the initially imputed data\n",
        "        try:\n",
        "            mice_imputer = IterativeImputer(\n",
        "                estimator=RandomForestRegressor(n_estimators=50, random_state=42),\n",
        "                max_iter=10,\n",
        "                random_state=42,\n",
        "                skip_complete=True\n",
        "            )\n",
        "\n",
        "            # Fit and transform the data\n",
        "            df_num_imputed = pd.DataFrame(\n",
        "                mice_imputer.fit_transform(df_num_initial),\n",
        "                columns=num_cols_for_mice\n",
        "            )\n",
        "\n",
        "            # Replace only the originally missing values\n",
        "            for col in num_cols_for_mice:\n",
        "                missing_mask = df[col].isnull()\n",
        "                if missing_mask.any():\n",
        "                    # Only replace the missing values\n",
        "                    df_imputed.loc[missing_mask, col] = df_num_imputed.loc[missing_mask, col].values\n",
        "                    print(f\"MICE imputation completed for {missing_mask.sum()} missing values in '{col}'\")\n",
        "\n",
        "                    # Special handling for integer columns\n",
        "                    if col in integer_columns:\n",
        "                        df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "                        print(f\"{col} column rounded to integers\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in MICE imputation: {str(e)}\")\n",
        "            print(\"Falling back to simple mean imputation for numerical data\")\n",
        "\n",
        "            # Fallback to simple mean imputation\n",
        "            for col in num_cols_for_mice:\n",
        "                if df_imputed[col].isnull().sum() > 0:\n",
        "                    mean_value = df_imputed[col].mean()\n",
        "                    df_imputed[col] = df_imputed[col].fillna(mean_value)\n",
        "                    print(f\"Imputed missing values in '{col}' with mean: {mean_value:.2f}\")\n",
        "\n",
        "                    # Special handling for integer columns in fallback\n",
        "                    if col in integer_columns:\n",
        "                        df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "                        print(f\"{col} column rounded to integers (fallback)\")\n",
        "\n",
        "    # Post-processing: ensure integer columns are properly converted\n",
        "    for col in integer_columns:\n",
        "        if col in df_imputed.columns:\n",
        "            df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "            print(f\"Final check: {col} column converted to integers\")\n",
        "\n",
        "    # Drop any dummy columns that were added for KNN\n",
        "    original_columns = list(df.columns) + ['imputed_columns']\n",
        "    columns_to_keep = [col for col in original_columns if col in df_imputed.columns]\n",
        "    df_imputed = df_imputed[columns_to_keep]\n",
        "\n",
        "    # Add a single column with imputed column names\n",
        "    df_imputed['imputed_columns'] = ''\n",
        "\n",
        "    # For each row, create a list of imputed columns\n",
        "    for idx in df_imputed.index:\n",
        "        imputed_cols = []\n",
        "        for col, missing_mask in missing_tracker.items():\n",
        "            if missing_mask[idx]:\n",
        "                imputed_cols.append(col)\n",
        "\n",
        "        # Update the summary column\n",
        "        if imputed_cols:\n",
        "            df_imputed.loc[idx, 'imputed_columns'] = ', '.join(imputed_cols)\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# Apply enhanced imputation with tracking to both datasets\n",
        "print(\"\\nPerforming enhanced imputation with tracking on Order Data...\")\n",
        "order_data_imputed = enhanced_imputation_with_tracking(order_data)\n",
        "\n",
        "print(\"\\nPerforming enhanced imputation with tracking on Product Data...\")\n",
        "product_data_imputed = enhanced_imputation_with_tracking(product_data)\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Remaining Missing Values (original columns):\")\n",
        "remaining_order = order_data_imputed[order_data.columns].isnull().sum()\n",
        "print(remaining_order[remaining_order > 0])\n",
        "if order_data_imputed[order_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Order Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {order_data_imputed[order_data.columns].isnull().sum().sum()} missing values remain in Order Data\")\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values (original columns):\")\n",
        "remaining_product = product_data_imputed[product_data.columns].isnull().sum()\n",
        "print(remaining_product[remaining_product > 0])\n",
        "if product_data_imputed[product_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Product Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {product_data_imputed[product_data.columns].isnull().sum().sum()} missing values remain in Product Data\")\n",
        "\n",
        "# Check for any non-integer values in integer columns\n",
        "integer_columns = ['Quantity', 'Aging', 'Sales']\n",
        "for col in integer_columns:\n",
        "    if col in order_data_imputed.columns:\n",
        "        non_integer_mask = order_data_imputed[col] != order_data_imputed[col].astype(int)\n",
        "        non_integer_count = non_integer_mask.sum()\n",
        "        if non_integer_count > 0:\n",
        "            print(f\"\\nWARNING: Found {non_integer_count} non-integer values in {col} column after imputation!\")\n",
        "            print(\"Converting these to integers now...\")\n",
        "            order_data_imputed[col] = order_data_imputed[col].round().astype(int)\n",
        "        else:\n",
        "            print(f\"\\nAll {col} values are proper integers.\")\n",
        "\n",
        "# Check price consistency for products with the same title\n",
        "if 'title' in product_data_imputed.columns and 'price' in product_data_imputed.columns:\n",
        "    print(\"\\nCHECKING PRICE CONSISTENCY:\")\n",
        "\n",
        "    # Group by title and check for price variation\n",
        "    title_groups = product_data_imputed.groupby('title')\n",
        "    inconsistent_products = []\n",
        "\n",
        "    for title, group in title_groups:\n",
        "        if len(group) > 1:\n",
        "            price_std = group['price'].std()\n",
        "            if price_std > 0.01:  # Allow for tiny floating point differences\n",
        "                inconsistent_products.append({\n",
        "                    'title': title,\n",
        "                    'count': len(group),\n",
        "                    'min_price': group['price'].min(),\n",
        "                    'max_price': group['price'].max(),\n",
        "                    'std_dev': price_std\n",
        "                })\n",
        "\n",
        "    if inconsistent_products:\n",
        "        print(f\"Found {len(inconsistent_products)} products with inconsistent prices:\")\n",
        "        for i, prod in enumerate(inconsistent_products[:5]):  # Show first 5 examples\n",
        "            print(f\"{i+1}. {prod['title']}: {prod['count']} instances, prices from ${prod['min_price']:.2f} to ${prod['max_price']:.2f}\")\n",
        "\n",
        "        if len(inconsistent_products) > 5:\n",
        "            print(f\"...and {len(inconsistent_products) - 5} more.\")\n",
        "\n",
        "        print(\"\\nStandardizing prices for products with the same title...\")\n",
        "\n",
        "        # Standardize prices\n",
        "        for title, group in title_groups:\n",
        "            if len(group) > 1:\n",
        "                # Use the median price for all instances of this product\n",
        "                median_price = group['price'].median()\n",
        "                product_data_imputed.loc[group.index, 'price'] = median_price\n",
        "\n",
        "        print(\"Price standardization complete.\")\n",
        "    else:\n",
        "        print(\"All products with the same title have consistent prices.\")\n",
        "\n",
        "# Save the clean datasets with imputation tracking\n",
        "print(\"\\nSaving clean datasets with imputation tracking...\")\n",
        "order_data_imputed.to_csv('Order_Data_Dataset_KNN_Imputed.csv', index=False)\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_KNN_Imputed.csv', index=False)\n",
        "print(\"Clean datasets saved with tracking information!\")\n",
        "\n",
        "print(\"\\nEnhanced imputation with tracking complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvsRKHavGybJ",
        "outputId": "84c7343a-85e5-40b8-b70f-71b9eb60a5e2"
      },
      "id": "hvsRKHavGybJ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "INITIAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Missing Values:\n",
            "Aging             1\n",
            "Sales             1\n",
            "Quantity          2\n",
            "Discount          1\n",
            "Shipping_Cost     1\n",
            "Order_Priority    2\n",
            "dtype: int64\n",
            "\n",
            "Product Data Missing Values:\n",
            "main_category      30\n",
            "price            1266\n",
            "store               1\n",
            "dtype: int64\n",
            "\n",
            "Performing enhanced imputation with tracking on Order Data...\n",
            "Imputed missing values in 'Order_Priority' with mode: Medium\n",
            "MICE imputation completed for 1 missing values in 'Aging'\n",
            "Aging column rounded to integers\n",
            "MICE imputation completed for 1 missing values in 'Sales'\n",
            "Sales column rounded to integers\n",
            "MICE imputation completed for 2 missing values in 'Quantity'\n",
            "Quantity column rounded to integers\n",
            "MICE imputation completed for 1 missing values in 'Discount'\n",
            "MICE imputation completed for 1 missing values in 'Shipping_Cost'\n",
            "Final check: Quantity column converted to integers\n",
            "Final check: Aging column converted to integers\n",
            "Final check: Sales column converted to integers\n",
            "\n",
            "Performing enhanced imputation with tracking on Product Data...\n",
            "Imputed missing values in 'main_category' with mode: Musical Instruments\n",
            "Imputed missing values in 'store' with mode: Fender\n",
            "Applying KNN imputation for price column...\n",
            "KNN imputation completed for 1266 missing prices\n",
            "Error in KNN price imputation: operands could not be broadcast together with shapes (2,) (5000,) \n",
            "Falling back to mean imputation for prices\n",
            "Imputed 1266 missing prices with mean: 67.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b214c1c853b0>:225: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_imputed['imputed_columns'] = ''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FINAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Remaining Missing Values (original columns):\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Order Data!\n",
            "\n",
            "Product Data Remaining Missing Values (original columns):\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Product Data!\n",
            "\n",
            "All Quantity values are proper integers.\n",
            "\n",
            "All Aging values are proper integers.\n",
            "\n",
            "All Sales values are proper integers.\n",
            "\n",
            "CHECKING PRICE CONSISTENCY:\n",
            "Found 40 products with inconsistent prices:\n",
            "1. 1/4 TRS to XLR Female Adapter Female XLR to 1/4 Stereo Balanced Audio Connector - 2 Pack: 2 instances, prices from $8.99 to $67.11\n",
            "2. Acme Metropolitan (Bobby) Police Standard Whistle 15: 3 instances, prices from $13.52 to $14.58\n",
            "3. Acoustic Foam Panels 24 Pack 2”x12”x12” Sound Proof Padding Soundproofing Studio Foam Wedges (24 Square Feet): 2 instances, prices from $22.99 to $67.11\n",
            "4. Acoustic Panels Studio Foam Sound Proof Panels Nosie Dampening Foam Studio Music Equipment Acoustical Foam 12 Pack: 3 instances, prices from $11.99 to $67.11\n",
            "5. Al Cass Valve Oil, 2.0 fluid Oz.: 3 instances, prices from $8.15 to $67.11\n",
            "...and 35 more.\n",
            "\n",
            "Standardizing prices for products with the same title...\n",
            "Price standardization complete.\n",
            "\n",
            "Saving clean datasets with imputation tracking...\n",
            "Clean datasets saved with tracking information!\n",
            "\n",
            "Enhanced imputation with tracking complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN for price"
      ],
      "metadata": {
        "id": "kQFH4HekKdpY"
      },
      "id": "kQFH4HekKdpY"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the product dataset\n",
        "print(\"Loading datasets...\")\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Define a function to evaluate different K values for KNN imputation\n",
        "def find_optimal_k_for_price_imputation(df, k_values=range(1, 21)):\n",
        "    \"\"\"\n",
        "    Find the optimal K value for KNN imputation of price using the elbow method\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame - The product dataset\n",
        "    k_values : iterable - Range of K values to test\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    errors : dict - Dictionary of K values and their corresponding errors\n",
        "    \"\"\"\n",
        "    # Only use data with non-null prices for evaluation\n",
        "    df_complete = df.dropna(subset=['price']).copy()\n",
        "\n",
        "    # Create artificial missing values for evaluation\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    mask = np.random.rand(len(df_complete)) < 0.2  # Mask 20% of the data\n",
        "\n",
        "    # Store true values for later comparison\n",
        "    true_prices = df_complete.loc[mask, 'price'].copy()\n",
        "\n",
        "    # Create testing dataframe with artificially masked prices\n",
        "    df_test = df_complete.copy()\n",
        "    df_test.loc[mask, 'price'] = np.nan\n",
        "\n",
        "    # Prepare features for KNN\n",
        "    features_for_knn = []\n",
        "\n",
        "    # Use numeric features if available\n",
        "    numeric_features = ['average_rating', 'rating_number']\n",
        "    for feature in numeric_features:\n",
        "        if feature in df_test.columns:\n",
        "            features_for_knn.append(feature)\n",
        "\n",
        "    # Encode categorical features\n",
        "    categorical_features = []\n",
        "    if 'main_category' in df_test.columns:\n",
        "        # Create dummy variables for main_category\n",
        "        category_dummies = pd.get_dummies(df_test['main_category'], prefix='cat')\n",
        "        df_test = pd.concat([df_test, category_dummies], axis=1)\n",
        "        categorical_features.extend(category_dummies.columns.tolist())\n",
        "\n",
        "    if 'store' in df_test.columns:\n",
        "        # Create dummy variables for store\n",
        "        store_dummies = pd.get_dummies(df_test['store'], prefix='store')\n",
        "        df_test = pd.concat([df_test, store_dummies], axis=1)\n",
        "        categorical_features.extend(store_dummies.columns.tolist())\n",
        "\n",
        "    # Combine all features for KNN\n",
        "    all_features = features_for_knn + categorical_features\n",
        "\n",
        "    # Scale the features\n",
        "    X = df_test[all_features].copy()\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Store errors for each K value\n",
        "    errors = {}\n",
        "\n",
        "    # Test different K values\n",
        "    for k in k_values:\n",
        "        print(f\"Testing K={k}...\")\n",
        "\n",
        "        # Use KNN imputation with the current K\n",
        "        knn_imputer = KNNImputer(n_neighbors=k, weights='distance')\n",
        "\n",
        "        # Create a dataframe with scaled features and price\n",
        "        knn_df = pd.DataFrame(X_scaled, columns=all_features, index=df_test.index)\n",
        "        knn_df['price'] = df_test['price']\n",
        "\n",
        "        # Impute price\n",
        "        knn_imputed = knn_imputer.fit_transform(knn_df)\n",
        "\n",
        "        # Extract imputed prices\n",
        "        imputed_prices = pd.DataFrame(knn_imputed, columns=list(all_features) + ['price'], index=df_test.index).loc[mask, 'price'].values\n",
        "\n",
        "        # Calculate RMSE\n",
        "        rmse = np.sqrt(mean_squared_error(true_prices, imputed_prices))\n",
        "        errors[k] = rmse\n",
        "\n",
        "        print(f\"K={k}, RMSE={rmse:.4f}\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "# Test different K values and plot the results\n",
        "k_values = list(range(1, 16))  # Test K from 1 to 15\n",
        "print(\"\\nEvaluating different K values for KNN price imputation using Elbow Method...\")\n",
        "errors = find_optimal_k_for_price_imputation(product_data, k_values)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(errors.keys()), list(errors.values()), marker='o')\n",
        "plt.title('Elbow Method for Optimal K in KNN Price Imputation')\n",
        "plt.xlabel('Number of Neighbors (K)')\n",
        "plt.ylabel('Root Mean Squared Error (RMSE)')\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "plt.savefig('knn_elbow_method.png')\n",
        "plt.close()\n",
        "\n",
        "# Find the optimal K (elbow point)\n",
        "# One simple approach is to find the point where the rate of RMSE decrease slows down\n",
        "rmse_values = list(errors.values())\n",
        "rmse_diff = [rmse_values[i] - rmse_values[i+1] for i in range(len(rmse_values)-1)]\n",
        "rmse_diff_rate = [rmse_diff[i]/rmse_diff[i+1] if rmse_diff[i+1] != 0 else float('inf') for i in range(len(rmse_diff)-1)]\n",
        "\n",
        "# Find where the improvement rate drops below a threshold\n",
        "threshold = 1.2  # If improvement is less than 20% of previous improvement\n",
        "optimal_idx = 0\n",
        "for i, rate in enumerate(rmse_diff_rate):\n",
        "    if rate < threshold:\n",
        "        optimal_idx = i + 1  # +1 because diff list is shorter\n",
        "        break\n",
        "\n",
        "optimal_k = k_values[optimal_idx]\n",
        "print(f\"\\nOptimal K value according to the elbow method: {optimal_k}\")\n",
        "print(f\"RMSE at K={optimal_k}: {errors[optimal_k]:.4f}\")\n",
        "\n",
        "# Now run the final imputation with the optimal K\n",
        "print(f\"\\nRunning final price imputation with optimal K={optimal_k}...\")\n",
        "\n",
        "# Now integrate this optimal K value into your main imputation function\n",
        "\n",
        "print(\"\\nThe elbow curve has been saved to 'knn_elbow_method.png'. You can review it to confirm the optimal K value.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsbTaunaKcES",
        "outputId": "a5be91c9-d602-4e8c-a0cc-f789b9890377"
      },
      "id": "VsbTaunaKcES",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "Evaluating different K values for KNN price imputation using Elbow Method...\n",
            "Testing K=1...\n",
            "K=1, RMSE=75.4389\n",
            "Testing K=2...\n",
            "K=2, RMSE=66.0887\n",
            "Testing K=3...\n",
            "K=3, RMSE=63.5699\n",
            "Testing K=4...\n",
            "K=4, RMSE=62.7642\n",
            "Testing K=5...\n",
            "K=5, RMSE=62.3543\n",
            "Testing K=6...\n",
            "K=6, RMSE=62.1727\n",
            "Testing K=7...\n",
            "K=7, RMSE=61.6075\n",
            "Testing K=8...\n",
            "K=8, RMSE=61.3171\n",
            "Testing K=9...\n",
            "K=9, RMSE=60.5745\n",
            "Testing K=10...\n",
            "K=10, RMSE=60.2628\n",
            "Testing K=11...\n",
            "K=11, RMSE=60.0695\n",
            "Testing K=12...\n",
            "K=12, RMSE=60.0000\n",
            "Testing K=13...\n",
            "K=13, RMSE=59.9236\n",
            "Testing K=14...\n",
            "K=14, RMSE=59.8064\n",
            "Testing K=15...\n",
            "K=15, RMSE=59.7134\n",
            "\n",
            "Optimal K value according to the elbow method: 6\n",
            "RMSE at K=6: 62.1727\n",
            "\n",
            "Running final price imputation with optimal K=6...\n",
            "\n",
            "The elbow curve has been saved to 'knn_elbow_method.png'. You can review it to confirm the optimal K value.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Load the product dataset\n",
        "print(\"Loading datasets...\")\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Define a function to evaluate different K values for KNN imputation\n",
        "def find_optimal_k_for_price_imputation(df, k_values=range(1, 21)):\n",
        "    \"\"\"\n",
        "    Find the optimal K value for KNN imputation of price using the elbow method\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame - The product dataset\n",
        "    k_values : iterable - Range of K values to test\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    errors : dict - Dictionary of K values and their corresponding errors\n",
        "    \"\"\"\n",
        "    # Only use data with non-null prices for evaluation\n",
        "    df_complete = df.dropna(subset=['price']).copy()\n",
        "\n",
        "    # Create artificial missing values for evaluation\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    mask = np.random.rand(len(df_complete)) < 0.2  # Mask 20% of the data\n",
        "\n",
        "    # Store true values for later comparison\n",
        "    true_prices = df_complete.loc[mask, 'price'].copy()\n",
        "\n",
        "    # Create testing dataframe with artificially masked prices\n",
        "    df_test = df_complete.copy()\n",
        "    df_test.loc[mask, 'price'] = np.nan\n",
        "\n",
        "    # Prepare features for KNN\n",
        "    features_for_knn = []\n",
        "\n",
        "    # Use numeric features if available\n",
        "    numeric_features = ['average_rating', 'rating_number']\n",
        "    for feature in numeric_features:\n",
        "        if feature in df_test.columns:\n",
        "            features_for_knn.append(feature)\n",
        "\n",
        "    # Encode categorical features\n",
        "    categorical_features = []\n",
        "    if 'main_category' in df_test.columns:\n",
        "        # Create dummy variables for main_category\n",
        "        category_dummies = pd.get_dummies(df_test['main_category'], prefix='cat')\n",
        "        df_test = pd.concat([df_test, category_dummies], axis=1)\n",
        "        categorical_features.extend(category_dummies.columns.tolist())\n",
        "\n",
        "    if 'store' in df_test.columns:\n",
        "        # Create dummy variables for store\n",
        "        store_dummies = pd.get_dummies(df_test['store'], prefix='store')\n",
        "        df_test = pd.concat([df_test, store_dummies], axis=1)\n",
        "        categorical_features.extend(store_dummies.columns.tolist())\n",
        "\n",
        "    # Combine all features for KNN\n",
        "    all_features = features_for_knn + categorical_features\n",
        "\n",
        "    # Scale the features\n",
        "    X = df_test[all_features].copy()\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Store errors for each K value\n",
        "    errors = {}\n",
        "\n",
        "    # Test different K values\n",
        "    for k in k_values:\n",
        "        print(f\"Testing K={k}...\")\n",
        "\n",
        "        # Use KNN imputation with the current K\n",
        "        knn_imputer = KNNImputer(n_neighbors=k, weights='distance')\n",
        "\n",
        "        # Create a dataframe with scaled features and price\n",
        "        knn_df = pd.DataFrame(X_scaled, columns=all_features, index=df_test.index)\n",
        "        knn_df['price'] = df_test['price']\n",
        "\n",
        "        # Impute price\n",
        "        knn_imputed = knn_imputer.fit_transform(knn_df)\n",
        "\n",
        "        # Extract imputed prices\n",
        "        imputed_prices = pd.DataFrame(knn_imputed, columns=list(all_features) + ['price'], index=df_test.index).loc[mask, 'price'].values\n",
        "\n",
        "        # Calculate RMSE\n",
        "        rmse = np.sqrt(mean_squared_error(true_prices, imputed_prices))\n",
        "        errors[k] = rmse\n",
        "\n",
        "        print(f\"K={k}, RMSE={rmse:.4f}\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "# Test different K values and plot the results\n",
        "k_values = list(range(1, 16))  # Test K from 1 to 15\n",
        "print(\"\\nEvaluating different K values for KNN price imputation using Elbow Method...\")\n",
        "errors = find_optimal_k_for_price_imputation(product_data, k_values)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(errors.keys()), list(errors.values()), marker='o')\n",
        "plt.title('Elbow Method for Optimal K in KNN Price Imputation')\n",
        "plt.xlabel('Number of Neighbors (K)')\n",
        "plt.ylabel('Root Mean Squared Error (RMSE)')\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "plt.savefig('knn_elbow_method.png')\n",
        "plt.close()\n",
        "\n",
        "# Find the optimal K (elbow point)\n",
        "# One simple approach is to find the point where the rate of RMSE decrease slows down\n",
        "rmse_values = list(errors.values())\n",
        "rmse_diff = [rmse_values[i] - rmse_values[i+1] for i in range(len(rmse_values)-1)]\n",
        "rmse_diff_rate = [rmse_diff[i]/rmse_diff[i+1] if rmse_diff[i+1] != 0 else float('inf') for i in range(len(rmse_diff)-1)]\n",
        "\n",
        "# Find where the improvement rate drops below a threshold\n",
        "threshold = 1.2  # If improvement is less than 20% of previous improvement\n",
        "optimal_idx = 0\n",
        "for i, rate in enumerate(rmse_diff_rate):\n",
        "    if rate < threshold:\n",
        "        optimal_idx = i + 1  # +1 because diff list is shorter\n",
        "        break\n",
        "\n",
        "optimal_k = k_values[optimal_idx]\n",
        "print(f\"\\nOptimal K value according to the elbow method: {optimal_k}\")\n",
        "print(f\"RMSE at K={optimal_k}: {errors[optimal_k]:.4f}\")\n",
        "\n",
        "# Now we'll implement an improved KNN imputation with cluster-based variation\n",
        "\n",
        "def improved_knn_price_imputation(df, optimal_k):\n",
        "    \"\"\"\n",
        "    Perform KNN imputation for price column with added variation based on product clusters\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame - The product dataset\n",
        "    optimal_k : int - Optimal K value for KNN imputation\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_imputed : pandas DataFrame - Dataset with imputed prices\n",
        "    \"\"\"\n",
        "    # Create a copy of the dataframe\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # Create a mask for missing prices\n",
        "    price_missing_mask = df_imputed['price'].isnull()\n",
        "    price_missing_count = price_missing_mask.sum()\n",
        "\n",
        "    if price_missing_count == 0:\n",
        "        print(\"No missing prices to impute.\")\n",
        "        return df_imputed\n",
        "\n",
        "    print(f\"Imputing {price_missing_count} missing prices...\")\n",
        "\n",
        "    # Prepare features for KNN\n",
        "    features_for_knn = []\n",
        "\n",
        "    # Use numeric features if available\n",
        "    numeric_features = ['average_rating', 'rating_number']\n",
        "    for feature in numeric_features:\n",
        "        if feature in df_imputed.columns:\n",
        "            features_for_knn.append(feature)\n",
        "\n",
        "    # Encode categorical features\n",
        "    categorical_features = []\n",
        "    if 'main_category' in df_imputed.columns:\n",
        "        # Create dummy variables for main_category\n",
        "        category_dummies = pd.get_dummies(df_imputed['main_category'], prefix='cat')\n",
        "        df_imputed = pd.concat([df_imputed, category_dummies], axis=1)\n",
        "        categorical_features.extend(category_dummies.columns.tolist())\n",
        "\n",
        "    if 'store' in df_imputed.columns:\n",
        "        # Create dummy variables for store\n",
        "        store_dummies = pd.get_dummies(df_imputed['store'], prefix='store')\n",
        "        df_imputed = pd.concat([df_imputed, store_dummies], axis=1)\n",
        "        categorical_features.extend(store_dummies.columns.tolist())\n",
        "\n",
        "    # Combine all features for KNN\n",
        "    all_features = features_for_knn + categorical_features\n",
        "\n",
        "    if not all_features:\n",
        "        print(\"No suitable features found for KNN imputation.\")\n",
        "        return df_imputed\n",
        "\n",
        "    # Scale the features\n",
        "    X = df_imputed[all_features].copy()\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Step 1: First use standard KNN imputation to get initial values\n",
        "    knn_imputer = KNNImputer(n_neighbors=optimal_k, weights='distance')\n",
        "\n",
        "    # Create a dataframe with scaled features and price\n",
        "    knn_df = pd.DataFrame(X_scaled, columns=all_features)\n",
        "    knn_df['price'] = df_imputed['price']\n",
        "\n",
        "    # Impute price\n",
        "    knn_imputed = knn_imputer.fit_transform(knn_df)\n",
        "\n",
        "    # Get initial imputed prices\n",
        "    initial_imputed_prices = pd.DataFrame(knn_imputed, columns=list(all_features) + ['price'])['price'].values\n",
        "\n",
        "    # Step 2: Identify clusters in the data\n",
        "    # First, determine optimal number of clusters\n",
        "    cluster_data = X_scaled\n",
        "\n",
        "    # Use products with known prices to determine price distribution within clusters\n",
        "    non_missing_mask = ~price_missing_mask\n",
        "\n",
        "    # Try different numbers of clusters\n",
        "    max_clusters = min(10, sum(non_missing_mask) // 20)  # Limit clusters based on data size\n",
        "    inertia = []\n",
        "\n",
        "    for n_clusters in range(2, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        kmeans.fit(cluster_data[non_missing_mask])\n",
        "        inertia.append(kmeans.inertia_)\n",
        "\n",
        "    # Find elbow point for optimal clusters\n",
        "    inertia_diff = [inertia[i] - inertia[i+1] for i in range(len(inertia)-1)]\n",
        "    inertia_diff_rate = [inertia_diff[i]/inertia_diff[i+1] if inertia_diff[i+1] != 0 else float('inf')\n",
        "                         for i in range(len(inertia_diff)-1)]\n",
        "\n",
        "    # Find where the improvement rate drops below a threshold\n",
        "    threshold = 1.2  # If improvement is less than 20% of previous improvement\n",
        "    optimal_clusters_idx = 0\n",
        "    for i, rate in enumerate(inertia_diff_rate):\n",
        "        if rate < threshold:\n",
        "            optimal_clusters_idx = i + 1  # +1 because diff list is shorter\n",
        "            break\n",
        "\n",
        "    optimal_clusters = optimal_clusters_idx + 2  # +2 because we started from 2 clusters\n",
        "\n",
        "    print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
        "\n",
        "    # Apply KMeans with optimal number of clusters\n",
        "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
        "    df_imputed['cluster'] = kmeans.fit_predict(cluster_data)\n",
        "\n",
        "    # Step 3: For each cluster, analyze price distribution\n",
        "    cluster_price_stats = {}\n",
        "\n",
        "    for cluster in range(optimal_clusters):\n",
        "        # Get products in this cluster with known prices\n",
        "        cluster_mask = (df_imputed['cluster'] == cluster) & non_missing_mask\n",
        "        if sum(cluster_mask) > 0:\n",
        "            cluster_prices = df_imputed.loc[cluster_mask, 'price']\n",
        "            cluster_price_stats[cluster] = {\n",
        "                'mean': cluster_prices.mean(),\n",
        "                'std': max(cluster_prices.std(), 1.0),  # Ensure some minimum variation\n",
        "                'min': cluster_prices.min(),\n",
        "                'max': cluster_prices.max()\n",
        "            }\n",
        "        else:\n",
        "            # If no products with known prices in this cluster, use overall stats\n",
        "            cluster_price_stats[cluster] = {\n",
        "                'mean': df_imputed.loc[non_missing_mask, 'price'].mean(),\n",
        "                'std': max(df_imputed.loc[non_missing_mask, 'price'].std(), 1.0),\n",
        "                'min': df_imputed.loc[non_missing_mask, 'price'].min(),\n",
        "                'max': df_imputed.loc[non_missing_mask, 'price'].max()\n",
        "            }\n",
        "\n",
        "    # Step 4: Add appropriate variation to imputed prices\n",
        "    missing_indices = df_imputed[price_missing_mask].index\n",
        "\n",
        "    for idx in missing_indices:\n",
        "        cluster = df_imputed.loc[idx, 'cluster']\n",
        "        base_price = initial_imputed_prices[idx]\n",
        "\n",
        "        # Get statistics for this cluster\n",
        "        stats = cluster_price_stats[cluster]\n",
        "\n",
        "        # Add some random variation based on the cluster's price distribution\n",
        "        # Use truncated normal distribution to keep prices within reasonable range\n",
        "        variation_scale = stats['std'] * 0.3  # Scale factor to control variation\n",
        "\n",
        "        # Ensure price stays within the range observed in the cluster\n",
        "        min_val = max(stats['min'] * 0.8, 0.1)  # Don't go below 0\n",
        "        max_val = stats['max'] * 1.2  # Allow some extrapolation\n",
        "\n",
        "        a = (min_val - base_price) / variation_scale\n",
        "        b = (max_val - base_price) / variation_scale\n",
        "\n",
        "        # Generate a random value from a truncated normal distribution\n",
        "        random_factor = stats['std'] * np.random.randn() / 5\n",
        "        varied_price = base_price + random_factor\n",
        "\n",
        "        # Ensure price is positive and within reasonable range\n",
        "        varied_price = max(varied_price, 0.1)\n",
        "\n",
        "        # Round to 2 decimal places for currency\n",
        "        varied_price = round(varied_price, 2)\n",
        "\n",
        "        # Update the price\n",
        "        df_imputed.loc[idx, 'price'] = varied_price\n",
        "\n",
        "    # Step 5: Ensure consistency for prices by product title\n",
        "    if 'title' in df_imputed.columns:\n",
        "        # Group by title\n",
        "        title_groups = df_imputed.groupby('title')\n",
        "\n",
        "        for title, group in title_groups:\n",
        "            if len(group) > 1:\n",
        "                # Check if any products in this group had missing prices\n",
        "                if any(price_missing_mask[group.index]):\n",
        "                    # Find a representative price for this title\n",
        "                    known_prices = group.loc[~price_missing_mask[group.index], 'price']\n",
        "\n",
        "                    if not known_prices.empty:\n",
        "                        # If there are known prices for this title, use their median\n",
        "                        representative_price = known_prices.median()\n",
        "                    else:\n",
        "                        # If all prices were missing, use the median of imputed prices\n",
        "                        representative_price = group['price'].median()\n",
        "\n",
        "                    # Add small variations around the representative price for each instance\n",
        "                    for idx in group.index[price_missing_mask[group.index]]:\n",
        "                        varied_price = representative_price * np.random.uniform(0.97, 1.03)\n",
        "                        df_imputed.loc[idx, 'price'] = round(varied_price, 2)\n",
        "\n",
        "    # Drop the temporary cluster column\n",
        "    df_imputed.drop('cluster', axis=1, inplace=True)\n",
        "\n",
        "    # Drop any dummy columns that were added for KNN\n",
        "    original_columns = list(df.columns)\n",
        "    columns_to_keep = [col for col in original_columns if col in df_imputed.columns]\n",
        "    df_imputed = df_imputed[columns_to_keep]\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# Run the improved KNN imputation\n",
        "print(\"\\nRunning improved KNN imputation with cluster-based variation...\")\n",
        "product_data_imputed = improved_knn_price_imputation(product_data, optimal_k)\n",
        "\n",
        "# Check the distribution of imputed prices\n",
        "original_prices = product_data['price'].dropna()\n",
        "imputed_prices = product_data_imputed.loc[product_data['price'].isnull(), 'price']\n",
        "\n",
        "# Plot distribution comparison\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(original_prices, bins=30, alpha=0.7, label='Original Prices')\n",
        "plt.title('Original Price Distribution')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(imputed_prices, bins=30, alpha=0.7, label='Imputed Prices', color='orange')\n",
        "plt.title('Imputed Price Distribution')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('price_distribution_comparison.png')\n",
        "plt.close()\n",
        "\n",
        "# Output statistics\n",
        "print(\"\\nPRICE IMPUTATION STATISTICS:\")\n",
        "print(f\"Original prices - Mean: {original_prices.mean():.2f}, Min: {original_prices.min():.2f}, Max: {original_prices.max():.2f}, Std: {original_prices.std():.2f}\")\n",
        "print(f\"Imputed prices - Mean: {imputed_prices.mean():.2f}, Min: {imputed_prices.min():.2f}, Max: {imputed_prices.max():.2f}, Std: {imputed_prices.std():.2f}\")\n",
        "\n",
        "# Count number of unique imputed prices\n",
        "unique_imputed_prices = imputed_prices.nunique()\n",
        "print(f\"Number of unique imputed prices: {unique_imputed_prices}\")\n",
        "print(f\"Number of missing prices imputed: {len(imputed_prices)}\")\n",
        "print(f\"Percentage of unique values: {(unique_imputed_prices / len(imputed_prices)) * 100:.2f}%\")\n",
        "\n",
        "# Save the imputed dataset\n",
        "print(\"\\nSaving imputed product dataset...\")\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_Improved_KNN_Imputed.csv', index=False)\n",
        "print(\"Imputed dataset saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0NeDdkAKgx2",
        "outputId": "c754c4d9-7e8f-420d-900e-faea4151cc92"
      },
      "id": "O0NeDdkAKgx2",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "Evaluating different K values for KNN price imputation using Elbow Method...\n",
            "Testing K=1...\n",
            "K=1, RMSE=75.4389\n",
            "Testing K=2...\n",
            "K=2, RMSE=66.0887\n",
            "Testing K=3...\n",
            "K=3, RMSE=63.5699\n",
            "Testing K=4...\n",
            "K=4, RMSE=62.7642\n",
            "Testing K=5...\n",
            "K=5, RMSE=62.3543\n",
            "Testing K=6...\n",
            "K=6, RMSE=62.1727\n",
            "Testing K=7...\n",
            "K=7, RMSE=61.6075\n",
            "Testing K=8...\n",
            "K=8, RMSE=61.3171\n",
            "Testing K=9...\n",
            "K=9, RMSE=60.5745\n",
            "Testing K=10...\n",
            "K=10, RMSE=60.2628\n",
            "Testing K=11...\n",
            "K=11, RMSE=60.0695\n",
            "Testing K=12...\n",
            "K=12, RMSE=60.0000\n",
            "Testing K=13...\n",
            "K=13, RMSE=59.9236\n",
            "Testing K=14...\n",
            "K=14, RMSE=59.8064\n",
            "Testing K=15...\n",
            "K=15, RMSE=59.7134\n",
            "\n",
            "Optimal K value according to the elbow method: 6\n",
            "RMSE at K=6: 62.1727\n",
            "\n",
            "Running improved KNN imputation with cluster-based variation...\n",
            "Imputing 1266 missing prices...\n",
            "Optimal number of clusters: 3\n",
            "\n",
            "PRICE IMPUTATION STATISTICS:\n",
            "Original prices - Mean: 56.51, Min: 1.99, Max: 1328.00, Std: 83.45\n",
            "Imputed prices - Mean: 111.56, Min: 0.10, Max: 758.55, Std: 95.42\n",
            "Number of unique imputed prices: 1166\n",
            "Number of missing prices imputed: 1266\n",
            "Percentage of unique values: 92.10%\n",
            "\n",
            "Saving imputed product dataset...\n",
            "Imputed dataset saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "\n",
        "# Define a function to evaluate different K values for KNN imputation\n",
        "def find_optimal_k_for_price_imputation(df, k_values=range(1, 21)):\n",
        "    \"\"\"\n",
        "    Find the optimal K value for KNN imputation of price using the elbow method\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame - The product dataset\n",
        "    k_values : iterable - Range of K values to test\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    errors : dict - Dictionary of K values and their corresponding errors\n",
        "    \"\"\"\n",
        "    # Only use data with non-null prices for evaluation\n",
        "    df_complete = df.dropna(subset=['price']).copy()\n",
        "\n",
        "    # Create artificial missing values for evaluation\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    mask = np.random.rand(len(df_complete)) < 0.2  # Mask 20% of the data\n",
        "\n",
        "    # Store true values for later comparison\n",
        "    true_prices = df_complete.loc[mask, 'price'].copy()\n",
        "\n",
        "    # Create testing dataframe with artificially masked prices\n",
        "    df_test = df_complete.copy()\n",
        "    df_test.loc[mask, 'price'] = np.nan\n",
        "\n",
        "    # Prepare features for KNN\n",
        "    features_for_knn = []\n",
        "\n",
        "    # Use numeric features if available\n",
        "    numeric_features = ['average_rating', 'rating_number']\n",
        "    for feature in numeric_features:\n",
        "        if feature in df_test.columns:\n",
        "            features_for_knn.append(feature)\n",
        "\n",
        "    # Encode categorical features\n",
        "    categorical_features = []\n",
        "    if 'main_category' in df_test.columns:\n",
        "        # Create dummy variables for main_category\n",
        "        category_dummies = pd.get_dummies(df_test['main_category'], prefix='cat')\n",
        "        df_test = pd.concat([df_test, category_dummies], axis=1)\n",
        "        categorical_features.extend(category_dummies.columns.tolist())\n",
        "\n",
        "    if 'store' in df_test.columns:\n",
        "        # Create dummy variables for store\n",
        "        store_dummies = pd.get_dummies(df_test['store'], prefix='store')\n",
        "        df_test = pd.concat([df_test, store_dummies], axis=1)\n",
        "        categorical_features.extend(store_dummies.columns.tolist())\n",
        "\n",
        "    # Combine all features for KNN\n",
        "    all_features = features_for_knn + categorical_features\n",
        "\n",
        "    # Scale the features\n",
        "    X = df_test[all_features].copy()\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Store errors for each K value\n",
        "    errors = {}\n",
        "\n",
        "    # Test different K values\n",
        "    for k in k_values:\n",
        "        print(f\"Testing K={k}...\")\n",
        "\n",
        "        # Use KNN imputation with the current K\n",
        "        knn_imputer = KNNImputer(n_neighbors=k, weights='distance')\n",
        "\n",
        "        # Create a dataframe with scaled features and price\n",
        "        knn_df = pd.DataFrame(X_scaled, columns=all_features, index=df_test.index)\n",
        "        knn_df['price'] = df_test['price']\n",
        "\n",
        "        # Impute price\n",
        "        knn_imputed = knn_imputer.fit_transform(knn_df)\n",
        "\n",
        "        # Extract imputed prices\n",
        "        imputed_prices = pd.DataFrame(knn_imputed, columns=list(all_features) + ['price'], index=df_test.index).loc[mask, 'price'].values\n",
        "\n",
        "        # Calculate RMSE\n",
        "        rmse = np.sqrt(mean_squared_error(true_prices, imputed_prices))\n",
        "        errors[k] = rmse\n",
        "\n",
        "        print(f\"K={k}, RMSE={rmse:.4f}\")\n",
        "\n",
        "    return errors\n",
        "\n",
        "# Function to perform enhanced imputation with tracking\n",
        "def enhanced_imputation_with_tracking(df, optimal_k=5):\n",
        "    \"\"\"\n",
        "    Perform improved KNN imputation with cluster-based variation and tracking of imputed columns\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame - The dataset to impute\n",
        "    optimal_k : int - Optimal K value for KNN imputation\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_imputed : pandas DataFrame - Dataset with imputed values and tracking column\n",
        "    \"\"\"\n",
        "    # Store original missing value locations before imputation\n",
        "    missing_tracker = {}\n",
        "    for col in df.columns:\n",
        "        if df[col].isnull().any():\n",
        "            missing_tracker[col] = df[col].isnull()\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # Identify columns that should be integers\n",
        "    integer_columns = []\n",
        "    if 'Quantity' in df_imputed.columns:\n",
        "        integer_columns.append('Quantity')\n",
        "    if 'Aging' in df_imputed.columns:\n",
        "        integer_columns.append('Aging')\n",
        "    if 'Sales' in df_imputed.columns:\n",
        "        integer_columns.append('Sales')\n",
        "\n",
        "    # Separate numerical and categorical columns\n",
        "    num_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()\n",
        "    cat_cols = df_imputed.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Handle text columns with simple imputation first\n",
        "    text_columns = ['description', 'features', 'details', 'title']\n",
        "    for col in [c for c in text_columns if c in cat_cols]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            if col == 'title':\n",
        "                df_imputed[col] = df_imputed[col].fillna(\"Unnamed Product\")\n",
        "            else:\n",
        "                placeholder = f\"No {col} available\"\n",
        "                df_imputed[col] = df_imputed[col].fillna(placeholder)\n",
        "            print(f\"Imputed missing values in '{col}' with placeholder text\")\n",
        "\n",
        "    # Handle remaining categorical columns with mode imputation\n",
        "    for col in [c for c in cat_cols if c not in text_columns]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            mode_value = df_imputed[col].mode()[0]\n",
        "            df_imputed[col] = df_imputed[col].fillna(mode_value)\n",
        "            print(f\"Imputed missing values in '{col}' with mode: {mode_value}\")\n",
        "\n",
        "    # Special handling for price column\n",
        "    if 'price' in df_imputed.columns and df_imputed['price'].isnull().any():\n",
        "        print(\"Applying improved KNN imputation for price column...\")\n",
        "\n",
        "        # Create a mask for missing prices\n",
        "        price_missing_mask = df_imputed['price'].isnull()\n",
        "        price_missing_count = price_missing_mask.sum()\n",
        "\n",
        "        try:\n",
        "            # Prepare features for KNN\n",
        "            features_for_knn = []\n",
        "\n",
        "            # Use numeric features if available\n",
        "            numeric_features = ['average_rating', 'rating_number']\n",
        "            for feature in numeric_features:\n",
        "                if feature in df_imputed.columns:\n",
        "                    features_for_knn.append(feature)\n",
        "\n",
        "            # Encode categorical features\n",
        "            categorical_features = []\n",
        "            if 'main_category' in df_imputed.columns:\n",
        "                # Create dummy variables for main_category\n",
        "                category_dummies = pd.get_dummies(df_imputed['main_category'], prefix='cat')\n",
        "                df_imputed = pd.concat([df_imputed, category_dummies], axis=1)\n",
        "                categorical_features.extend(category_dummies.columns.tolist())\n",
        "\n",
        "            if 'store' in df_imputed.columns:\n",
        "                # Create dummy variables for store\n",
        "                store_dummies = pd.get_dummies(df_imputed['store'], prefix='store')\n",
        "                df_imputed = pd.concat([df_imputed, store_dummies], axis=1)\n",
        "                categorical_features.extend(store_dummies.columns.tolist())\n",
        "\n",
        "            # Combine all features for KNN\n",
        "            all_features = features_for_knn + categorical_features\n",
        "\n",
        "            if all_features:\n",
        "                # Select features and scale them\n",
        "                X = df_imputed[all_features].copy()\n",
        "                scaler = StandardScaler()\n",
        "                X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "                # Step 1: First use standard KNN imputation to get initial values\n",
        "                knn_imputer = KNNImputer(n_neighbors=optimal_k, weights='distance')\n",
        "\n",
        "                # Create a dataframe with scaled features and price\n",
        "                knn_df = pd.DataFrame(X_scaled, columns=all_features)\n",
        "                knn_df['price'] = df_imputed['price']\n",
        "\n",
        "                # Impute price\n",
        "                knn_imputed = knn_imputer.fit_transform(knn_df)\n",
        "\n",
        "                # Get initial imputed prices\n",
        "                df_knn_imputed = pd.DataFrame(knn_imputed, columns=list(all_features) + ['price'], index=df_imputed.index)\n",
        "                initial_imputed_prices = df_knn_imputed['price'].values\n",
        "\n",
        "                # Step 2: Identify clusters in the data\n",
        "                # Use KMeans to find clusters\n",
        "                n_clusters = min(8, len(df_imputed) // 100)  # Rule of thumb for number of clusters\n",
        "                kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "                df_imputed['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "                # Step 3: Add variation based on cluster characteristics\n",
        "                # For each cluster, calculate price statistics\n",
        "                cluster_price_stats = {}\n",
        "\n",
        "                for cluster in range(n_clusters):\n",
        "                    # Get non-missing prices in this cluster\n",
        "                    cluster_prices = df_imputed.loc[(df_imputed['cluster'] == cluster) & (~price_missing_mask), 'price']\n",
        "\n",
        "                    if len(cluster_prices) > 0:\n",
        "                        cluster_price_stats[cluster] = {\n",
        "                            'mean': cluster_prices.mean(),\n",
        "                            'std': max(cluster_prices.std(), 1.0),  # Ensure some minimum variation\n",
        "                            'min': cluster_prices.min(),\n",
        "                            'max': cluster_prices.max()\n",
        "                        }\n",
        "                    else:\n",
        "                        # If no prices in this cluster, use overall stats\n",
        "                        all_prices = df_imputed.loc[~price_missing_mask, 'price']\n",
        "                        cluster_price_stats[cluster] = {\n",
        "                            'mean': all_prices.mean(),\n",
        "                            'std': max(all_prices.std(), 1.0),\n",
        "                            'min': all_prices.min(),\n",
        "                            'max': all_prices.max()\n",
        "                        }\n",
        "\n",
        "                # Step 4: Add appropriate variation to each imputed price\n",
        "                for idx in df_imputed[price_missing_mask].index:\n",
        "                    cluster = df_imputed.loc[idx, 'cluster']\n",
        "                    base_price = df_knn_imputed.loc[idx, 'price']\n",
        "\n",
        "                    # Get statistics for this cluster\n",
        "                    stats = cluster_price_stats[cluster]\n",
        "\n",
        "                    # Add controlled random variation\n",
        "                    # Scale the variation based on the cluster's standard deviation\n",
        "                    variation_factor = np.random.normal(0, 0.1)  # Normal distribution with mean 0, std 0.1\n",
        "                    varied_price = base_price * (1 + variation_factor)\n",
        "\n",
        "                    # Ensure the price stays within a reasonable range\n",
        "                    min_price = max(stats['min'] * 0.8, 0.1)  # Don't go below 0.1\n",
        "                    max_price = stats['max'] * 1.2  # Allow some extrapolation\n",
        "\n",
        "                    varied_price = max(min(varied_price, max_price), min_price)\n",
        "                    varied_price = round(varied_price, 2)  # Round to 2 decimal places\n",
        "\n",
        "                    # Update the price\n",
        "                    df_imputed.loc[idx, 'price'] = varied_price\n",
        "\n",
        "                # Step 5: Ensure consistency for products with the same title\n",
        "                if 'title' in df_imputed.columns:\n",
        "                    title_groups = df_imputed.groupby('title')\n",
        "\n",
        "                    for title, group in title_groups:\n",
        "                        if len(group) > 1:\n",
        "                            # Find products of this title with non-missing original prices\n",
        "                            known_prices = df.loc[group.index & ~price_missing_mask, 'price']\n",
        "\n",
        "                            if len(known_prices) > 0:\n",
        "                                # If we have known prices for this title, use their median\n",
        "                                median_price = known_prices.median()\n",
        "\n",
        "                                # Apply to all missing prices for this title with small variations\n",
        "                                for idx in group.index & price_missing_mask.index:\n",
        "                                    variation = np.random.uniform(0.98, 1.02)\n",
        "                                    df_imputed.loc[idx, 'price'] = round(median_price * variation, 2)\n",
        "\n",
        "                print(f\"Improved KNN imputation completed for {price_missing_count} missing prices\")\n",
        "\n",
        "            else:\n",
        "                # Fallback to mean imputation with variation\n",
        "                mean_price = df_imputed['price'].dropna().mean()\n",
        "                for idx in df_imputed[price_missing_mask].index:\n",
        "                    variation = np.random.uniform(0.7, 1.3)\n",
        "                    df_imputed.loc[idx, 'price'] = round(mean_price * variation, 2)\n",
        "                print(f\"No suitable features found for KNN. Used mean imputation with variation for {price_missing_count} missing prices\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in price imputation: {str(e)}\")\n",
        "            print(\"Falling back to mean imputation for prices\")\n",
        "\n",
        "            # Fallback to mean imputation\n",
        "            mean_price = df_imputed['price'].dropna().mean()\n",
        "            df_imputed.loc[price_missing_mask, 'price'] = mean_price\n",
        "            print(f\"Imputed {price_missing_count} missing prices with mean: {mean_price:.2f}\")\n",
        "\n",
        "        # Remove temporary cluster column if it exists\n",
        "        if 'cluster' in df_imputed.columns:\n",
        "            df_imputed.drop('cluster', axis=1, inplace=True)\n",
        "\n",
        "    # Handle remaining numerical columns with MICE or KNN\n",
        "    num_cols_for_impute = [col for col in num_cols if col != 'price']\n",
        "\n",
        "    if num_cols_for_impute and df_imputed[num_cols_for_impute].isnull().sum().sum() > 0:\n",
        "        print(\"Imputing remaining numerical columns...\")\n",
        "\n",
        "        # For simplicity, use KNN imputation for other numerical columns too\n",
        "        for col in num_cols_for_impute:\n",
        "            if df_imputed[col].isnull().sum() > 0:\n",
        "                missing_mask = df_imputed[col].isnull()\n",
        "                missing_count = missing_mask.sum()\n",
        "\n",
        "                try:\n",
        "                    # Use a simple KNN imputer for this column\n",
        "                    imputer = KNNImputer(n_neighbors=min(5, len(df_imputed) - missing_count - 1), weights='distance')\n",
        "                    df_imputed[col] = imputer.fit_transform(df_imputed[[col]].copy())\n",
        "                    print(f\"KNN imputation completed for {missing_count} missing values in '{col}'\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in KNN imputation for {col}: {str(e)}\")\n",
        "                    print(f\"Falling back to mean imputation for {col}\")\n",
        "\n",
        "                    # Fallback to mean imputation\n",
        "                    mean_value = df_imputed[col].dropna().mean()\n",
        "                    df_imputed.loc[missing_mask, col] = mean_value\n",
        "                    print(f\"Imputed {missing_count} missing values in '{col}' with mean: {mean_value:.2f}\")\n",
        "\n",
        "                # Handle integer columns\n",
        "                if col in integer_columns:\n",
        "                    df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "                    print(f\"{col} column rounded to integers\")\n",
        "\n",
        "    # Drop any dummy/temporary columns added during imputation\n",
        "    original_columns = list(df.columns)\n",
        "    extra_columns = ['imputed_columns']  # Columns to add\n",
        "    columns_to_keep = original_columns + extra_columns\n",
        "    df_imputed = df_imputed.reindex(columns=columns_to_keep, fill_value='')\n",
        "\n",
        "    # Add a single column with imputed column names\n",
        "    df_imputed['imputed_columns'] = ''\n",
        "\n",
        "    # For each row, create a list of imputed columns\n",
        "    for idx in df_imputed.index:\n",
        "        imputed_cols = []\n",
        "        for col, missing_mask in missing_tracker.items():\n",
        "            if idx in missing_mask.index and missing_mask[idx]:\n",
        "                imputed_cols.append(col)\n",
        "\n",
        "        # Update the summary column\n",
        "        if imputed_cols:\n",
        "            df_imputed.loc[idx, 'imputed_columns'] = ', '.join(imputed_cols)\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# Test different K values and find optimal K\n",
        "k_values = list(range(1, 11))  # Test K from 1 to 10\n",
        "print(\"\\nEvaluating different K values for KNN price imputation using Elbow Method...\")\n",
        "errors = find_optimal_k_for_price_imputation(product_data, k_values)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(list(errors.keys()), list(errors.values()), marker='o')\n",
        "plt.title('Elbow Method for Optimal K in KNN Price Imputation')\n",
        "plt.xlabel('Number of Neighbors (K)')\n",
        "plt.ylabel('Root Mean Squared Error (RMSE)')\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "plt.savefig('knn_elbow_method.png')\n",
        "plt.close()\n",
        "\n",
        "# Find the optimal K (elbow point)\n",
        "# Use a simple approach - find where the improvement slows down\n",
        "optimal_k = 5  # Default if we can't determine optimal K\n",
        "if len(errors) > 2:\n",
        "    rmse_values = list(errors.values())\n",
        "    rmse_diff = [rmse_values[i] - rmse_values[i+1] for i in range(len(rmse_values)-1)]\n",
        "\n",
        "    for i, diff in enumerate(rmse_diff):\n",
        "        if i > 0 and diff / rmse_diff[i-1] < 0.5:\n",
        "            optimal_k = k_values[i+1]\n",
        "            break\n",
        "\n",
        "print(f\"\\nOptimal K value according to the elbow method: {optimal_k}\")\n",
        "print(f\"RMSE at K={optimal_k}: {errors.get(optimal_k, 'N/A')}\")\n",
        "\n",
        "# Apply enhanced imputation with tracking to both datasets\n",
        "print(\"\\nPerforming enhanced imputation with tracking on Order Data...\")\n",
        "order_data_imputed = enhanced_imputation_with_tracking(order_data, optimal_k)\n",
        "\n",
        "print(\"\\nPerforming enhanced imputation with tracking on Product Data...\")\n",
        "product_data_imputed = enhanced_imputation_with_tracking(product_data, optimal_k)\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Remaining Missing Values (original columns):\")\n",
        "remaining_order = order_data_imputed[order_data.columns].isnull().sum()\n",
        "print(remaining_order[remaining_order > 0])\n",
        "if order_data_imputed[order_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Order Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {order_data_imputed[order_data.columns].isnull().sum().sum()} missing values remain in Order Data\")\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values (original columns):\")\n",
        "remaining_product = product_data_imputed[product_data.columns].isnull().sum()\n",
        "print(remaining_product[remaining_product > 0])\n",
        "if product_data_imputed[product_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Product Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {product_data_imputed[product_data.columns].isnull().sum().sum()} missing values remain in Product Data\")\n",
        "\n",
        "# Check for any non-integer values in integer columns\n",
        "integer_columns = ['Quantity', 'Aging', 'Sales']\n",
        "for col in integer_columns:\n",
        "    if col in order_data_imputed.columns:\n",
        "        non_integer_mask = order_data_imputed[col] != order_data_imputed[col].astype(int)\n",
        "        non_integer_count = non_integer_mask.sum()\n",
        "        if non_integer_count > 0:\n",
        "            print(f\"\\nWARNING: Found {non_integer_count} non-integer values in {col} column after imputation!\")\n",
        "            print(\"Converting these to integers now...\")\n",
        "            order_data_imputed[col] = order_data_imputed[col].round().astype(int)\n",
        "        else:\n",
        "            print(f\"\\nAll {col} values are proper integers.\")\n",
        "\n",
        "# Check price distribution after imputation\n",
        "if 'price' in product_data_imputed.columns:\n",
        "    original_prices = product_data['price'].dropna()\n",
        "    imputed_prices = product_data_imputed.loc[product_data['price'].isnull(), 'price']\n",
        "\n",
        "    # Plot histograms to compare distributions\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(original_prices, bins=30, alpha=0.7, label='Original')\n",
        "    plt.title('Original Price Distribution')\n",
        "    plt.xlabel('Price')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(imputed_prices, bins=30, alpha=0.7, label='Imputed', color='orange')\n",
        "    plt.title('Imputed Price Distribution')\n",
        "    plt.xlabel('Price')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('price_distributions.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\nPRICE IMPUTATION STATISTICS:\")\n",
        "    print(f\"Original prices - Mean: {original_prices.mean():.2f}, Min: {original_prices.min():.2f}, Max: {original_prices.max():.2f}\")\n",
        "    print(f\"Imputed prices - Mean: {imputed_prices.mean():.2f}, Min: {imputed_prices.min():.2f}, Max: {imputed_prices.max():.2f}\")\n",
        "\n",
        "    # Count number of unique imputed prices\n",
        "    unique_imputed_prices = imputed_prices.nunique()\n",
        "    print(f\"Number of unique imputed prices: {unique_imputed_prices}\")\n",
        "    print(f\"Percentage of unique values: {(unique_imputed_prices / len(imputed_prices)) * 100:.2f}%\")\n",
        "\n",
        "# Sample imputation tracking data\n",
        "print(\"\\nSample rows with imputed values:\")\n",
        "order_imputed_rows = order_data_imputed[order_data_imputed['imputed_columns'] != ''].head(3)\n",
        "if not order_imputed_rows.empty:\n",
        "    print(\"\\nOrder Data sample imputed rows:\")\n",
        "    for idx, row in order_imputed_rows.iterrows():\n",
        "        print(f\"Row {idx}: Imputed columns: {row['imputed_columns']}\")\n",
        "\n",
        "product_imputed_rows = product_data_imputed[product_data_imputed['imputed_columns'] != ''].head(3)\n",
        "if not product_imputed_rows.empty:\n",
        "    print(\"\\nProduct Data sample imputed rows:\")\n",
        "    for idx, row in product_imputed_rows.iterrows():\n",
        "        print(f\"Row {idx}: Imputed columns: {row['imputed_columns']}\")\n",
        "\n",
        "# Save the clean datasets with imputation tracking\n",
        "print(\"\\nSaving clean datasets with imputation tracking...\")\n",
        "order_data_imputed.to_csv('Order_Data_Dataset_Optimal_KNN_Imputed.csv', index=False)\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_Optimal_KNN_Imputed.csv', index=False)\n",
        "print(\"Clean datasets saved with tracking information!\")\n",
        "\n",
        "print(\"\\nKNN Imputation with tracking and optimal K complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75A5TbpwWatq",
        "outputId": "3264842d-5d3d-4200-f2fe-046244f1fe21"
      },
      "id": "75A5TbpwWatq",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "INITIAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Missing Values:\n",
            "Aging             1\n",
            "Sales             1\n",
            "Quantity          2\n",
            "Discount          1\n",
            "Shipping_Cost     1\n",
            "Order_Priority    2\n",
            "dtype: int64\n",
            "\n",
            "Product Data Missing Values:\n",
            "main_category      30\n",
            "price            1266\n",
            "store               1\n",
            "dtype: int64\n",
            "\n",
            "Evaluating different K values for KNN price imputation using Elbow Method...\n",
            "Testing K=1...\n",
            "K=1, RMSE=75.4389\n",
            "Testing K=2...\n",
            "K=2, RMSE=66.0887\n",
            "Testing K=3...\n",
            "K=3, RMSE=63.5699\n",
            "Testing K=4...\n",
            "K=4, RMSE=62.7642\n",
            "Testing K=5...\n",
            "K=5, RMSE=62.3543\n",
            "Testing K=6...\n",
            "K=6, RMSE=62.1727\n",
            "Testing K=7...\n",
            "K=7, RMSE=61.6075\n",
            "Testing K=8...\n",
            "K=8, RMSE=61.3171\n",
            "Testing K=9...\n",
            "K=9, RMSE=60.5745\n",
            "Testing K=10...\n",
            "K=10, RMSE=60.2628\n",
            "\n",
            "Optimal K value according to the elbow method: 3\n",
            "RMSE at K=3: 63.569926663954256\n",
            "\n",
            "Performing enhanced imputation with tracking on Order Data...\n",
            "Imputed missing values in 'Order_Priority' with mode: Medium\n",
            "Imputing remaining numerical columns...\n",
            "KNN imputation completed for 1 missing values in 'Aging'\n",
            "Aging column rounded to integers\n",
            "KNN imputation completed for 1 missing values in 'Sales'\n",
            "Sales column rounded to integers\n",
            "KNN imputation completed for 2 missing values in 'Quantity'\n",
            "Quantity column rounded to integers\n",
            "KNN imputation completed for 1 missing values in 'Discount'\n",
            "KNN imputation completed for 1 missing values in 'Shipping_Cost'\n",
            "\n",
            "Performing enhanced imputation with tracking on Product Data...\n",
            "Imputed missing values in 'main_category' with mode: Musical Instruments\n",
            "Imputed missing values in 'store' with mode: Fender\n",
            "Applying improved KNN imputation for price column...\n",
            "Error in price imputation: operands could not be broadcast together with shapes (2,) (5000,) \n",
            "Falling back to mean imputation for prices\n",
            "Imputed 1266 missing prices with mean: 64.49\n",
            "\n",
            "FINAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Remaining Missing Values (original columns):\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Order Data!\n",
            "\n",
            "Product Data Remaining Missing Values (original columns):\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Product Data!\n",
            "\n",
            "All Quantity values are proper integers.\n",
            "\n",
            "All Aging values are proper integers.\n",
            "\n",
            "All Sales values are proper integers.\n",
            "\n",
            "PRICE IMPUTATION STATISTICS:\n",
            "Original prices - Mean: 56.51, Min: 1.99, Max: 1328.00\n",
            "Imputed prices - Mean: 64.49, Min: 64.49, Max: 64.49\n",
            "Number of unique imputed prices: 1\n",
            "Percentage of unique values: 0.08%\n",
            "\n",
            "Sample rows with imputed values:\n",
            "\n",
            "Order Data sample imputed rows:\n",
            "Row 27: Imputed columns: Aging\n",
            "Row 95: Imputed columns: Quantity\n",
            "Row 211: Imputed columns: Discount\n",
            "\n",
            "Product Data sample imputed rows:\n",
            "Row 18: Imputed columns: price\n",
            "Row 51: Imputed columns: main_category\n",
            "Row 56: Imputed columns: price\n",
            "\n",
            "Saving clean datasets with imputation tracking...\n",
            "Clean datasets saved with tracking information!\n",
            "\n",
            "KNN Imputation with tracking and optimal K complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load the datasets\n",
        "print(\"Loading datasets...\")\n",
        "order_data = pd.read_csv('data/Order_Data_Dataset.csv')\n",
        "product_data = pd.read_csv('data/Product_Information_Dataset.csv')\n",
        "\n",
        "# Display initial missing value counts\n",
        "print(\"\\nINITIAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Missing Values:\")\n",
        "order_missing = order_data.isnull().sum()\n",
        "print(order_missing[order_missing > 0])\n",
        "\n",
        "print(\"\\nProduct Data Missing Values:\")\n",
        "product_missing = product_data.isnull().sum()\n",
        "print(product_missing[product_missing > 0])\n",
        "\n",
        "# Function to perform enhanced imputation with tracking\n",
        "def enhanced_imputation_with_tracking(df):\n",
        "    \"\"\"\n",
        "    Perform enhanced imputation with tracking of imputed columns\n",
        "    \"\"\"\n",
        "    # Store original missing value locations before imputation\n",
        "    missing_tracker = {}\n",
        "    for col in df.columns:\n",
        "        if df[col].isnull().any():\n",
        "            missing_tracker[col] = df[col].isnull()\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # Identify columns that should be integers\n",
        "    integer_columns = []\n",
        "    if 'Quantity' in df_imputed.columns:\n",
        "        integer_columns.append('Quantity')\n",
        "    if 'Aging' in df_imputed.columns:\n",
        "        integer_columns.append('Aging')\n",
        "    if 'Sales' in df_imputed.columns:\n",
        "        integer_columns.append('Sales')\n",
        "\n",
        "    # Separate numerical and categorical columns\n",
        "    num_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()\n",
        "    cat_cols = df_imputed.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Handle text columns with simple imputation first\n",
        "    text_columns = ['description', 'features', 'details', 'title']\n",
        "    for col in [c for c in text_columns if c in cat_cols]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            if col == 'title':\n",
        "                df_imputed[col] = df_imputed[col].fillna(\"Unnamed Product\")\n",
        "            else:\n",
        "                placeholder = f\"No {col} available\"\n",
        "                df_imputed[col] = df_imputed[col].fillna(placeholder)\n",
        "            print(f\"Imputed missing values in '{col}' with placeholder text\")\n",
        "\n",
        "    # Handle remaining categorical columns with mode imputation\n",
        "    for col in [c for c in cat_cols if c not in text_columns]:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            mode_value = df_imputed[col].mode()[0]\n",
        "            df_imputed[col] = df_imputed[col].fillna(mode_value)\n",
        "            print(f\"Imputed missing values in '{col}' with mode: {mode_value}\")\n",
        "\n",
        "    # Special handling for price column\n",
        "    if 'price' in df_imputed.columns and df_imputed['price'].isnull().any():\n",
        "        print(\"Applying robust price imputation...\")\n",
        "\n",
        "        # Create a mask for missing prices\n",
        "        price_missing_mask = df_imputed['price'].isnull()\n",
        "        price_missing_count = price_missing_mask.sum()\n",
        "\n",
        "        try:\n",
        "            # Prepare features for price imputation\n",
        "            numeric_features = ['average_rating', 'rating_number']\n",
        "            valid_numeric_features = [f for f in numeric_features if f in df_imputed.columns]\n",
        "\n",
        "            # Check if we have valid features for imputation\n",
        "            if valid_numeric_features and 'main_category' in df_imputed.columns:\n",
        "                # Get category-specific price statistics\n",
        "                category_stats = df_imputed.groupby('main_category')['price'].agg(['mean', 'std', 'count']).reset_index()\n",
        "                category_stats = category_stats[category_stats['count'] > 5]  # Only use categories with enough samples\n",
        "\n",
        "                # For each missing price, impute based on category statistics with added variation\n",
        "                for idx in df_imputed[price_missing_mask].index:\n",
        "                    category = df_imputed.loc[idx, 'main_category']\n",
        "\n",
        "                    # Find statistics for this category\n",
        "                    cat_stats = category_stats[category_stats['main_category'] == category]\n",
        "\n",
        "                    if len(cat_stats) > 0:\n",
        "                        # Use category-specific mean and std\n",
        "                        cat_mean = cat_stats['mean'].values[0]\n",
        "                        cat_std = cat_stats['std'].values[0]\n",
        "\n",
        "                        # Add controlled random variation\n",
        "                        if not np.isnan(cat_std):\n",
        "                            # Use smaller standard deviation for more conservative variation\n",
        "                            variation = np.random.normal(0, min(cat_std / 3, 10))\n",
        "                            imputed_price = cat_mean + variation\n",
        "                        else:\n",
        "                            # If std is NaN, just use slight variation on the mean\n",
        "                            variation = np.random.uniform(-5, 5)\n",
        "                            imputed_price = cat_mean + variation\n",
        "                    else:\n",
        "                        # If category not found in stats, use global mean with variation\n",
        "                        global_mean = df_imputed['price'].dropna().mean()\n",
        "                        variation = np.random.uniform(-10, 10)\n",
        "                        imputed_price = global_mean + variation\n",
        "\n",
        "                    # Ensure price is positive and round to 2 decimal places\n",
        "                    imputed_price = max(round(imputed_price, 2), 0.99)\n",
        "                    df_imputed.loc[idx, 'price'] = imputed_price\n",
        "\n",
        "                print(f\"Category-based price imputation completed for {price_missing_count} values\")\n",
        "\n",
        "                # Check consistency for products with same title\n",
        "                if 'title' in df_imputed.columns:\n",
        "                    title_counts = df_imputed['title'].value_counts()\n",
        "                    duplicate_titles = title_counts[title_counts > 1].index\n",
        "\n",
        "                    for title in duplicate_titles:\n",
        "                        # Get indices for this title\n",
        "                        title_indices = df_imputed[df_imputed['title'] == title].index\n",
        "\n",
        "                        # Check if any of these had imputed prices\n",
        "                        if any(price_missing_mask[title_indices]):\n",
        "                            # Get non-missing prices for this title\n",
        "                            known_prices = df_imputed.loc[title_indices & ~price_missing_mask.index, 'price']\n",
        "\n",
        "                            if len(known_prices) > 0:\n",
        "                                # Use median of known prices\n",
        "                                median_price = known_prices.median()\n",
        "\n",
        "                                # Update all imputed prices for this title with small variation\n",
        "                                for idx in title_indices & price_missing_mask.index:\n",
        "                                    small_var = np.random.uniform(0.97, 1.03)\n",
        "                                    df_imputed.loc[idx, 'price'] = round(median_price * small_var, 2)\n",
        "            else:\n",
        "                # Fallback to simple mean imputation with variation\n",
        "                mean_price = df_imputed['price'].dropna().mean()\n",
        "                std_price = df_imputed['price'].dropna().std()\n",
        "\n",
        "                for idx in df_imputed[price_missing_mask].index:\n",
        "                    # Add some variation to avoid all prices being the same\n",
        "                    variation = np.random.normal(0, min(std_price / 3, 10))\n",
        "                    varied_price = mean_price + variation\n",
        "                    varied_price = max(round(varied_price, 2), 0.99)  # Ensure positive and rounded\n",
        "                    df_imputed.loc[idx, 'price'] = varied_price\n",
        "\n",
        "                print(f\"Mean-based price imputation with variation completed for {price_missing_count} values\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in price imputation: {str(e)}\")\n",
        "            print(\"Falling back to mean imputation with variation for prices\")\n",
        "\n",
        "            # Fallback to mean with variation\n",
        "            mean_price = df_imputed['price'].dropna().mean()\n",
        "            std_price = max(df_imputed['price'].dropna().std() / 3, 5)\n",
        "\n",
        "            for idx in df_imputed[price_missing_mask].index:\n",
        "                variation = np.random.normal(0, std_price)\n",
        "                varied_price = mean_price + variation\n",
        "                varied_price = max(round(varied_price, 2), 0.99)\n",
        "                df_imputed.loc[idx, 'price'] = varied_price\n",
        "\n",
        "            print(f\"Fallback price imputation completed for {price_missing_count} values\")\n",
        "\n",
        "    # Handle remaining numerical columns\n",
        "    for col in [c for c in num_cols if c != 'price']:\n",
        "        if df_imputed[col].isnull().sum() > 0:\n",
        "            missing_mask = df_imputed[col].isnull()\n",
        "            missing_count = missing_mask.sum()\n",
        "\n",
        "            try:\n",
        "                # Use simple imputation for each numerical column\n",
        "                mean_value = df_imputed[col].dropna().mean()\n",
        "                std_value = df_imputed[col].dropna().std()\n",
        "\n",
        "                for idx in df_imputed[missing_mask].index:\n",
        "                    # Add some variation around the mean\n",
        "                    variation = np.random.normal(0, std_value / 3)\n",
        "                    imputed_value = mean_value + variation\n",
        "                    df_imputed.loc[idx, col] = imputed_value\n",
        "\n",
        "                print(f\"Mean-based imputation with variation completed for {missing_count} values in '{col}'\")\n",
        "\n",
        "                # Handle integer columns\n",
        "                if col in integer_columns:\n",
        "                    df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "                    print(f\"{col} column rounded to integers\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in imputation for {col}: {str(e)}\")\n",
        "                print(f\"Falling back to simple mean imputation for {col}\")\n",
        "\n",
        "                # Fallback to simple mean\n",
        "                mean_value = df_imputed[col].dropna().mean()\n",
        "                df_imputed.loc[missing_mask, col] = mean_value\n",
        "\n",
        "                # Handle integer columns\n",
        "                if col in integer_columns:\n",
        "                    df_imputed[col] = df_imputed[col].round().astype(int)\n",
        "                    print(f\"{col} column rounded to integers (fallback)\")\n",
        "\n",
        "    # Add a single column with imputed column names\n",
        "    df_imputed['imputed_columns'] = ''\n",
        "\n",
        "    # For each row, create a list of imputed columns\n",
        "    for idx in df_imputed.index:\n",
        "        imputed_cols = []\n",
        "        for col, missing_mask in missing_tracker.items():\n",
        "            if idx in missing_mask.index and missing_mask[idx]:\n",
        "                imputed_cols.append(col)\n",
        "\n",
        "        # Update the summary column\n",
        "        if imputed_cols:\n",
        "            df_imputed.loc[idx, 'imputed_columns'] = ', '.join(imputed_cols)\n",
        "\n",
        "    return df_imputed\n",
        "\n",
        "# Apply enhanced imputation with tracking to both datasets\n",
        "print(\"\\nPerforming enhanced imputation with tracking on Order Data...\")\n",
        "order_data_imputed = enhanced_imputation_with_tracking(order_data)\n",
        "\n",
        "print(\"\\nPerforming enhanced imputation with tracking on Product Data...\")\n",
        "product_data_imputed = enhanced_imputation_with_tracking(product_data)\n",
        "\n",
        "# Verify all missing values are gone\n",
        "print(\"\\nFINAL MISSING VALUES CHECK:\")\n",
        "print(\"\\nOrder Data Remaining Missing Values (original columns):\")\n",
        "remaining_order = order_data_imputed[order_data.columns].isnull().sum()\n",
        "print(remaining_order[remaining_order > 0])\n",
        "if order_data_imputed[order_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Order Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {order_data_imputed[order_data.columns].isnull().sum().sum()} missing values remain in Order Data\")\n",
        "\n",
        "print(\"\\nProduct Data Remaining Missing Values (original columns):\")\n",
        "remaining_product = product_data_imputed[product_data.columns].isnull().sum()\n",
        "print(remaining_product[remaining_product > 0])\n",
        "if product_data_imputed[product_data.columns].isnull().sum().sum() == 0:\n",
        "    print(\"SUCCESS: All missing values have been imputed in Product Data!\")\n",
        "else:\n",
        "    print(f\"WARNING: {product_data_imputed[product_data.columns].isnull().sum().sum()} missing values remain in Product Data\")\n",
        "\n",
        "# Check for any non-integer values in integer columns\n",
        "integer_columns = ['Quantity', 'Aging', 'Sales']\n",
        "for col in integer_columns:\n",
        "    if col in order_data_imputed.columns:\n",
        "        non_integer_mask = order_data_imputed[col] != order_data_imputed[col].astype(int)\n",
        "        non_integer_count = non_integer_mask.sum()\n",
        "        if non_integer_count > 0:\n",
        "            print(f\"\\nWARNING: Found {non_integer_count} non-integer values in {col} column after imputation!\")\n",
        "            print(\"Converting these to integers now...\")\n",
        "            order_data_imputed[col] = order_data_imputed[col].round().astype(int)\n",
        "        else:\n",
        "            print(f\"\\nAll {col} values are proper integers.\")\n",
        "\n",
        "# Check price distribution after imputation\n",
        "if 'price' in product_data_imputed.columns:\n",
        "    original_prices = product_data['price'].dropna()\n",
        "    imputed_prices = product_data_imputed.loc[product_data['price'].isnull(), 'price']\n",
        "\n",
        "    # Calculate statistics about imputed prices\n",
        "    print(\"\\nPRICE IMPUTATION STATISTICS:\")\n",
        "    print(f\"Original prices - Mean: {original_prices.mean():.2f}, Min: {original_prices.min():.2f}, Max: {original_prices.max():.2f}, Std: {original_prices.std():.2f}\")\n",
        "    print(f\"Imputed prices - Mean: {imputed_prices.mean():.2f}, Min: {imputed_prices.min():.2f}, Max: {imputed_prices.max():.2f}, Std: {imputed_prices.std():.2f}\")\n",
        "\n",
        "    # Count number of unique imputed prices\n",
        "    unique_imputed_prices = imputed_prices.nunique()\n",
        "    print(f\"Number of unique imputed prices: {unique_imputed_prices}\")\n",
        "    print(f\"Percentage of unique values: {(unique_imputed_prices / len(imputed_prices)) * 100:.2f}%\")\n",
        "\n",
        "    # Plot price distributions\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(original_prices, bins=30, alpha=0.7, label='Original Prices')\n",
        "    plt.title('Original Price Distribution')\n",
        "    plt.xlabel('Price')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(imputed_prices, bins=30, alpha=0.7, label='Imputed Prices', color='orange')\n",
        "    plt.title('Imputed Price Distribution')\n",
        "    plt.xlabel('Price')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('price_distribution_comparison.png')\n",
        "    plt.close()\n",
        "    print(\"Price distribution comparison saved to 'price_distribution_comparison.png'\")\n",
        "\n",
        "# Sample imputation tracking data\n",
        "print(\"\\nSample rows with imputed values:\")\n",
        "order_imputed_rows = order_data_imputed[order_data_imputed['imputed_columns'] != ''].head(3)\n",
        "if not order_imputed_rows.empty:\n",
        "    print(\"\\nOrder Data sample imputed rows:\")\n",
        "    for idx, row in order_imputed_rows.iterrows():\n",
        "        print(f\"Row {idx}: Imputed columns: {row['imputed_columns']}\")\n",
        "\n",
        "product_imputed_rows = product_data_imputed[product_data_imputed['imputed_columns'] != ''].head(3)\n",
        "if not product_imputed_rows.empty:\n",
        "    print(\"\\nProduct Data sample imputed rows:\")\n",
        "    for idx, row in product_imputed_rows.iterrows():\n",
        "        print(f\"Row {idx}: Imputed columns: {row['imputed_columns']}\")\n",
        "\n",
        "# Save the clean datasets with imputation tracking\n",
        "print(\"\\nSaving clean datasets with imputation tracking...\")\n",
        "order_data_imputed.to_csv('Order_Data_Dataset_Enhanced_Imputed.csv', index=False)\n",
        "product_data_imputed.to_csv('Product_Information_Dataset_Enhanced_Imputed.csv', index=False)\n",
        "print(\"Clean datasets saved with tracking information!\")\n",
        "\n",
        "print(\"\\nEnhanced imputation with tracking complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rvKqPa7bJPw",
        "outputId": "086d3976-b370-4d7d-9e55-7a2a601ed014"
      },
      "id": "5rvKqPa7bJPw",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "\n",
            "INITIAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Missing Values:\n",
            "Aging             1\n",
            "Sales             1\n",
            "Quantity          2\n",
            "Discount          1\n",
            "Shipping_Cost     1\n",
            "Order_Priority    2\n",
            "dtype: int64\n",
            "\n",
            "Product Data Missing Values:\n",
            "main_category      30\n",
            "price            1266\n",
            "store               1\n",
            "dtype: int64\n",
            "\n",
            "Performing enhanced imputation with tracking on Order Data...\n",
            "Imputed missing values in 'Order_Priority' with mode: Medium\n",
            "Mean-based imputation with variation completed for 1 values in 'Aging'\n",
            "Aging column rounded to integers\n",
            "Mean-based imputation with variation completed for 1 values in 'Sales'\n",
            "Sales column rounded to integers\n",
            "Mean-based imputation with variation completed for 2 values in 'Quantity'\n",
            "Quantity column rounded to integers\n",
            "Mean-based imputation with variation completed for 1 values in 'Discount'\n",
            "Mean-based imputation with variation completed for 1 values in 'Shipping_Cost'\n",
            "\n",
            "Performing enhanced imputation with tracking on Product Data...\n",
            "Imputed missing values in 'main_category' with mode: Musical Instruments\n",
            "Imputed missing values in 'store' with mode: Fender\n",
            "Applying robust price imputation...\n",
            "Category-based price imputation completed for 1266 values\n",
            "Error in price imputation: operands could not be broadcast together with shapes (8,) (5000,) \n",
            "Falling back to mean imputation with variation for prices\n",
            "Fallback price imputation completed for 1266 values\n",
            "\n",
            "FINAL MISSING VALUES CHECK:\n",
            "\n",
            "Order Data Remaining Missing Values (original columns):\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Order Data!\n",
            "\n",
            "Product Data Remaining Missing Values (original columns):\n",
            "Series([], dtype: int64)\n",
            "SUCCESS: All missing values have been imputed in Product Data!\n",
            "\n",
            "All Quantity values are proper integers.\n",
            "\n",
            "All Aging values are proper integers.\n",
            "\n",
            "All Sales values are proper integers.\n",
            "\n",
            "PRICE IMPUTATION STATISTICS:\n",
            "Original prices - Mean: 56.51, Min: 1.99, Max: 1328.00, Std: 83.45\n",
            "Imputed prices - Mean: 55.28, Min: 0.99, Max: 134.54, Std: 24.87\n",
            "Number of unique imputed prices: 1160\n",
            "Percentage of unique values: 91.63%\n",
            "Price distribution comparison saved to 'price_distribution_comparison.png'\n",
            "\n",
            "Sample rows with imputed values:\n",
            "\n",
            "Order Data sample imputed rows:\n",
            "Row 27: Imputed columns: Aging\n",
            "Row 95: Imputed columns: Quantity\n",
            "Row 211: Imputed columns: Discount\n",
            "\n",
            "Product Data sample imputed rows:\n",
            "Row 18: Imputed columns: price\n",
            "Row 51: Imputed columns: main_category\n",
            "Row 56: Imputed columns: price\n",
            "\n",
            "Saving clean datasets with imputation tracking...\n",
            "Clean datasets saved with tracking information!\n",
            "\n",
            "Enhanced imputation with tracking complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x0k_uO26chte"
      },
      "id": "x0k_uO26chte",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}