{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9e31bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"COHERE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "39a281f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain_core.tools import tool\n",
    "from datetime import date, datetime\n",
    "from typing import Optional\n",
    "\n",
    "import pytz\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# ==========================================\n",
    "# Configuration\n",
    "# ==========================================\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Service endpoints (can also be set via environment variables)\n",
    "PRODUCT_SERVICE_URL = os.getenv(\"PRODUCT_SERVICE_URL\", \"http://localhost:8001\")\n",
    "ORDER_SERVICE_URL   = os.getenv(\"ORDER_SERVICE_URL\",   \"http://localhost:8002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24511bae",
   "metadata": {},
   "source": [
    "# Product Service Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Product Service Tools\n",
    "# ==========================================\n",
    "\n",
    "@tool\n",
    "def search_products(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search for products based on a text query using RAG.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        top_k: Number of results to return (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of product dictionaries containing details like title, description, price, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{PRODUCT_SERVICE_URL}/search\",\n",
    "            params={\"query\": query, \"top_k\": top_k}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"results\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error searching products: {e}\")\n",
    "        return []\n",
    "\n",
    "@tool\n",
    "def search_product_by_category(category: str, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search for products in a specific category.\n",
    "    \n",
    "    Args:\n",
    "        category: Category to search in\n",
    "        query: Search query string\n",
    "        top_k: Number of results to return (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of product dictionaries containing details like title, description, price, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{PRODUCT_SERVICE_URL}/search/category\",\n",
    "            params={\"category\": category, \"query\": query, \"top_k\": top_k}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"results\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error searching products by category: {e}\")\n",
    "        return []\n",
    "\n",
    "@tool\n",
    "def get_top_rated_products(category: Optional[str] = None,\n",
    "                           min_rating: float = 4.5,\n",
    "                           top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get top-rated products, optionally filtered by category.\n",
    "    \n",
    "    Args:\n",
    "        category: Category to filter by (optional)\n",
    "        min_rating: Minimum rating threshold (default: 4.5)\n",
    "        top_k: Number of results to return (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of top-rated product dictionaries\n",
    "    \"\"\"\n",
    "    try:\n",
    "        params = {\"min_rating\": min_rating, \"top_k\": top_k}\n",
    "        if category:\n",
    "            params[\"category\"] = category\n",
    "        response = requests.get(f\"{PRODUCT_SERVICE_URL}/top-rated\", params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"results\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching top-rated products: {e}\")\n",
    "        return []\n",
    "\n",
    "# @tool\n",
    "# def get_product_details(product_id: int) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     Get detailed information about a specific product.\n",
    "    \n",
    "#     Args:\n",
    "#         product_id: ID of the product\n",
    "        \n",
    "#     Returns:\n",
    "#         Dictionary containing product details\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         response = requests.get(f\"{PRODUCT_SERVICE_URL}/product/{product_id}\")\n",
    "#         response.raise_for_status()\n",
    "#         return response.json()\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error fetching product details: {e}\")\n",
    "#         return {}\n",
    "\n",
    "@tool\n",
    "def get_specific_instrument_details(instrument_type: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get information about a specific type of musical instrument.\n",
    "    \n",
    "    Args:\n",
    "        instrument_type: Type of instrument (e.g., 'guitar', 'piano', 'drums')\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing instrument details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return search_products(instrument_type, top_k=3)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching specific instrument details: {e}\")\n",
    "        return []\n",
    "\n",
    "# @tool\n",
    "# def compare_products(product_ids: List[int]) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     Compare multiple products side by side.\n",
    "    \n",
    "#     Args:\n",
    "#         product_ids: List of product IDs to compare\n",
    "        \n",
    "#     Returns:\n",
    "#         List of dictionaries containing product details for comparison\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "#     for pid in product_ids:\n",
    "#         try:\n",
    "#             details = get_product_details(pid)\n",
    "#             if details:\n",
    "#                 results.append(details)\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error fetching product {pid} for comparison: {e}\")\n",
    "#     return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97dfec1",
   "metadata": {},
   "source": [
    "# Order Service Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b5e6ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Order Service Tools\n",
    "# ==========================================\n",
    "\n",
    "# @tool\n",
    "# def get_customer_orders(config: RunnableConfig) -> List[Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     Get all orders for a specific customer.\n",
    "    \n",
    "#     \"\"\"\n",
    "#     configuration = config.get(\"configurable\", {})\n",
    "#     customer_id = configuration.get(\"customer_id\", None)\n",
    "#     if not customer_id:\n",
    "#         # customer_id = input(\"Can you please enter your customer ID: \")\n",
    "        \n",
    "#         raise ValueError(\"No customer ID configured.\")\n",
    "\n",
    "#     try:\n",
    "#         response = requests.get(f\"{ORDER_SERVICE_URL}/customer/{customer_id}\")\n",
    "#         response.raise_for_status()\n",
    "#         return response.json().get(\"orders\", [])\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error fetching customer orders: {e}\")\n",
    "#         return []\n",
    "\n",
    "@tool\n",
    "def get_customer_orders(customer_id: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get all orders for a specific customer.\n",
    "    \n",
    "    Args:\n",
    "        customer_id: Customer ID\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing order information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{ORDER_SERVICE_URL}/customer/{customer_id}\")\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"orders\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching customer orders: {e}\")\n",
    "        return []\n",
    "\n",
    "@tool\n",
    "def get_customer_recent_order(customer_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get the most recent order for a specific customer.\n",
    "    \n",
    "    Args:\n",
    "        customer_id: Customer ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the most recent order details\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{ORDER_SERVICE_URL}/customer/{customer_id}/recent\")\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"order\", {})\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching recent order: {e}\")\n",
    "        return {}\n",
    "\n",
    "@tool\n",
    "def get_customer_product_orders(customer_id: int,\n",
    "                                product_keyword: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get orders containing a specific product for a customer.\n",
    "    \n",
    "    Args:\n",
    "        customer_id: Customer ID\n",
    "        product_keyword: Keyword to search in product name or category\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing matching order information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{ORDER_SERVICE_URL}/customer/{customer_id}/product\",\n",
    "            params={\"product_keyword\": product_keyword}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"orders\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching product orders: {e}\")\n",
    "        return []\n",
    "\n",
    "@tool\n",
    "def get_high_priority_orders(limit: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get recent high-priority orders.\n",
    "    \n",
    "    Args:\n",
    "        limit: Maximum number of orders to return (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing high-priority order information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{ORDER_SERVICE_URL}/high-priority\",\n",
    "            params={\"limit\": limit}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"orders\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching high-priority orders: {e}\")\n",
    "        return []\n",
    "\n",
    "@tool\n",
    "def get_sales_by_category() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get total sales data aggregated by product category.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with category and sales data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{ORDER_SERVICE_URL}/total-sales-by-category\")\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"categories\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching sales by category: {e}\")\n",
    "        return []\n",
    "\n",
    "@tool\n",
    "def get_high_profit_products(min_profit: float = 100.0) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get high-profit products.\n",
    "    \n",
    "    Args:\n",
    "        min_profit: Minimum profit threshold (default: 100.0)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing high-profit product order information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{ORDER_SERVICE_URL}/high-profit-products\",\n",
    "            params={\"min_profit\": min_profit}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"products\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching high-profit products: {e}\")\n",
    "        return []\n",
    "\n",
    "@tool\n",
    "def get_shipping_cost_summary() -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Get shipping cost summary (average, min, max).\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with shipping cost statistics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{ORDER_SERVICE_URL}/shipping-cost-summary\")\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching shipping cost summary: {e}\")\n",
    "        return {}\n",
    "\n",
    "@tool\n",
    "def get_profit_by_gender() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get total profit aggregated by customer gender.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with gender and profit data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{ORDER_SERVICE_URL}/profit-by-gender\")\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"genders\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching profit by gender: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5d313",
   "metadata": {},
   "source": [
    "# Combined Helper Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7eda9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Combined / Helper Tools\n",
    "# ==========================================\n",
    "\n",
    "@tool\n",
    "def check_product_availability(product_name: str,\n",
    "                               customer_id: Optional[int] = None\n",
    "                              ) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Check if a product is available and if the customer has ordered it before.\n",
    "    \n",
    "    Args:\n",
    "        product_name: Name of the product to check\n",
    "        customer_id: Optional customer ID to check order history\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with availability information and order history if applicable\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"product_found\": False,\n",
    "        \"product_details\": None,\n",
    "        \"previously_ordered\": False,\n",
    "        \"previous_orders\": []\n",
    "    }\n",
    "    \n",
    "    products = search_products(product_name)\n",
    "    if products:\n",
    "        result[\"product_found\"] = True\n",
    "        result[\"product_details\"] = products[0]\n",
    "    \n",
    "    if customer_id and result[\"product_found\"]:\n",
    "        orders = get_customer_product_orders(customer_id, product_name)\n",
    "        if orders:\n",
    "            result[\"previously_ordered\"] = True\n",
    "            result[\"previous_orders\"] = orders\n",
    "    \n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def recommend_similar_products(product_name: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Recommend products similar to the specified product.\n",
    "    \n",
    "    Args:\n",
    "        product_name: Name of the reference product\n",
    "        top_k: Number of recommendations to return (default: 3)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing recommended product details\n",
    "    \"\"\"\n",
    "    products = search_products(product_name, top_k=1)\n",
    "    if not products:\n",
    "        return []\n",
    "    category = products[0].get(\"main_category\", \"\")\n",
    "    if category:\n",
    "        return search_product_by_category(category, product_name, top_k=top_k)\n",
    "    return search_products(product_name, top_k=top_k+1)[1:]\n",
    "\n",
    "@tool\n",
    "def get_customer_order_summary(customer_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get a summary of a customer's order history.\n",
    "    \n",
    "    Args:\n",
    "        customer_id: Customer ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with order summary statistics\n",
    "    \"\"\"\n",
    "    orders = get_customer_orders(customer_id)\n",
    "    if not orders:\n",
    "        return {\n",
    "            \"customer_id\": customer_id,\n",
    "            \"total_orders\": 0,\n",
    "            \"message\": \"No order history found for this customer.\"\n",
    "        }\n",
    "    df = pd.DataFrame(orders)\n",
    "    summary = {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"total_orders\": len(df),\n",
    "        \"total_spend\": round(df.get(\"Sales\", 0).sum(), 2),\n",
    "        \"average_order_value\": round(df.get(\"Sales\", 0).mean(), 2),\n",
    "        \"total_shipping_cost\": round(df.get(\"Shipping_Cost\", 0).sum(), 2),\n",
    "    }\n",
    "    if \"Product_Category\" in df:\n",
    "        top_cats = df[\"Product_Category\"].value_counts().head(3).items()\n",
    "        summary[\"top_categories\"] = [{\"category\": c, \"count\": n} for c, n in top_cats]\n",
    "    if \"Order_Date\" in df:\n",
    "        df[\"Order_Date\"] = pd.to_datetime(df[\"Order_Date\"])\n",
    "        summary[\"most_recent_order_date\"] = df[\"Order_Date\"].max().strftime(\"%Y-%m-%d\")\n",
    "        summary[\"first_order_date\"]        = df[\"Order_Date\"].min().strftime(\"%Y-%m-%d\")\n",
    "    return summary\n",
    "\n",
    "@tool\n",
    "def search_and_get_rating_info(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get rating information for products matching the search query.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with rating statistics for matching products\n",
    "    \"\"\"\n",
    "    products = search_products(query)\n",
    "    if not products:\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"products_found\": 0,\n",
    "            \"message\": \"No products found matching the query.\"\n",
    "        }\n",
    "    ratings = [p.get(\"average_rating\", 0) for p in products if p.get(\"average_rating\") is not None]\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"products_found\": len(products),\n",
    "        \"average_rating\": round(sum(ratings) / len(ratings), 1) if ratings else 0,\n",
    "        \"highest_rated_product\": max(products, key=lambda p: p.get(\"average_rating\", 0)),\n",
    "        \"lowest_rated_product\":  min(products, key=lambda p: p.get(\"average_rating\", float('inf'))),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34817536",
   "metadata": {},
   "source": [
    "# Utitlities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "50b6b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\nWhat is your customer ID?\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41452a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "954955cb",
   "metadata": {},
   "source": [
    "# Assistant Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7e2b6",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f4bae69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "import time\n",
    "from datetime import datetime\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    user_info: str\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        while True:\n",
    "            result = self.runnable.invoke(state)\n",
    "            # If the LLM happens to return an empty response, we will re-prompt it\n",
    "            # for an actual response.\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "llm = ChatCohere(model=\"command-r\", temperature=1)\n",
    "\n",
    "assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful customer support assistant for an E-Commerce Website. \"\n",
    "            \" Use the provided tools to search for products, orders, and other information to assist the user's queries. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\n\\nCurrent user:\\n<User>\\n{user_info}\\n</User>\"\n",
    "            \"\\nCurrent time: {time}.\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ").partial(time=datetime.now)\n",
    "\n",
    "\n",
    "# \"Read\"-only tools (such as retrievers) don't need user information.\n",
    "product_service_tools = [\n",
    "    search_products,\n",
    "    search_product_by_category,\n",
    "    get_top_rated_products, \n",
    "    get_product_details,\n",
    "    get_specific_instrument_details, \n",
    "    compare_products\n",
    "]\n",
    "\n",
    "# These tools for order lookup.\n",
    "# The user will be able to provide their customer ID, and the assistant will be able to look up their order history.\n",
    "order_service_tools = [\n",
    "    get_customer_orders,\n",
    "    get_customer_recent_order, \n",
    "    get_customer_product_orders,\n",
    "    get_high_priority_orders,\n",
    "    get_sales_by_category,\n",
    "    get_high_profit_products,\n",
    "    get_shipping_cost_summary,\n",
    "    get_profit_by_gender\n",
    "]\n",
    "order_service_tool_names = {t.name for t in order_service_tools}\n",
    "# Our LLM doesn't have to know which nodes it has to route to. In its 'mind', it's just invoking functions.\n",
    "ecommerce_assistant_runnable = assistant_prompt | llm.bind_tools(\n",
    "    product_service_tools + order_service_tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "16e7e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(\"Hello, how can I help you today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca74c6",
   "metadata": {},
   "source": [
    "## Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "def user_info(state: State):\n",
    "    return {\"user_info\": get_customer_orders.invoke({})}\n",
    "\n",
    "\n",
    "# NEW: The fetch_user_info node runs first, meaning our assistant can see the user's order information without\n",
    "# having to take an action\n",
    "builder.add_node(\"fetch_user_info\", user_info)\n",
    "builder.add_edge(START, \"fetch_user_info\")\n",
    "builder.add_node(\"assistant\", Assistant(ecommerce_assistant_runnable))\n",
    "builder.add_node(\"product_service_tools\", create_tool_node_with_fallback(product_service_tools))\n",
    "builder.add_node(\n",
    "    \"order_service_tools\", create_tool_node_with_fallback(order_service_tools)\n",
    ")\n",
    "# Define logic\n",
    "builder.add_edge(\"fetch_user_info\", \"assistant\")\n",
    "\n",
    "\n",
    "def route_tools(state: State):\n",
    "    next_node = tools_condition(state)\n",
    "    # If no tools are invoked, return to the user\n",
    "    if next_node == END:\n",
    "        return END\n",
    "    ai_message = state[\"messages\"][-1]\n",
    "    # This assumes single tool calls. To handle parallel tool calling, you'd want to\n",
    "    # use an ANY condition\n",
    "    first_tool_call = ai_message.tool_calls[0]\n",
    "    if first_tool_call[\"name\"] in order_service_tool_names:\n",
    "        return \"order_service_tools\"\n",
    "    return \"product_service_tools\"\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\", route_tools, [\"product_service_tools\", \"order_service_tools\", END]\n",
    ")\n",
    "builder.add_edge(\"product_service_tools\", \"assistant\")\n",
    "builder.add_edge(\"order_service_tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "ecommerce_assistant_graph = builder.compile(\n",
    "    checkpointer=memory,\n",
    "    # NEW: The graph will always halt before executing the \"tools\" node.\n",
    "    # The user can approve or reject (or even alter the request) before\n",
    "    # the assistant continues\n",
    "    interrupt_before=[\"order_service_tools\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ba32f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# try:\n",
    "#     display(Image(ecommerce_assistant_graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     # This requires some extra dependencies and is optional\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c06033",
   "metadata": {},
   "source": [
    "# Example Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3b189cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi there, What are the details of my last order?\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for get_customer_orders\ncustomer_id\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m tutorial_questions:\n\u001b[32m     32\u001b[39m     events = ecommerce_assistant_graph.stream(\n\u001b[32m     33\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, question)}, config, stream_mode=\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_print_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_printed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     snapshot = ecommerce_assistant_graph.get_state(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2461\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2455\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2456\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2457\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2458\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2459\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2460\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2468\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\runner.py:153\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    151\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36muser_info\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34muser_info\u001b[39m(state: State):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33muser_info\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mget_customer_orders\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\tools\\base.py:513\u001b[39m, in \u001b[36mBaseTool.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    507\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs: Any,\n\u001b[32m    511\u001b[39m ) -> Any:\n\u001b[32m    512\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\tools\\base.py:774\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    773\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    775\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    776\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\tools\\base.py:736\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    734\u001b[39m child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     tool_args, tool_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_args_and_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m signature(\u001b[38;5;28mself\u001b[39m._run).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    740\u001b[39m         tool_kwargs = tool_kwargs | {\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m: run_manager}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\tools\\base.py:651\u001b[39m, in \u001b[36mBaseTool._to_args_and_kwargs\u001b[39m\u001b[34m(self, tool_input, tool_call_id)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    644\u001b[39m     \u001b[38;5;28mself\u001b[39m.args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    645\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.args_schema, \u001b[38;5;28mtype\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m ):\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# StructuredTool with no args\u001b[39;00m\n\u001b[32m    650\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (), {}\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m tool_input = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[38;5;66;03m# For backwards compatibility, if run_input is a string,\u001b[39;00m\n\u001b[32m    653\u001b[39m \u001b[38;5;66;03m# pass as a positional argument.\u001b[39;00m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_input, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\tools\\base.py:570\u001b[39m, in \u001b[36mBaseTool._parse_input\u001b[39m\u001b[34m(self, tool_input, tool_call_id)\u001b[39m\n\u001b[32m    568\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    569\u001b[39m             tool_input[k] = tool_call_id\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     result = \u001b[43minput_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m     result_dict = result.model_dump()\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(input_args, BaseModelV1):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\pydantic\\main.py:703\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context, by_alias, by_name)\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    698\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    699\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    700\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    701\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for get_customer_orders\ncustomer_id\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
      "During task with name 'fetch_user_info' and id 'a750fceb-d254-55f4-5a73-04dd4fa90c98'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import uuid\n",
    "\n",
    "# Update with the backup file so we can restart from the original place in each section\n",
    "thread_id = str(uuid.uuid4())\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # The customer_id is used in our order service tools to\n",
    "        # fetch the user's order information\n",
    "        # \"customer_id\": \"38178\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "tutorial_questions = [\n",
    "    \"Hi there, What are the details of my last order?\",\n",
    "    # \"Can you also fetch 5 most recent high-priority orders\",\n",
    "    # \"What are the top 5 highly-rated guitar products?\",\n",
    "    # \"Whats a good product for thin guitar strings?\",\n",
    "    # \"Is the BOYA BYM1 Microphone good for a cello?\",\n",
    "    # \"What are the details of my most recent order?\",\n",
    "    # \"What is the status of my car body covers?\",\n",
    "    # \"What is the status of my cell-phone order?\"\n",
    "]\n",
    "\n",
    "\n",
    "_printed = set()\n",
    "# We can reuse the tutorial questions from part 1 to see how it does.\n",
    "for question in tutorial_questions:\n",
    "    events = ecommerce_assistant_graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)\n",
    "    snapshot = ecommerce_assistant_graph.get_state(config)\n",
    "    while snapshot.next:\n",
    "        # We have an interrupt! The agent is trying to use a tool, and the user can approve or deny it\n",
    "        # Note: This code is all outside of your graph. Typically, you would stream the output to a UI.\n",
    "        # Then, you would have the frontend trigger a new run via an API call when the user has provided input.\n",
    "        try:\n",
    "            user_input = input(\n",
    "                \"Do you approve of the above actions? Type 'y' to continue;\"\n",
    "                \" otherwise, explain your requested changed.\\n\\n\"\n",
    "            )\n",
    "        except:\n",
    "            user_input = \"y\"\n",
    "        if user_input.strip() == \"y\":\n",
    "            # Just continue\n",
    "            result = ecommerce_assistant_graph.invoke(\n",
    "                None,\n",
    "                config,\n",
    "            )\n",
    "        else:\n",
    "            # Satisfy the tool invocation by\n",
    "            # providing instructions on the requested changes / change of mind\n",
    "            result = ecommerce_assistant_graph.invoke(\n",
    "                {\n",
    "                    \"messages\": [\n",
    "                        ToolMessage(\n",
    "                            tool_call_id=event[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "                            content=f\"API call denied by user. Reasoning: '{user_input}'. Continue assisting, accounting for the user's input.\",\n",
    "                        )\n",
    "                    ]\n",
    "                },\n",
    "                config,\n",
    "            )\n",
    "        snapshot = ecommerce_assistant_graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c94b7d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_11268\\656278458.py:35: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7869/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7869/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# from LG_Tutorial import ecommerce_assistant_graph  # Replace with actual module name if different\n",
    "\n",
    "def simple_chatbot_response(user_input: str):\n",
    "    thread_id = str(uuid.uuid4())\n",
    "    config = {\n",
    "    \"configurable\": {\n",
    "        # The customer_id is used in our order service tools to\n",
    "        # fetch the user's order information\n",
    "        # \"customer_id\": \"38178\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "      }\n",
    "    }\n",
    "    state_input = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "\n",
    "    try:\n",
    "        result = ecommerce_assistant_graph.invoke(state_input, config)\n",
    "        messages = result[\"messages\"]\n",
    "\n",
    "        # Extract only final assistant response\n",
    "        ai_messages = [m.content for m in messages if isinstance(m, AIMessage)]\n",
    "        return ai_messages[-1] if ai_messages else \"Sorry, I couldn't find a final answer.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\" Error: {str(e)}\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"##  E-commerce Chatbot (LangGraph + Cohere)\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input = gr.Textbox(placeholder=\"Ask something like 'Whats my last order?'\")\n",
    "    send_btn = gr.Button(\"Send\")\n",
    "\n",
    "    def process_input(input_text, chat_history):\n",
    "        response = simple_chatbot_response(input_text)\n",
    "        chat_history.append((input_text, response))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    user_input.submit(process_input, [user_input, chatbot], [user_input, chatbot])\n",
    "    send_btn.click(process_input, [user_input, chatbot], [user_input, chatbot])\n",
    "\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c1c98c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7869\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d2574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ca029c3",
   "metadata": {},
   "source": [
    "# Information gathering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "edc5ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (2.11.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from pydantic) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from pydantic) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce4a8e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bab827e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.cohere.com/v1/models?endpoint=chat&default_only=true \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\"\"\"\n",
    "\n",
    "\n",
    "def get_messages_info(messages):\n",
    "    return [SystemMessage(content=template)] + messages\n",
    "\n",
    "\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "\n",
    "    objective: str\n",
    "    variables: List[str]\n",
    "    constraints: List[str]\n",
    "    requirements: List[str]\n",
    "\n",
    "\n",
    "llm = ChatCohere(temperature=0)\n",
    "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
    "\n",
    "\n",
    "def info_chain(state):\n",
    "    messages = get_messages_info(state[\"messages\"])\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "# New system prompt\n",
    "prompt_system = \"\"\"Based on the following requirements, write a good prompt template:\n",
    "\n",
    "{reqs}\"\"\"\n",
    "\n",
    "\n",
    "# Function to get the messages for the prompt\n",
    "# Will only get messages AFTER the tool call\n",
    "def get_prompt_messages(messages: list):\n",
    "    tool_call = None\n",
    "    other_msgs = []\n",
    "    for m in messages:\n",
    "        if isinstance(m, AIMessage) and m.tool_calls:\n",
    "            tool_call = m.tool_calls[0][\"args\"]\n",
    "        elif isinstance(m, ToolMessage):\n",
    "            continue\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs\n",
    "\n",
    "\n",
    "def prompt_gen_chain(state):\n",
    "    messages = get_prompt_messages(state[\"messages\"])\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "520a8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "def get_state(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
    "        return \"add_tool_message\"\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        return END\n",
    "    return \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "80947383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"info\", info_chain)\n",
    "workflow.add_node(\"prompt\", prompt_gen_chain)\n",
    "\n",
    "\n",
    "@workflow.add_node\n",
    "def add_tool_message(state: State):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=\"Prompt generated!\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\"info\", get_state, [\"add_tool_message\", \"info\", END])\n",
    "workflow.add_edge(\"add_tool_message\", \"prompt\")\n",
    "workflow.add_edge(\"prompt\", END)\n",
    "workflow.add_edge(START, \"info\")\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a4420075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c3601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_11268\\1294710854.py:35: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7870/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7870/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# from LG_Tutorial import ecommerce_assistant_graph  # Replace with actual module name if different\n",
    "\n",
    "def simple_chatbot_response(user_input: str):\n",
    "    thread_id = str(uuid.uuid4())\n",
    "    config = {\n",
    "    \"configurable\": {\n",
    "        # The customer_id is used in our order service tools to\n",
    "        # fetch the user's order information\n",
    "        # \"customer_id\": \"38178\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "      }\n",
    "    }\n",
    "    state_input = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "\n",
    "    try:\n",
    "        result = graph.invoke(state_input, config)\n",
    "        messages = result[\"messages\"]\n",
    "\n",
    "        # Extract only final assistant response\n",
    "        ai_messages = [m.content for m in messages if isinstance(m, AIMessage)]\n",
    "        return ai_messages[-1] if ai_messages else \"Sorry, I couldn't find a final answer.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\" Error: {str(e)}\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"##  E-commerce Chatbot (LangGraph + Cohere)\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input = gr.Textbox(placeholder=\"Ask something \")\n",
    "    send_btn = gr.Button(\"Send\")\n",
    "\n",
    "    def process_input(input_text, chat_history):\n",
    "        response = simple_chatbot_response(input_text)\n",
    "        chat_history.append((input_text, response))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    user_input.submit(process_input, [user_input, chatbot], [user_input, chatbot])\n",
    "    send_btn.click(process_input, [user_input, chatbot], [user_input, chatbot])\n",
    "\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7e6461cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7870\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "77afb165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit): Hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi! I can help you create a prompt template. To get started, could you tell me:\n",
      "\n",
      "1. What is the objective of the prompt?\n",
      "2. What variables will be passed into the prompt template?\n",
      "3. Are there any constraints for what the output should NOT do?\n",
      "4. Are there any requirements that the output MUST adhere to?\n",
      "\n",
      "Once I have this information, I can assist you further!\n",
      "User (q/Q to quit): rag prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Thank you for the information! To clarify, it seems like you want to create a prompt template for a Retrieval-Augmented Generation (RAG) system. Could you please provide more details on the following:\n",
      "\n",
      "1. **Objective**: What specific task or goal do you want the RAG system to achieve (e.g., answering questions, summarizing documents, etc.)?\n",
      "2. **Variables**: What inputs will be passed into the prompt template (e.g., user query, retrieved documents, etc.)?\n",
      "3. **Constraints**: Are there any restrictions on the output (e.g., avoid certain topics, limit response length, etc.)?\n",
      "4. **Requirements**: Are there any specific formatting or content requirements for the output (e.g., include citations, use a particular tone, etc.)?\n",
      "\n",
      "Once I have this information, I can create a prompt template tailored to your needs.\n",
      "User (q/Q to quit): 1 rag, 2 none, 3 no, 4 no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I will use the 'PromptInstructions' tool to create a prompt template for the user. I will input the following information:\n",
      "\n",
      "- Objective: rag\n",
      "- Variables: none\n",
      "- Constraints: no\n",
      "- Requirements: no\n",
      "Tool Calls:\n",
      "  PromptInstructions (PromptInstructions_f2dn2fw137gd)\n",
      " Call ID: PromptInstructions_f2dn2fw137gd\n",
      "  Args:\n",
      "    objective: rag\n",
      "    variables: []\n",
      "    constraints: ['no']\n",
      "    requirements: ['no']\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Prompt generated!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The last message is not an ToolMessage or HumanMessage",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     16\u001b[39m output = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlast_message\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlast_message\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretty_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2461\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2455\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2456\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2457\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2458\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2459\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2460\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2468\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\runner.py:153\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    151\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mprompt_gen_chain\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprompt_gen_chain\u001b[39m(state):\n\u001b[32m     25\u001b[39m     messages = get_prompt_messages(state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:370\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    360\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    365\u001b[39m     **kwargs: Any,\n\u001b[32m    366\u001b[39m ) -> BaseMessage:\n\u001b[32m    367\u001b[39m     config = ensure_config(config)\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    369\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    380\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:947\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    940\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     **kwargs: Any,\n\u001b[32m    945\u001b[39m ) -> LLMResult:\n\u001b[32m    946\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:766\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    765\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    772\u001b[39m         )\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    774\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1012\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1016\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_cohere\\chat_models.py:1136\u001b[39m, in \u001b[36mChatCohere._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1131\u001b[39m     stream_iter = \u001b[38;5;28mself\u001b[39m._stream(\n\u001b[32m   1132\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1133\u001b[39m     )\n\u001b[32m   1134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m request = \u001b[43mget_cohere_chat_request_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m response = \u001b[38;5;28mself\u001b[39m.client.v2.chat(**request)\n\u001b[32m   1141\u001b[39m generation_info = \u001b[38;5;28mself\u001b[39m._get_generation_info_v2(\n\u001b[32m   1142\u001b[39m     response, request.get(\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1143\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_cohere\\chat_models.py:535\u001b[39m, in \u001b[36mget_cohere_chat_request_v2\u001b[39m\u001b[34m(messages, documents, stop_sequences, **kwargs)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;66;03m# check if the last message is a tool message or human message\u001b[39;00m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m    533\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(messages[-\u001b[32m1\u001b[39m], ToolMessage) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages[-\u001b[32m1\u001b[39m], HumanMessage)\n\u001b[32m    534\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe last message is not an ToolMessage or HumanMessage\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpreamble\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    538\u001b[39m     messages = [SystemMessage(content=\u001b[38;5;28mstr\u001b[39m(kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpreamble\u001b[39m\u001b[33m\"\u001b[39m)))] + messages\n",
      "\u001b[31mValueError\u001b[39m: The last message is not an ToolMessage or HumanMessage",
      "During task with name 'prompt' and id 'a21d86b9-f2ee-f109-8109-58d89245dc38'"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
    "cached_response_index = 0\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "while True:\n",
    "    try:\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        user = cached_human_responses[cached_response_index]\n",
    "        cached_response_index += 1\n",
    "    print(f\"User (q/Q to quit): {user}\")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    output = None\n",
    "    for output in graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()\n",
    "\n",
    "    if output and \"prompt\" in output:\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4db15af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_11268\\3753379182.py:33: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EventListener._setup.<locals>.event_trigger() got an unexpected keyword argument 'stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m     clear_btn = gr.Button(\u001b[33m\"\u001b[39m\u001b[33m Start New Session\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# Only textbox supports stream=True\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[43muser_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchatbot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchatbot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     clear_btn.click(reset, outputs=chatbot)\n\u001b[32m     41\u001b[39m demo.launch()\n",
      "\u001b[31mTypeError\u001b[39m: EventListener._setup.<locals>.event_trigger() got an unexpected keyword argument 'stream'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import uuid\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "# from your_module import graph  # <-- Update this to your module name\n",
    "\n",
    "# Setup config for consistent session\n",
    "session_config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "def chat_stream(user_input, chat_history):\n",
    "    \"\"\"LangGraph streaming function for Gradio.\"\"\"\n",
    "    stream = graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config=session_config,\n",
    "        stream_mode=\"updates\"\n",
    "    )\n",
    "\n",
    "    full_text = \"\"\n",
    "    for update in stream:\n",
    "        messages = next(iter(update.values()))[\"messages\"]\n",
    "        last = messages[-1]\n",
    "        if isinstance(last, AIMessage):\n",
    "            full_text = last.content\n",
    "            yield chat_history + [(user_input, full_text)]\n",
    "\n",
    "def reset():\n",
    "    global session_config\n",
    "    session_config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "    return []\n",
    "\n",
    "# Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"##  LangGraph Chatbot (Streaming)\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input = gr.Textbox(label=\"Type your message and press Enter\")\n",
    "    clear_btn = gr.Button(\" Start New Session\")\n",
    "\n",
    "    # Only textbox supports stream=True\n",
    "    user_input.submit(chat_stream, [user_input, chatbot], chatbot, stream=True)\n",
    "    clear_btn.click(reset, outputs=chatbot)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "33300f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (5.29.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.2.1)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.10.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (1.10.0)\n",
      "Requirement already satisfied: groovy~=0.1 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.30.2)\n",
      "Requirement already satisfied: jinja2<4.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (2.2.5)\n",
      "Requirement already satisfied: orjson~=3.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (11.2.1)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (2.11.4)\n",
      "Requirement already satisfied: pydub in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.11.9)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.15.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (4.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: fsspec in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from gradio-client==1.10.0->gradio) (10.4)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: colorama in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: certifi in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\genai.labs assignment\\assignment\\ecommerce_assistant_challenge 2025\\genailabs\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (1.26.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "17efa244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_11268\\2526089336.py:32: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7870/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7870/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\blocks.py\", line 2146, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\blocks.py\", line 1676, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\utils.py\", line 729, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\utils.py\", line 723, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        run_sync_iterator_async, self.iterator, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\utils.py\", line 706, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\utils.py\", line 867, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "  File \"C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_11268\\2526089336.py\", line 17, in chat_stream\n",
      "    for update in stream:\n",
      "                  ^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2461, in stream\n",
      "    for _ in runner.tick(\n",
      "             ~~~~~~~~~~~^\n",
      "        loop.tasks.values(),\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        get_waiter=get_waiter,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\runner.py\", line 153, in tick\n",
      "    run_with_retry(\n",
      "    ~~~~~~~~~~~~~~^\n",
      "        t,\n",
      "        ^^\n",
      "    ...<11 lines>...\n",
      "        },\n",
      "        ^^\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 40, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 623, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 377, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_11268\\2177956022.py\", line 26, in prompt_gen_chain\n",
      "    response = llm.invoke(messages)\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 370, in invoke\n",
      "    self.generate_prompt(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        [self._convert_input(input)],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    ).generations[0][0],\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 947, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 766, in generate\n",
      "    self._generate_with_cache(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        m,\n",
      "        ^^\n",
      "    ...<2 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1012, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
      "    )\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_cohere\\chat_models.py\", line 1136, in _generate\n",
      "    request = get_cohere_chat_request_v2(\n",
      "        messages, stop_sequences=stop, **self._default_params, **kwargs\n",
      "    )\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_cohere\\chat_models.py\", line 535, in get_cohere_chat_request_v2\n",
      "    raise ValueError(\"The last message is not an ToolMessage or HumanMessage\")\n",
      "ValueError: The last message is not an ToolMessage or HumanMessage\n",
      "During task with name 'prompt' and id 'a889ab41-e7f3-e665-87f2-dd89e2368402'\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\blocks.py\", line 2146, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\blocks.py\", line 1676, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\utils.py\", line 729, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\utils.py\", line 723, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        run_sync_iterator_async, self.iterator, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\utils.py\", line 706, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\gradio\\utils.py\", line 867, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "  File \"C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_11268\\2526089336.py\", line 17, in chat_stream\n",
      "    for update in stream:\n",
      "                  ^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 2461, in stream\n",
      "    for _ in runner.tick(\n",
      "             ~~~~~~~~~~~^\n",
      "        loop.tasks.values(),\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "        get_waiter=get_waiter,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ):\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\runner.py\", line 153, in tick\n",
      "    run_with_retry(\n",
      "    ~~~~~~~~~~~~~~^\n",
      "        t,\n",
      "        ^^\n",
      "    ...<11 lines>...\n",
      "        },\n",
      "        ^^\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 40, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 623, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 377, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_11268\\2177956022.py\", line 26, in prompt_gen_chain\n",
      "    response = llm.invoke(messages)\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 370, in invoke\n",
      "    self.generate_prompt(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        [self._convert_input(input)],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<6 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    ).generations[0][0],\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 947, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 766, in generate\n",
      "    self._generate_with_cache(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        m,\n",
      "        ^^\n",
      "    ...<2 lines>...\n",
      "        **kwargs,\n",
      "        ^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1012, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "        messages, stop=stop, run_manager=run_manager, **kwargs\n",
      "    )\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_cohere\\chat_models.py\", line 1136, in _generate\n",
      "    request = get_cohere_chat_request_v2(\n",
      "        messages, stop_sequences=stop, **self._default_params, **kwargs\n",
      "    )\n",
      "  File \"d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\langchain_cohere\\chat_models.py\", line 535, in get_cohere_chat_request_v2\n",
      "    raise ValueError(\"The last message is not an ToolMessage or HumanMessage\")\n",
      "ValueError: The last message is not an ToolMessage or HumanMessage\n",
      "During task with name 'prompt' and id '3cb72c90-deda-18d8-bbce-48adf60227f7'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import uuid\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "# from your_graph_module import graph  # replace with your actual file name\n",
    "\n",
    "# Global session config (thread_id persists across turns)\n",
    "session_config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "def chat_stream(user_input, history):\n",
    "    stream = graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config=session_config,\n",
    "        stream_mode=\"updates\"\n",
    "    )\n",
    "    final_response = \"\"\n",
    "\n",
    "    for update in stream:\n",
    "        messages = next(iter(update.values()))[\"messages\"]\n",
    "        last = messages[-1]\n",
    "        if isinstance(last, AIMessage):\n",
    "            final_response = last.content\n",
    "            yield history + [(user_input, final_response)]\n",
    "\n",
    "def reset():\n",
    "    global session_config\n",
    "    session_config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "    return []\n",
    "\n",
    "# UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"##  LangGraph Streaming Chatbot\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    txt = gr.Textbox(placeholder=\"Ask me something...\", label=\"User Message\")\n",
    "    reset_btn = gr.Button(\" Start Over\")\n",
    "\n",
    "    txt.submit(chat_stream, [txt, chatbot], chatbot)\n",
    "    reset_btn.click(reset, outputs=chatbot)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c1c667d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7870\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b3e89",
   "metadata": {},
   "source": [
    "# Simple Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df27fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "llm = ChatCohere(\n",
    "    cohere_api_key='PEy1tsUjWmM66wjbm8SgFAHulAHZOHlLI0kiiIRM',\n",
    "    #os.getenv(\"COHERE_API_KEY\"),\n",
    "    model=\"command-r\"\n",
    ")\n",
    "\n",
    "tools=[search_products,\n",
    "      search_product_by_category,\n",
    "      get_top_rated_products, \n",
    "      get_specific_instrument_details, \n",
    "      get_customer_orders,\n",
    "      get_customer_recent_order, \n",
    "      get_customer_product_orders,\n",
    "      get_high_priority_orders,\n",
    "      get_sales_by_category,\n",
    "      get_high_profit_products,\n",
    "      get_shipping_cost_summary,\n",
    "      get_profit_by_gender,\n",
    "     ]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5e50f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=\"\"\"\n",
    "            \"You are a helpful customer support assistant for an E-Commerce Website. \"\n",
    "            \" Use the provided tools to search for products, orders, and other information to assist the user's queries. \"\n",
    "            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n",
    "            \" If a search comes up empty, expand your search before giving up.\"\n",
    "            \"\\nCurrent time: {time}.\n",
    "            \"\"\")\n",
    "\n",
    "# Node\n",
    "def assistant(state: MessagesState):\n",
    "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "79dc77aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "react_graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ea40c3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_11268\\627553085.py:35: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7880/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7880/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7880/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_11268\\1142836813.py:109: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return search_products(instrument_type, top_k=3)\n",
      "ERROR:__main__:Error fetching specific instrument details: BaseTool.__call__() got an unexpected keyword argument 'top_k'\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.com/v2/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# from LG_Tutorial import ecommerce_assistant_graph  # Replace with actual module name if different\n",
    "\n",
    "def simple_chatbot_response(user_input: str):\n",
    "    thread_id = str(5)\n",
    "    config = {\n",
    "    \"configurable\": {\n",
    "        # The customer_id is used in our order service tools to\n",
    "        # fetch the user's order information\n",
    "        # \"customer_id\": \"38178\",\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "      }\n",
    "    }\n",
    "    state_input = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "\n",
    "    try:\n",
    "        result = react_graph.invoke(state_input,config)\n",
    "        messages = result[\"messages\"]\n",
    "\n",
    "        # Extract only final assistant response\n",
    "        ai_messages = [m.content for m in messages if isinstance(m, AIMessage)]\n",
    "        return ai_messages[-1] if ai_messages else \"Sorry, I couldn't find a final answer.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\" Error: {str(e)}\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"##  E-commerce Chatbot (LangGraph + Cohere)\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    user_input = gr.Textbox(placeholder=\"Ask something like 'Whats my last order?'\")\n",
    "    send_btn = gr.Button(\"Send\")\n",
    "\n",
    "    def process_input(input_text, chat_history):\n",
    "        response = simple_chatbot_response(input_text)\n",
    "        chat_history.append((input_text, response))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    user_input.submit(process_input, [user_input, chatbot], [user_input, chatbot])\n",
    "    send_btn.click(process_input, [user_input, chatbot], [user_input, chatbot])\n",
    "\n",
    "demo.launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec8d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genailabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
