{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9e47eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ae8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_service/vector_store.py\n",
    "# Import necessary libraries here...\n",
    "\n",
    "class ProductVectorStore:\n",
    "    \"\"\"\n",
    "    Vector store for product information retrieval.\n",
    "    Uses embeddings to perform semantic search on product data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the vector store with a model for embeddings.\"\"\"\n",
    "        # Initialize model, index, and other necessary components\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        self.embeddings = None\n",
    "        self.product_ids = None\n",
    "    \n",
    "    def load_data(self, file_path):\n",
    "        \"\"\"Load product data from CSV file.\"\"\"\n",
    "        \n",
    "        try: \n",
    "            df = pd.read_csv(file_path)\n",
    "            self.product_df = df\n",
    "\n",
    "            print(f\"Loaded {len(df)} products from {file_path}\")\n",
    "\n",
    "            return self.preprocess_data(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def preprocess_data(self, df=None):\n",
    "        \"\"\"Preprocess product data for embedding generation.\"\"\"\n",
    "        if df is None: \n",
    "            df = self.product_df\n",
    "\n",
    "        # Create a copy to avoid modifying the original\n",
    "        processed_df = df.copy()\n",
    "        \n",
    "        # Generate product IDs if they don't exist\n",
    "        if \"product_id\" not in processed_df.columns:\n",
    "            processed_df[\"product_id\"] = processed_df.index\n",
    "\n",
    "        # Check for duplicate titles and remove duplicates\n",
    "        duplicate_count = processed_df.duplicated(subset = ['title'], keep = 'first').sum()\n",
    "        if duplicate_count > 0:\n",
    "            print(f\"Found {duplicate_count} duplicate products - keeping only first occurrences\")\n",
    "            processed_df = processed_df.drop_duplicates(subset = ['title'], keep = 'first')\n",
    "        \n",
    "        # Fill missing values in text fields\n",
    "        text_columns = ['title', 'description', 'features', 'categories', 'details']\n",
    "        for col in text_columns:\n",
    "            if col in processed_df.columns:\n",
    "                processed_df[col] = processed_df[col].fillna('')\n",
    "        \n",
    "        # Normalize text - lowercase, remove extra whitespace\n",
    "        for col in text_columns:\n",
    "            if col in processed_df.columns:\n",
    "                processed_df[col] = processed_df[col].str.lower().str.strip()\n",
    "        \n",
    "        # Store processed dataframe\n",
    "        self.product_df = processed_df\n",
    "        self.product_ids = processed_df['product_id'].tolist()\n",
    "\n",
    "        return processed_df\n",
    "\n",
    "    \n",
    "    def create_embeddings(self):\n",
    "        \"\"\"Generate embeddings for product data.\"\"\"\n",
    "\n",
    "        # Ensure we have data to embed\n",
    "        if self.product_df is None or len(self.product_df) == 0:\n",
    "            print(\"No data available for embedding generation\")\n",
    "            return None\n",
    "        \n",
    "        # Get all text columns \n",
    "        text_columns = ['main_category', 'title', 'features', 'description', 'categories', 'details']\n",
    "        \n",
    "        # Create text representations by combining all fields\n",
    "        print(\"Creating text representations for embedding...\")\n",
    "        product_texts = []\n",
    "        \n",
    "        for _, row in self.product_df.iterrows():\n",
    "            # Combine all text fields into one string\n",
    "            text_parts = []\n",
    "            for col in text_columns:\n",
    "                if col in row and not pd.isna(row[col]) and row[col]:\n",
    "                    text_parts.append(f\"{col}: {row[col]}\")\n",
    "            \n",
    "            # Add numeric information\n",
    "            if 'average_rating' in row and not pd.isna(row['average_rating']):\n",
    "                text_parts.append(f\"rating: {row['average_rating']}\")\n",
    "            \n",
    "            if 'price' in row and not pd.isna(row['price']):\n",
    "                text_parts.append(f\"price: {row['price']}\")\n",
    "    \n",
    "            if ('imputed_columns' in row and row['imputed_columns'] != None and row['imputed_columns']!= []):\n",
    "                text_parts.append(f\"imputed_columns: {row['imputed_columns']}\")\n",
    "            \n",
    "            # Combine all parts with spaces\n",
    "            product_text = \" \".join(text_parts)\n",
    "            product_texts.append(product_text)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        print(f\"Generating embeddings for {len(product_texts)} products...\")\n",
    "        try:\n",
    "            embeddings = self.model.encode(product_texts)\n",
    "            self.embeddings = embeddings\n",
    "            \n",
    "            print(f\"Successfully created embeddings of shape {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build search index from embeddings.\"\"\"\n",
    "        # Check if embeddings exist\n",
    "        if self.embeddings is None:\n",
    "            print(\"No embeddings available. Call create_embeddings() first.\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Get embedding dimensions\n",
    "            vector_dimension = self.embeddings.shape[1]\n",
    "            \n",
    "            # Create a new index with the correct dimensions\n",
    "            self.index = faiss.IndexFlatL2(vector_dimension)\n",
    "            \n",
    "            # Add the embeddings to the index\n",
    "            self.index.add(self.embeddings)\n",
    "            \n",
    "            print(f\"Successfully built index with {self.index.ntotal} vectors\")\n",
    "            return self.index\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error building index: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_index(self, index_path, data_path):\n",
    "        \"\"\"\n",
    "        Save the FAISS index and product data to disk.\n",
    "\n",
    "        Args:\n",
    "            index_path: Path to save the FAISS index\n",
    "            data_path: Path to save the product data\n",
    "\n",
    "        Returns:\n",
    "            bool: True if saving was successful, False otherwise\n",
    "        \"\"\"\n",
    "        # Check if index exists\n",
    "        if self.index is None:\n",
    "            print(\"No index available to save. Call build_index() first.\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            # Create directories if they don't exist\n",
    "            os.makedirs(os.path.dirname(index_path), exist_ok=True)\n",
    "            os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "\n",
    "            # Save the FAISS index\n",
    "            faiss.write_index(self.index, index_path)\n",
    "\n",
    "            # Save the embeddings\n",
    "            if self.embeddings is not None:\n",
    "                embeddings_path = os.path.splitext(index_path)[0] + \"_embeddings.npy\"\n",
    "                np.save(embeddings_path, self.embeddings)\n",
    "                print(f\"Embeddings saved to {embeddings_path}\")\n",
    "\n",
    "            # Save the product dataframe\n",
    "            self.product_df.to_pickle(data_path)\n",
    "\n",
    "            print(f\"Index saved to {index_path}\")\n",
    "            print(f\"Product data saved to {data_path}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_index(self, index_path, data_path):\n",
    "        \"\"\"\n",
    "        Load a FAISS index and product data from disk.\n",
    "\n",
    "        Args:\n",
    "            index_path: Path to the saved FAISS index\n",
    "            data_path: Path to the saved product data\n",
    "\n",
    "        Returns:\n",
    "            bool: True if loading was successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the FAISS index\n",
    "            self.index = faiss.read_index(index_path)\n",
    "\n",
    "            # Load the embeddings if available\n",
    "            embeddings_path = os.path.splitext(index_path)[0] + \"_embeddings.npy\"\n",
    "            if os.path.exists(embeddings_path):\n",
    "                self.embeddings = np.load(embeddings_path)\n",
    "                print(f\"Loaded embeddings of shape {self.embeddings.shape}\")\n",
    "\n",
    "            # Load the product dataframe\n",
    "            self.product_df = pd.read_pickle(data_path)\n",
    "\n",
    "            # Recreate product_ids\n",
    "            if 'product_id' in self.product_df.columns:\n",
    "                self.product_ids = self.product_df['product_id'].tolist()\n",
    "            else:\n",
    "                self.product_ids = list(range(len(self.product_df)))\n",
    "\n",
    "            print(f\"Loaded index with {self.index.ntotal} vectors\")\n",
    "            print(f\"Loaded {len(self.product_df)} products\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def search(self, query, top_k=5):\n",
    "        \"\"\"\n",
    "        Search for products similar to the query.\n",
    "\n",
    "        Args:\n",
    "            query: Text query to search for\n",
    "            top_k: Number of results to return\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing product information\n",
    "        \"\"\"\n",
    "        # Check if index exists\n",
    "        if self.index is None:\n",
    "            print(\"No index available. Call build_index() first.\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # Convert query to embedding\n",
    "            query_embedding = self.model.encode([query])\n",
    "\n",
    "            # Normalize the query embedding for cosine similarity\n",
    "            faiss.normalize_L2(query_embedding)\n",
    "\n",
    "            # Search the index\n",
    "            distances, indices = self.index.search(query_embedding, top_k)\n",
    "\n",
    "            # Fetch the actual product data\n",
    "            results = []\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx < len(self.product_df):\n",
    "                    # Get the product data\n",
    "                    product = self.product_df.iloc[idx].to_dict()\n",
    "\n",
    "                    # Add distance score (lower is better for L2 distance)\n",
    "                    product['search_score'] = float(distances[0][i])\n",
    "\n",
    "                    # Add to results\n",
    "                    results.append(product)\n",
    "\n",
    "            print(f\"Found {len(results)} products matching the query: '{query}'\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching index: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_by_category(self, query, category, top_k=5):\n",
    "        \"\"\"\n",
    "        Search for products within a specific category.\n",
    "        \n",
    "        Args:\n",
    "            query: Text query to search for\n",
    "            category: Category to filter by\n",
    "            top_k: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries containing product information\n",
    "        \"\"\"\n",
    "        # First get more results than we need\n",
    "        results = self.search(query, top_k=top_k*3)\n",
    "        \n",
    "        # Filter by category\n",
    "        filtered_results = []\n",
    "        for product in results:\n",
    "            if 'main_category' in product and product['main_category'] == category:\n",
    "                filtered_results.append(product)\n",
    "\n",
    "        \n",
    "        # Return the top k results\n",
    "        return filtered_results[:top_k]\n",
    "\n",
    "    def get_product_by_id(self, product_id):\n",
    "        \"\"\"\n",
    "        Retrieve product details by ID.\n",
    "\n",
    "        Args:\n",
    "            product_id: ID of the product to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing product information, or None if not found\n",
    "        \"\"\"\n",
    "        # Check if we have product data\n",
    "        if self.product_df is None: \n",
    "            print(\"No product data available.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Find the product by ID\n",
    "            if 'product_id' in self.product_df.columns:\n",
    "                product = self.product_df[self.product_df['product_id'] == product_id]\n",
    "\n",
    "                # If product found, return it as a dictionary\n",
    "                if not product.empty:\n",
    "                    return product.iloc[0].to_dict()\n",
    "\n",
    "            # If we reach here, the product wasn't found\n",
    "            print(f\"Product with ID {product_id} not found.\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving product: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "695ebf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_24032\\4250044334.py:25: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  data_path = \"D:/Genai.labs assignment/assignment\\Ecommerce_Assistant_Challenge 2025/new_data/Product_Information_Dataset.csv\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Running all ProductVectorStore tests =====\n",
      "\n",
      "===== Testing Initialization =====\n",
      "Model name: all-MiniLM-L6-v2\n",
      "Model loaded: True\n",
      "\n",
      "===== Testing load_data() =====\n",
      "Loaded 5000 products from D:/Genai.labs assignment/assignment\\Ecommerce_Assistant_Challenge 2025/new_data/Product_Information_Dataset.csv\n",
      "Found 169 duplicate products - keeping only first occurrences\n",
      "Loaded 4831 products in 0.60 seconds\n",
      "DataFrame columns: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'store', 'categories', 'details', 'parent_asin', 'imputed_columns', 'product_id']\n",
      "\n",
      "===== Testing preprocess_data() =====\n",
      "Found 1 duplicate products - keeping only first occurrences\n",
      "Removed 1 duplicates\n",
      "Preprocessed 4830 products in 0.06 seconds\n",
      "\n",
      "===== Testing create_embeddings() =====\n",
      "Creating text representations for embedding...\n",
      "Generating embeddings for 4830 products...\n",
      "Successfully created embeddings of shape (4830, 384)\n",
      "Created embeddings of shape (4830, 384) in 246.58 seconds\n",
      "\n",
      "===== Testing build_index() =====\n",
      "Successfully built index with 4830 vectors\n",
      "Built index with 4830 vectors in 0.00 seconds\n",
      "\n",
      "===== Testing save_index() and load_index() =====\n",
      "Embeddings saved to services/product_service/test_index/test_product_index_embeddings.npy\n",
      "Index saved to services/product_service/test_index/test_product_index.bin\n",
      "Product data saved to services/product_service/test_index/test_product_data.pkl\n",
      "Saved index and data in 0.09 seconds\n",
      "Loaded embeddings of shape (4830, 384)\n",
      "Loaded index with 4830 vectors\n",
      "Loaded 4830 products\n",
      "Loaded index and data in 0.06 seconds\n",
      "\n",
      "===== Testing search() =====\n",
      "Found 3 products matching the query: 'acoustic guitar'\n",
      "\n",
      "Query: 'acoustic guitar'\n",
      "Found 3 results in 0.0303 seconds:\n",
      "1. martin guitar authentic acoustic lifespan 2.0 ma150t, 80/20 bronze, treated medium-gauge acoustic strings (Score: 0.6055)\n",
      "2. first act discovery 30\" beginner acoustic guitar, sunburst (Score: 0.6304)\n",
      "3. best choice products beginner acoustic electric guitar starter set 38in w/all wood cutaway design, case, strap, picks, tuner - blue (Score: 0.6324)\n",
      "Found 3 products matching the query: 'high quality microphone'\n",
      "\n",
      "Query: 'high quality microphone'\n",
      "Found 3 results in 0.0153 seconds:\n",
      "1. gls audio instrument microphone es-57 & mic clip - professional series es57 dynamic cardioid mike unidirectional - for instruments, drums, percussion, vocals, and more (Score: 0.7436)\n",
      "2. professional wired lavalier lapel clip on microphone for iphone and android smartphone or camera omnidirectional tiny shirt mic for recording with clip-on perfect for vloggers and bloggers (Score: 0.7540)\n",
      "3. 1mii long range wireless microphone,wireless headset mic system, 165ft range, 2.4g wireless microphone 2 in 1, fitness microphone headset for speakers, voice amplifier, pa speakers (Score: 0.7617)\n",
      "Found 3 products matching the query: 'music equipment with good ratings'\n",
      "\n",
      "Query: 'music equipment with good ratings'\n",
      "Found 3 results in 0.0178 seconds:\n",
      "1. adj products db display mkii dj mixer (Score: 0.8140)\n",
      "2. ltgem case for pioneer dj ddj-400 / ddj-sb3 (ddj-sb3-n) / ddj-sb2 or portable 2-channel controller or ddj-rb performance dj controller-black (Score: 0.8336)\n",
      "3. karaoke machine, wireless rechargeable portable pa system/speaker with 2 metal wireless mics, b fm radio, led party light, remote control, built in handel, 800w peak (2 mics) (Score: 0.8369)\n",
      "\n",
      "===== Testing search_by_category() =====\n",
      "Found 9 products matching the query: 'high quality'\n",
      "Query: 'high quality' in category 'Musical Instruments'\n",
      "Found 3 results in 0.0238 seconds:\n",
      "1. behringer micromix mx400 ultra low-noise 4-channel line mixer, black (Score: 1.2871)\n",
      "2. rosefinch acoustic guitar 38 inch 3/4 size cutaway basswood guitar for beginner adults childs starter bundle kit w/bag,strap, picks,extra steel strings, tuner,capo,strings winder(white 38 inch) (Score: 1.3003)\n",
      "3. shure super 55 deluxe vocal microphone - vintage supercardioid dynamic unidyne mic, iconic look, classic sound - rugged die-cast casing, includes 5/8\" to 3/8\" thread adapter and zippered, padded pouch (Score: 1.3012)\n",
      "\n",
      "===== Testing get_product_by_id() =====\n",
      "Retrieved product with ID 0 in 0.0011 seconds:\n",
      "Title: ernie ball mondo slinky nickelwound electric guitar strings 10.5-52 gauge\n",
      "Price: $6.99\n",
      "Rating: 4.8\n",
      "\n",
      "===== All tests completed successfully! =====\n"
     ]
    }
   ],
   "source": [
    "# test_vector_store.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "# Import the ProductVectorStore class\n",
    "# from vector_store import ProductVectorStore\n",
    "\n",
    "def test_init():\n",
    "    \"\"\"Test initialization of the vector store.\"\"\"\n",
    "    print(\"\\n===== Testing Initialization =====\")\n",
    "    vector_store = ProductVectorStore()\n",
    "    print(f\"Model name: {vector_store.model_name}\")\n",
    "    print(f\"Model loaded: {vector_store.model is not None}\")\n",
    "    assert vector_store.model is not None, \"Model should be initialized\"\n",
    "    return vector_store\n",
    "\n",
    "def test_load_data(vector_store):\n",
    "    \"\"\"Test loading data from CSV.\"\"\"\n",
    "    print(\"\\n===== Testing load_data() =====\")\n",
    "    # Define the path to your CSV file\n",
    "    data_path = \"D:/Genai.labs assignment/assignment\\Ecommerce_Assistant_Challenge 2025/new_data/Product_Information_Dataset.csv\"\n",
    "    \n",
    "    # Ensure file exists\n",
    "    assert os.path.exists(data_path), f\"Data file not found at {data_path}\"\n",
    "    \n",
    "    # Load the data\n",
    "    start_time = time.time()\n",
    "    df = vector_store.load_data(data_path)\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    # Check if data loaded successfully\n",
    "    assert df is not None, \"DataFrame should not be None\"\n",
    "    assert len(df) > 0, \"DataFrame should not be empty\"\n",
    "    \n",
    "    print(f\"Loaded {len(df)} products in {load_time:.2f} seconds\")\n",
    "    print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if product_id was created\n",
    "    assert 'product_id' in df.columns, \"product_id column should exist\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "def test_preprocess_data(vector_store):\n",
    "    \"\"\"Test preprocessing of product data.\"\"\"\n",
    "    print(\"\\n===== Testing preprocess_data() =====\")\n",
    "    \n",
    "    # Get the raw dataframe size\n",
    "    raw_df_size = len(vector_store.product_df)\n",
    "    \n",
    "    # Process the data\n",
    "    start_time = time.time()\n",
    "    processed_df = vector_store.preprocess_data()\n",
    "    process_time = time.time() - start_time\n",
    "    \n",
    "    # Check if processing worked\n",
    "    assert processed_df is not None, \"Processed DataFrame should not be None\"\n",
    "    \n",
    "    # Check for duplicates removal\n",
    "    if raw_df_size > len(processed_df):\n",
    "        print(f\"Removed {raw_df_size - len(processed_df)} duplicates\")\n",
    "    \n",
    "    # Check text normalization (sample a few rows)\n",
    "    if len(processed_df) > 0:\n",
    "        sample_row = processed_df.iloc[0]\n",
    "        if 'title' in sample_row:\n",
    "            title = sample_row['title']\n",
    "            assert title.lower() == title, \"Title should be lowercase\"\n",
    "            assert title == title.strip(), \"Title should be stripped of whitespace\"\n",
    "    \n",
    "    print(f\"Preprocessed {len(processed_df)} products in {process_time:.2f} seconds\")\n",
    "    return processed_df\n",
    "\n",
    "def test_create_embeddings(vector_store):\n",
    "    \"\"\"Test creation of embeddings.\"\"\"\n",
    "    print(\"\\n===== Testing create_embeddings() =====\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    start_time = time.time()\n",
    "    embeddings = vector_store.create_embeddings()\n",
    "    embed_time = time.time() - start_time\n",
    "    \n",
    "    # Check if embeddings were created\n",
    "    assert embeddings is not None, \"Embeddings should not be None\"\n",
    "    assert vector_store.embeddings is not None, \"Embeddings should be stored in vector_store\"\n",
    "    \n",
    "    # Check shape of embeddings\n",
    "    num_products = len(vector_store.product_df)\n",
    "    embed_dim = vector_store.model.get_sentence_embedding_dimension()\n",
    "    assert embeddings.shape == (num_products, embed_dim), f\"Embeddings shape should be ({num_products}, {embed_dim})\"\n",
    "    \n",
    "    print(f\"Created embeddings of shape {embeddings.shape} in {embed_time:.2f} seconds\")\n",
    "    return embeddings\n",
    "\n",
    "def test_build_index(vector_store):\n",
    "    \"\"\"Test building the FAISS index.\"\"\"\n",
    "    print(\"\\n===== Testing build_index() =====\")\n",
    "    \n",
    "    # Build index\n",
    "    start_time = time.time()\n",
    "    index = vector_store.build_index()\n",
    "    build_time = time.time() - start_time\n",
    "    \n",
    "    # Check if index was built\n",
    "    assert index is not None, \"Index should not be None\"\n",
    "    assert hasattr(index, 'ntotal'), \"Index should be a FAISS index\"\n",
    "    assert index.ntotal == len(vector_store.embeddings), \"Index should contain all embeddings\"\n",
    "    \n",
    "    print(f\"Built index with {index.ntotal} vectors in {build_time:.2f} seconds\")\n",
    "    return index\n",
    "\n",
    "def test_save_and_load_index(vector_store):\n",
    "    \"\"\"Test saving and loading the index.\"\"\"\n",
    "    print(\"\\n===== Testing save_index() and load_index() =====\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(\"services/product_service/test_index\", exist_ok=True)\n",
    "    \n",
    "    # Define paths\n",
    "    index_path = \"services/product_service/test_index/test_product_index.bin\"\n",
    "    data_path = \"services/product_service/test_index/test_product_data.pkl\"\n",
    "    \n",
    "    # Save index\n",
    "    start_time = time.time()\n",
    "    save_result = vector_store.save_index(index_path, data_path)\n",
    "    save_time = time.time() - start_time\n",
    "    \n",
    "    # Check if save was successful\n",
    "    assert save_result, \"Save operation should return True\"\n",
    "    assert os.path.exists(index_path), f\"Index file should exist at {index_path}\"\n",
    "    assert os.path.exists(data_path), f\"Data file should exist at {data_path}\"\n",
    "    \n",
    "    print(f\"Saved index and data in {save_time:.2f} seconds\")\n",
    "    \n",
    "    # Create a new vector store\n",
    "    new_vector_store = ProductVectorStore()\n",
    "    \n",
    "    # Load index\n",
    "    start_time = time.time()\n",
    "    load_result = new_vector_store.load_index(index_path, data_path)\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    # Check if load was successful\n",
    "    assert load_result, \"Load operation should return True\"\n",
    "    assert new_vector_store.index is not None, \"Index should not be None after loading\"\n",
    "    assert new_vector_store.product_df is not None, \"Product DataFrame should not be None after loading\"\n",
    "    assert len(new_vector_store.product_df) == len(vector_store.product_df), \"Loaded DataFrame should have same size\"\n",
    "    \n",
    "    print(f\"Loaded index and data in {load_time:.2f} seconds\")\n",
    "    return new_vector_store\n",
    "\n",
    "def test_search(vector_store):\n",
    "    \"\"\"Test search functionality.\"\"\"\n",
    "    print(\"\\n===== Testing search() =====\")\n",
    "    \n",
    "    # Define test queries\n",
    "    test_queries = [\n",
    "        \"acoustic guitar\",\n",
    "        \"high quality microphone\",\n",
    "        \"music equipment with good ratings\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        # Search for products\n",
    "        start_time = time.time()\n",
    "        results = vector_store.search(query, top_k=3)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        # Check if search returned results\n",
    "        assert isinstance(results, list), \"Search results should be a list\"\n",
    "        assert len(results) <= 3, \"Search should return at most top_k results\"\n",
    "        \n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"Found {len(results)} results in {search_time:.4f} seconds:\")\n",
    "        \n",
    "        # Display results\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"{i+1}. {result.get('title', 'No title')} (Score: {result.get('search_score', 0):.4f})\")\n",
    "\n",
    "def test_search_by_category(vector_store):\n",
    "    \"\"\"Test search by category functionality.\"\"\"\n",
    "    print(\"\\n===== Testing search_by_category() =====\")\n",
    "    \n",
    "    # Get a sample category from the data\n",
    "    if 'main_category' in vector_store.product_df.columns:\n",
    "        sample_category = vector_store.product_df['main_category'].iloc[0]\n",
    "        \n",
    "        # Search by category\n",
    "        query = \"high quality\"\n",
    "        start_time = time.time()\n",
    "        results = vector_store.search_by_category(query, sample_category, top_k=3)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        # Check if search returned results\n",
    "        assert isinstance(results, list), \"Search results should be a list\"\n",
    "        \n",
    "        print(f\"Query: '{query}' in category '{sample_category}'\")\n",
    "        print(f\"Found {len(results)} results in {search_time:.4f} seconds:\")\n",
    "        \n",
    "        # Display results\n",
    "        for i, result in enumerate(results):\n",
    "            if len(results) > 0:\n",
    "                print(f\"{i+1}. {result.get('title', 'No title')} (Score: {result.get('search_score', 0):.4f})\")\n",
    "    else:\n",
    "        print(\"Skipping category search test - no main_category column in data\")\n",
    "\n",
    "def test_get_product_by_id(vector_store):\n",
    "    \"\"\"Test retrieving a product by ID.\"\"\"\n",
    "    print(\"\\n===== Testing get_product_by_id() =====\")\n",
    "    \n",
    "    # Get a sample product ID\n",
    "    if len(vector_store.product_ids) > 0:\n",
    "        sample_id = vector_store.product_ids[0]\n",
    "        \n",
    "        # Get product by ID\n",
    "        start_time = time.time()\n",
    "        product = vector_store.get_product_by_id(sample_id)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        # Check if product was retrieved\n",
    "        assert product is not None, f\"Product with ID {sample_id} should be found\"\n",
    "        assert isinstance(product, dict), \"Retrieved product should be a dictionary\"\n",
    "        \n",
    "        print(f\"Retrieved product with ID {sample_id} in {retrieval_time:.4f} seconds:\")\n",
    "        print(f\"Title: {product.get('title', 'No title')}\")\n",
    "        print(f\"Price: ${product.get('price', 0):.2f}\")\n",
    "        print(f\"Rating: {product.get('average_rating', 0):.1f}\")\n",
    "    else:\n",
    "        print(\"Skipping product retrieval test - no product IDs available\")\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run all tests in sequence.\"\"\"\n",
    "    print(\"===== Running all ProductVectorStore tests =====\")\n",
    "    \n",
    "    # Test initialization\n",
    "    vector_store = test_init()\n",
    "    \n",
    "    # Test loading data\n",
    "    test_load_data(vector_store)\n",
    "    \n",
    "    # Test preprocessing\n",
    "    test_preprocess_data(vector_store)\n",
    "    \n",
    "    # Test embedding creation\n",
    "    test_create_embeddings(vector_store)\n",
    "    \n",
    "    # Test index building\n",
    "    test_build_index(vector_store)\n",
    "    \n",
    "    # Test saving and loading\n",
    "    loaded_vector_store = test_save_and_load_index(vector_store)\n",
    "    \n",
    "    # Test search functionality\n",
    "    test_search(loaded_vector_store)\n",
    "    \n",
    "    # Test category search\n",
    "    test_search_by_category(loaded_vector_store)\n",
    "    \n",
    "    # Test product retrieval\n",
    "    test_get_product_by_id(loaded_vector_store)\n",
    "    \n",
    "    print(\"\\n===== All tests completed successfully! =====\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a05178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaee2180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent with question: 'What is 223 to power 3.14?'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is 223 to power 3.14?</span>                                                                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ LiteLLMModel - huggingface/mistralai/Mistral-7B-Instruct-v0.2 ─────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is 223 to power 3.14?\u001b[0m                                                                                      \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m LiteLLMModel - huggingface/mistralai/Mistral-7B-Instruct-v0.2 \u001b[0m\u001b[38;2;212;183;2m────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">litellm.APIError: HuggingfaceException - Model mistralai/Mistral-7B-Instruct-v0.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> is not supported for provider </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">hf-inference</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating model output:\u001b[0m\n",
       "\u001b[1;31mlitellm.APIError: HuggingfaceException - Model mistralai/Mistral-7B-Instruct-v0.\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;31m is not supported for provider \u001b[0m\n",
       "\u001b[1;31mhf-inference\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.59 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 0.59 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Error in generating model output:\n",
      "litellm.APIError: HuggingfaceException - Model mistralai/Mistral-7B-Instruct-v0.2 is not supported for provider hf-inference\n",
      "Please ensure your HUGGINGFACE_API_KEY is set correctly and the model ID is valid.\n",
      "You might also want to check your internet connection and Hugging Face API status.\n"
     ]
    }
   ],
   "source": [
    "from smolagents import CodeAgent, LiteLLMModel # Removed DuckDuckGoSearchTool as it wasn't used\n",
    "import os\n",
    "\n",
    "# Ensure the HUGGINGFACE_API_KEY environment variable is set.\n",
    "# You can also set it directly here if needed, but environment variables are preferred:\n",
    "# os.environ[\"HUGGINGFACE_API_KEY\"] = \"your_hf_api_token_here\" # Less secure, use with caution\n",
    "\n",
    "# Choose a free model from Hugging Face Inference API\n",
    "# Examples:\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.2\" (good general purpose instruct model)\n",
    "# \"google/gemma-7b-it\" (you might need to agree to terms on Hugging Face model page)\n",
    "# \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# For a very small and fast model (less capable): \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# The \"huggingface/\" prefix tells LiteLLM to use the Hugging Face Inference API.\n",
    "hf_model_id = \"huggingface/mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# Or, for a smaller model if you encounter issues or for faster (but less capable) responses:\n",
    "# hf_model_id = \"huggingface/TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Initialize LiteLLMModel for a Hugging Face model\n",
    "# LiteLLM will use the HUGGINGFACE_API_KEY environment variable automatically.\n",
    "# If you want to pass the key explicitly (e.g., if it's not in the standard env var name LiteLLM expects,\n",
    "# or if smolagents' LiteLLMModel requires it), you can use the api_key parameter.\n",
    "# However, LiteLLM's standard way is to pick up HUGGINGFACE_API_KEY.\n",
    "# The api_base is not needed for Hugging Face Inference API via LiteLLM.\n",
    "model = LiteLLMModel(\n",
    "    model_id=hf_model_id,\n",
    "    api_key='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'\n",
    "    #os.environ.get(\"HUGGINGFACE_API_KEY\") # Explicitly pass the key\n",
    "    # api_base is generally not set for Hugging Face via LiteLLM\n",
    ")\n",
    "\n",
    "# If you are certain HUGGINGFACE_API_KEY is set and LiteLLM will pick it up,\n",
    "# and smolagents.LiteLLMModel doesn't strictly require api_key if LiteLLM handles it,\n",
    "# you might even omit api_key:\n",
    "# model = LiteLLMModel(model_id=hf_model_id)\n",
    "# However, being explicit as above is safer.\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[],  # No tools provided in this example\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Use the agent\n",
    "try:\n",
    "    # Question 1: Calculation\n",
    "    print(f\"Running agent with question: 'What is 223 to power 3.14?'\")\n",
    "    result1 = agent.run(\"What is 223 to power 3.14?\")\n",
    "    print(\"\\nResult for 'What is 223 to power 3.14?':\")\n",
    "    print(result1)\n",
    "\n",
    "    # Question 2: More creative/stochastic task (results will vary)\n",
    "    # print(f\"\\nRunning agent with question: 'Throw two dice and tell me what number you got?'\")\n",
    "    # result2 = agent.run(\"Throw two dice and tell me what number you got?\")\n",
    "    # print(\"\\nResult for 'Throw two dice and tell me what number you got?':\")\n",
    "    # print(result2)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure your HUGGINGFACE_API_KEY is set correctly and the model ID is valid.\")\n",
    "    print(\"You might also want to check your internet connection and Hugging Face API status.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd98ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d351688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Fetch the age of Tom Hanks and multiply it by 10</span>                                                                <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ CohereLiteModel - command ─────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mFetch the age of Tom Hanks and multiply it by 10\u001b[0m                                                                \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m CohereLiteModel - command \u001b[0m\u001b[38;2;212;183;2m────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'CohereLiteModel'</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> object has no attribute </span><span style=\"color: #008000; text-decoration-color: #008000\">'generate'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating model output:\u001b[0m\n",
       "\u001b[32m'CohereLiteModel'\u001b[0m\u001b[1;31m object has no attribute \u001b[0m\u001b[32m'generate'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.00 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 0.00 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AgentGenerationError",
     "evalue": "Error in generating model output:\n'CohereLiteModel' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\smolagents\\agents.py:1333\u001b[39m, in \u001b[36mCodeAgent._step_stream\u001b[39m\u001b[34m(self, memory_step)\u001b[39m\n\u001b[32m   1332\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m     chat_message: ChatMessage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m(\n\u001b[32m   1334\u001b[39m         input_messages,\n\u001b[32m   1335\u001b[39m         stop_sequences=[\u001b[33m\"\u001b[39m\u001b[33m<end_code>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mObservation:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCalling tools:\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1336\u001b[39m         **additional_args,\n\u001b[32m   1337\u001b[39m     )\n\u001b[32m   1338\u001b[39m     memory_step.model_output_message = chat_message\n",
      "\u001b[31mAttributeError\u001b[39m: 'CohereLiteModel' object has no attribute 'generate'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAgentGenerationError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     44\u001b[39m agent = CodeAgent(\n\u001b[32m     45\u001b[39m     tools=[DuckDuckGoSearchTool()],  \u001b[38;5;66;03m# free web lookups\u001b[39;00m\n\u001b[32m     46\u001b[39m     model=model                       \u001b[38;5;66;03m# any callable that returns text\u001b[39;00m\n\u001b[32m     47\u001b[39m )\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# ——— run it! ———\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFetch the age of Tom Hanks and multiply it by 10\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\smolagents\\agents.py:351\u001b[39m, in \u001b[36mMultiStepAgent.run\u001b[39m\u001b[34m(self, task, stream, reset, images, additional_args, max_steps)\u001b[39m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_stream(task=\u001b[38;5;28mself\u001b[39m.task, max_steps=max_steps, images=images)\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Outputs are returned only at the end. We only look at the last step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[-\u001b[32m1\u001b[39m].final_answer\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\smolagents\\agents.py:379\u001b[39m, in \u001b[36mMultiStepAgent._run_stream\u001b[39m\u001b[34m(self, task, max_steps, images)\u001b[39m\n\u001b[32m    376\u001b[39m     final_answer = el\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m AgentGenerationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    378\u001b[39m     \u001b[38;5;66;03m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m AgentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    381\u001b[39m     \u001b[38;5;66;03m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[39;00m\n\u001b[32m    382\u001b[39m     action_step.error = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\smolagents\\agents.py:374\u001b[39m, in \u001b[36mMultiStepAgent._run_stream\u001b[39m\u001b[34m(self, task, max_steps, images)\u001b[39m\n\u001b[32m    370\u001b[39m action_step = ActionStep(\n\u001b[32m    371\u001b[39m     step_number=\u001b[38;5;28mself\u001b[39m.step_number, start_time=step_start_time, observations_images=images\n\u001b[32m    372\u001b[39m )\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\n\u001b[32m    376\u001b[39m     final_answer = el\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\smolagents\\agents.py:397\u001b[39m, in \u001b[36mMultiStepAgent._execute_step\u001b[39m\u001b[34m(self, memory_step)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.log_rule(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.step_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, level=LogLevel.INFO)\n\u001b[32m    396\u001b[39m final_answer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_answer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\smolagents\\agents.py:1354\u001b[39m, in \u001b[36mCodeAgent._step_stream\u001b[39m\u001b[34m(self, memory_step)\u001b[39m\n\u001b[32m   1352\u001b[39m     memory_step.model_output = model_output\n\u001b[32m   1353\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1354\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AgentGenerationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in generating model output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.logger) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1356\u001b[39m \u001b[38;5;66;03m### Parse output ###\u001b[39;00m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mAgentGenerationError\u001b[39m: Error in generating model output:\n'CohereLiteModel' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cohere\n",
    "from smolagents import CodeAgent, DuckDuckGoSearchTool\n",
    "\n",
    "class CohereLiteModel:\n",
    "    \"\"\"\n",
    "    A tiny wrapper around the Cohere SDK exposing\n",
    "    the same __call__(prompt:str)->str interface as LiteLLMModel.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        api_key: str = 'PEy1tsUjWmM66wjbm8SgFAHulAHZOHlLI0kiiIRM',\n",
    "        api_base: str = \"https://api.cohere.com/v1\",\n",
    "        max_tokens: int = 512,\n",
    "        temperature: float = 0.7,\n",
    "    ):\n",
    "        self.client = cohere.Client(api_key or os.environ[\"COHERE_API_KEY\"])\n",
    "        self.model_id = model_id\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, prompt: str) -> str:\n",
    "        # generate() is the basic text-generation endpoint in the Cohere SDK :contentReference[oaicite:0]{index=0}\n",
    "        response = self.client.generate(\n",
    "            model=self.model_id,\n",
    "            prompt=prompt,\n",
    "            max_tokens=self.max_tokens,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        # return the top completion\n",
    "        return response.generations[0].text\n",
    "\n",
    "# ——— instantiate your Cohere-backed model ———\n",
    "model = CohereLiteModel(\n",
    "    model_id=\"command\",            # your chosen Cohere model\n",
    "    api_key=\"PEy1tsUjWmM66wjbm8SgFAHulAHZOHlLI0kiiIRM\",\n",
    "    #os.environ.get(\"COHERE_API_KEY\"),     # or hard-code it here\n",
    "    max_tokens=512,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# ——— build your agent exactly as before ———\n",
    "agent = CodeAgent(\n",
    "    tools=[DuckDuckGoSearchTool()],  # free web lookups\n",
    "    model=model                       # any callable that returns text\n",
    ")\n",
    "\n",
    "# ——— run it! ———\n",
    "result = agent.run(\"Fetch the age of Tom Hanks and multiply it by 10\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fbb6caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import HfApiModel, CodeAgent\n",
    "#from helper import get_huggingface_token\n",
    "\n",
    "model = HfApiModel(\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    provider=\"together\", # Choose a specific inference provider\n",
    "    max_tokens=4096,\n",
    "    temperature=0.1,\n",
    "    token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "975b805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53e7108d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What is 223 to power 3.14?</span>                                                                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-72B-Instruct ────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat is 223 to power 3.14?\u001b[0m                                                                                      \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-72B-Instruct \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">result </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">223</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">**</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">3.14</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(result)</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m223\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m*\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m*\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m3.14\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mresult\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: 23641622.040549703</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mOut - Final answer: 23641622.040549703\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.30 seconds| Input tokens: 2,000 | Output tokens: 55]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.30 seconds| Input tokens: 2,000 | Output tokens: 55]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23641622.040549703\n"
     ]
    }
   ],
   "source": [
    "agent = CodeAgent(\n",
    "    tools =[],\n",
    "    model=model)\n",
    "\n",
    "# Use the agent\n",
    "result = agent.run(\"What is 223 to power 3.14?\")\n",
    "# result = agent.run(\"Throw two dice and tell me what number you got?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bc26a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genailabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
