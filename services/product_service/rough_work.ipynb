{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9e47eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Genai.labs assignment\\assignment\\Ecommerce_Assistant_Challenge 2025\\genailabs\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ae8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_service/vector_store.py\n",
    "# Import necessary libraries here...\n",
    "\n",
    "class ProductVectorStore:\n",
    "    \"\"\"\n",
    "    Vector store for product information retrieval.\n",
    "    Uses embeddings to perform semantic search on product data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the vector store with a model for embeddings.\"\"\"\n",
    "        # Initialize model, index, and other necessary components\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "        self.embeddings = None\n",
    "        self.product_ids = None\n",
    "    \n",
    "    def load_data(self, file_path):\n",
    "        \"\"\"Load product data from CSV file.\"\"\"\n",
    "        \n",
    "        try: \n",
    "            df = pd.read_csv(file_path)\n",
    "            self.product_df = df\n",
    "\n",
    "            print(f\"Loaded {len(df)} products from {file_path}\")\n",
    "\n",
    "            return self.preprocess_data(df)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def preprocess_data(self, df=None):\n",
    "        \"\"\"Preprocess product data for embedding generation.\"\"\"\n",
    "        if df is None: \n",
    "            df = self.product_df\n",
    "\n",
    "        # Create a copy to avoid modifying the original\n",
    "        processed_df = df.copy()\n",
    "        \n",
    "        # Generate product IDs if they don't exist\n",
    "        if \"product_id\" not in processed_df.columns:\n",
    "            processed_df[\"product_id\"] = processed_df.index\n",
    "\n",
    "        # Check for duplicate titles and remove duplicates\n",
    "        duplicate_count = processed_df.duplicated(subset = ['title'], keep = 'first').sum()\n",
    "        if duplicate_count > 0:\n",
    "            print(f\"Found {duplicate_count} duplicate products - keeping only first occurrences\")\n",
    "            processed_df = processed_df.drop_duplicates(subset = ['title'], keep = 'first')\n",
    "        \n",
    "        # Fill missing values in text fields\n",
    "        text_columns = ['title', 'description', 'features', 'categories', 'details']\n",
    "        for col in text_columns:\n",
    "            if col in processed_df.columns:\n",
    "                processed_df[col] = processed_df[col].fillna('')\n",
    "        \n",
    "        # Normalize text - lowercase, remove extra whitespace\n",
    "        for col in text_columns:\n",
    "            if col in processed_df.columns:\n",
    "                processed_df[col] = processed_df[col].str.lower().str.strip()\n",
    "        \n",
    "        # Store processed dataframe\n",
    "        self.product_df = processed_df\n",
    "        self.product_ids = processed_df['product_id'].tolist()\n",
    "\n",
    "        return processed_df\n",
    "\n",
    "    \n",
    "    def create_embeddings(self):\n",
    "        \"\"\"Generate embeddings for product data.\"\"\"\n",
    "\n",
    "        # Ensure we have data to embed\n",
    "        if self.product_df is None or len(self.product_df) == 0:\n",
    "            print(\"No data available for embedding generation\")\n",
    "            return None\n",
    "        \n",
    "        # Get all text columns \n",
    "        text_columns = ['main_category', 'title', 'features', 'description', 'categories', 'details']\n",
    "        \n",
    "        # Create text representations by combining all fields\n",
    "        print(\"Creating text representations for embedding...\")\n",
    "        product_texts = []\n",
    "        \n",
    "        for _, row in self.product_df.iterrows():\n",
    "            # Combine all text fields into one string\n",
    "            text_parts = []\n",
    "            for col in text_columns:\n",
    "                if col in row and not pd.isna(row[col]) and row[col]:\n",
    "                    text_parts.append(f\"{col}: {row[col]}\")\n",
    "            \n",
    "            # Add numeric information\n",
    "            if 'average_rating' in row and not pd.isna(row['average_rating']):\n",
    "                text_parts.append(f\"rating: {row['average_rating']}\")\n",
    "            \n",
    "            if 'price' in row and not pd.isna(row['price']):\n",
    "                text_parts.append(f\"price: {row['price']}\")\n",
    "    \n",
    "            if ('imputed_columns' in row and row['imputed_columns'] != None and row['imputed_columns']!= []):\n",
    "                text_parts.append(f\"imputed_columns: {row['imputed_columns']}\")\n",
    "            \n",
    "            # Combine all parts with spaces\n",
    "            product_text = \" \".join(text_parts)\n",
    "            product_texts.append(product_text)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        print(f\"Generating embeddings for {len(product_texts)} products...\")\n",
    "        try:\n",
    "            embeddings = self.model.encode(product_texts)\n",
    "            self.embeddings = embeddings\n",
    "            \n",
    "            print(f\"Successfully created embeddings of shape {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embeddings: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build search index from embeddings.\"\"\"\n",
    "        # Check if embeddings exist\n",
    "        if self.embeddings is None:\n",
    "            print(\"No embeddings available. Call create_embeddings() first.\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Get embedding dimensions\n",
    "            vector_dimension = self.embeddings.shape[1]\n",
    "            \n",
    "            # Create a new index with the correct dimensions\n",
    "            self.index = faiss.IndexFlatL2(vector_dimension)\n",
    "            \n",
    "            # Add the embeddings to the index\n",
    "            self.index.add(self.embeddings)\n",
    "            \n",
    "            print(f\"Successfully built index with {self.index.ntotal} vectors\")\n",
    "            return self.index\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error building index: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_index(self, index_path, data_path):\n",
    "        \"\"\"\n",
    "        Save the FAISS index and product data to disk.\n",
    "\n",
    "        Args:\n",
    "            index_path: Path to save the FAISS index\n",
    "            data_path: Path to save the product data\n",
    "\n",
    "        Returns:\n",
    "            bool: True if saving was successful, False otherwise\n",
    "        \"\"\"\n",
    "        # Check if index exists\n",
    "        if self.index is None:\n",
    "            print(\"No index available to save. Call build_index() first.\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            # Create directories if they don't exist\n",
    "            os.makedirs(os.path.dirname(index_path), exist_ok=True)\n",
    "            os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "\n",
    "            # Save the FAISS index\n",
    "            faiss.write_index(self.index, index_path)\n",
    "\n",
    "            # Save the embeddings\n",
    "            if self.embeddings is not None:\n",
    "                embeddings_path = os.path.splitext(index_path)[0] + \"_embeddings.npy\"\n",
    "                np.save(embeddings_path, self.embeddings)\n",
    "                print(f\"Embeddings saved to {embeddings_path}\")\n",
    "\n",
    "            # Save the product dataframe\n",
    "            self.product_df.to_pickle(data_path)\n",
    "\n",
    "            print(f\"Index saved to {index_path}\")\n",
    "            print(f\"Product data saved to {data_path}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_index(self, index_path, data_path):\n",
    "        \"\"\"\n",
    "        Load a FAISS index and product data from disk.\n",
    "\n",
    "        Args:\n",
    "            index_path: Path to the saved FAISS index\n",
    "            data_path: Path to the saved product data\n",
    "\n",
    "        Returns:\n",
    "            bool: True if loading was successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the FAISS index\n",
    "            self.index = faiss.read_index(index_path)\n",
    "\n",
    "            # Load the embeddings if available\n",
    "            embeddings_path = os.path.splitext(index_path)[0] + \"_embeddings.npy\"\n",
    "            if os.path.exists(embeddings_path):\n",
    "                self.embeddings = np.load(embeddings_path)\n",
    "                print(f\"Loaded embeddings of shape {self.embeddings.shape}\")\n",
    "\n",
    "            # Load the product dataframe\n",
    "            self.product_df = pd.read_pickle(data_path)\n",
    "\n",
    "            # Recreate product_ids\n",
    "            if 'product_id' in self.product_df.columns:\n",
    "                self.product_ids = self.product_df['product_id'].tolist()\n",
    "            else:\n",
    "                self.product_ids = list(range(len(self.product_df)))\n",
    "\n",
    "            print(f\"Loaded index with {self.index.ntotal} vectors\")\n",
    "            print(f\"Loaded {len(self.product_df)} products\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading index: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def search(self, query, top_k=5):\n",
    "        \"\"\"\n",
    "        Search for products similar to the query.\n",
    "\n",
    "        Args:\n",
    "            query: Text query to search for\n",
    "            top_k: Number of results to return\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing product information\n",
    "        \"\"\"\n",
    "        # Check if index exists\n",
    "        if self.index is None:\n",
    "            print(\"No index available. Call build_index() first.\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # Convert query to embedding\n",
    "            query_embedding = self.model.encode([query])\n",
    "\n",
    "            # Normalize the query embedding for cosine similarity\n",
    "            faiss.normalize_L2(query_embedding)\n",
    "\n",
    "            # Search the index\n",
    "            distances, indices = self.index.search(query_embedding, top_k)\n",
    "\n",
    "            # Fetch the actual product data\n",
    "            results = []\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx < len(self.product_df):\n",
    "                    # Get the product data\n",
    "                    product = self.product_df.iloc[idx].to_dict()\n",
    "\n",
    "                    # Add distance score (lower is better for L2 distance)\n",
    "                    product['search_score'] = float(distances[0][i])\n",
    "\n",
    "                    # Add to results\n",
    "                    results.append(product)\n",
    "\n",
    "            print(f\"Found {len(results)} products matching the query: '{query}'\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching index: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_by_category(self, query, category, top_k=5):\n",
    "        \"\"\"\n",
    "        Search for products within a specific category.\n",
    "        \n",
    "        Args:\n",
    "            query: Text query to search for\n",
    "            category: Category to filter by\n",
    "            top_k: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries containing product information\n",
    "        \"\"\"\n",
    "        # First get more results than we need\n",
    "        results = self.search(query, top_k=top_k*3)\n",
    "        \n",
    "        # Filter by category\n",
    "        filtered_results = []\n",
    "        for product in results:\n",
    "            if 'main_category' in product and product['main_category'] == category:\n",
    "                filtered_results.append(product)\n",
    "\n",
    "        \n",
    "        # Return the top k results\n",
    "        return filtered_results[:top_k]\n",
    "\n",
    "    def get_product_by_id(self, product_id):\n",
    "        \"\"\"\n",
    "        Retrieve product details by ID.\n",
    "\n",
    "        Args:\n",
    "            product_id: ID of the product to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing product information, or None if not found\n",
    "        \"\"\"\n",
    "        # Check if we have product data\n",
    "        if self.product_df is None: \n",
    "            print(\"No product data available.\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Find the product by ID\n",
    "            if 'product_id' in self.product_df.columns:\n",
    "                product = self.product_df[self.product_df['product_id'] == product_id]\n",
    "\n",
    "                # If product found, return it as a dictionary\n",
    "                if not product.empty:\n",
    "                    return product.iloc[0].to_dict()\n",
    "\n",
    "            # If we reach here, the product wasn't found\n",
    "            print(f\"Product with ID {product_id} not found.\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving product: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "695ebf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:25: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:25: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\goldr\\AppData\\Local\\Temp\\ipykernel_24032\\4250044334.py:25: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  data_path = \"D:/Genai.labs assignment/assignment\\Ecommerce_Assistant_Challenge 2025/new_data/Product_Information_Dataset.csv\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Running all ProductVectorStore tests =====\n",
      "\n",
      "===== Testing Initialization =====\n",
      "Model name: all-MiniLM-L6-v2\n",
      "Model loaded: True\n",
      "\n",
      "===== Testing load_data() =====\n",
      "Loaded 5000 products from D:/Genai.labs assignment/assignment\\Ecommerce_Assistant_Challenge 2025/new_data/Product_Information_Dataset.csv\n",
      "Found 169 duplicate products - keeping only first occurrences\n",
      "Loaded 4831 products in 0.60 seconds\n",
      "DataFrame columns: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'store', 'categories', 'details', 'parent_asin', 'imputed_columns', 'product_id']\n",
      "\n",
      "===== Testing preprocess_data() =====\n",
      "Found 1 duplicate products - keeping only first occurrences\n",
      "Removed 1 duplicates\n",
      "Preprocessed 4830 products in 0.06 seconds\n",
      "\n",
      "===== Testing create_embeddings() =====\n",
      "Creating text representations for embedding...\n",
      "Generating embeddings for 4830 products...\n",
      "Successfully created embeddings of shape (4830, 384)\n",
      "Created embeddings of shape (4830, 384) in 246.58 seconds\n",
      "\n",
      "===== Testing build_index() =====\n",
      "Successfully built index with 4830 vectors\n",
      "Built index with 4830 vectors in 0.00 seconds\n",
      "\n",
      "===== Testing save_index() and load_index() =====\n",
      "Embeddings saved to services/product_service/test_index/test_product_index_embeddings.npy\n",
      "Index saved to services/product_service/test_index/test_product_index.bin\n",
      "Product data saved to services/product_service/test_index/test_product_data.pkl\n",
      "Saved index and data in 0.09 seconds\n",
      "Loaded embeddings of shape (4830, 384)\n",
      "Loaded index with 4830 vectors\n",
      "Loaded 4830 products\n",
      "Loaded index and data in 0.06 seconds\n",
      "\n",
      "===== Testing search() =====\n",
      "Found 3 products matching the query: 'acoustic guitar'\n",
      "\n",
      "Query: 'acoustic guitar'\n",
      "Found 3 results in 0.0303 seconds:\n",
      "1. martin guitar authentic acoustic lifespan 2.0 ma150t, 80/20 bronze, treated medium-gauge acoustic strings (Score: 0.6055)\n",
      "2. first act discovery 30\" beginner acoustic guitar, sunburst (Score: 0.6304)\n",
      "3. best choice products beginner acoustic electric guitar starter set 38in w/all wood cutaway design, case, strap, picks, tuner - blue (Score: 0.6324)\n",
      "Found 3 products matching the query: 'high quality microphone'\n",
      "\n",
      "Query: 'high quality microphone'\n",
      "Found 3 results in 0.0153 seconds:\n",
      "1. gls audio instrument microphone es-57 & mic clip - professional series es57 dynamic cardioid mike unidirectional - for instruments, drums, percussion, vocals, and more (Score: 0.7436)\n",
      "2. professional wired lavalier lapel clip on microphone for iphone and android smartphone or camera omnidirectional tiny shirt mic for recording with clip-on perfect for vloggers and bloggers (Score: 0.7540)\n",
      "3. 1mii long range wireless microphone,wireless headset mic system, 165ft range, 2.4g wireless microphone 2 in 1, fitness microphone headset for speakers, voice amplifier, pa speakers (Score: 0.7617)\n",
      "Found 3 products matching the query: 'music equipment with good ratings'\n",
      "\n",
      "Query: 'music equipment with good ratings'\n",
      "Found 3 results in 0.0178 seconds:\n",
      "1. adj products db display mkii dj mixer (Score: 0.8140)\n",
      "2. ltgem case for pioneer dj ddj-400 / ddj-sb3 (ddj-sb3-n) / ddj-sb2 or portable 2-channel controller or ddj-rb performance dj controller-black (Score: 0.8336)\n",
      "3. karaoke machine, wireless rechargeable portable pa system/speaker with 2 metal wireless mics, b fm radio, led party light, remote control, built in handel, 800w peak (2 mics) (Score: 0.8369)\n",
      "\n",
      "===== Testing search_by_category() =====\n",
      "Found 9 products matching the query: 'high quality'\n",
      "Query: 'high quality' in category 'Musical Instruments'\n",
      "Found 3 results in 0.0238 seconds:\n",
      "1. behringer micromix mx400 ultra low-noise 4-channel line mixer, black (Score: 1.2871)\n",
      "2. rosefinch acoustic guitar 38 inch 3/4 size cutaway basswood guitar for beginner adults childs starter bundle kit w/bag,strap, picks,extra steel strings, tuner,capo,strings winder(white 38 inch) (Score: 1.3003)\n",
      "3. shure super 55 deluxe vocal microphone - vintage supercardioid dynamic unidyne mic, iconic look, classic sound - rugged die-cast casing, includes 5/8\" to 3/8\" thread adapter and zippered, padded pouch (Score: 1.3012)\n",
      "\n",
      "===== Testing get_product_by_id() =====\n",
      "Retrieved product with ID 0 in 0.0011 seconds:\n",
      "Title: ernie ball mondo slinky nickelwound electric guitar strings 10.5-52 gauge\n",
      "Price: $6.99\n",
      "Rating: 4.8\n",
      "\n",
      "===== All tests completed successfully! =====\n"
     ]
    }
   ],
   "source": [
    "# test_vector_store.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "# Import the ProductVectorStore class\n",
    "# from vector_store import ProductVectorStore\n",
    "\n",
    "def test_init():\n",
    "    \"\"\"Test initialization of the vector store.\"\"\"\n",
    "    print(\"\\n===== Testing Initialization =====\")\n",
    "    vector_store = ProductVectorStore()\n",
    "    print(f\"Model name: {vector_store.model_name}\")\n",
    "    print(f\"Model loaded: {vector_store.model is not None}\")\n",
    "    assert vector_store.model is not None, \"Model should be initialized\"\n",
    "    return vector_store\n",
    "\n",
    "def test_load_data(vector_store):\n",
    "    \"\"\"Test loading data from CSV.\"\"\"\n",
    "    print(\"\\n===== Testing load_data() =====\")\n",
    "    # Define the path to your CSV file\n",
    "    data_path = \"D:/Genai.labs assignment/assignment\\Ecommerce_Assistant_Challenge 2025/new_data/Product_Information_Dataset.csv\"\n",
    "    \n",
    "    # Ensure file exists\n",
    "    assert os.path.exists(data_path), f\"Data file not found at {data_path}\"\n",
    "    \n",
    "    # Load the data\n",
    "    start_time = time.time()\n",
    "    df = vector_store.load_data(data_path)\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    # Check if data loaded successfully\n",
    "    assert df is not None, \"DataFrame should not be None\"\n",
    "    assert len(df) > 0, \"DataFrame should not be empty\"\n",
    "    \n",
    "    print(f\"Loaded {len(df)} products in {load_time:.2f} seconds\")\n",
    "    print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Check if product_id was created\n",
    "    assert 'product_id' in df.columns, \"product_id column should exist\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "def test_preprocess_data(vector_store):\n",
    "    \"\"\"Test preprocessing of product data.\"\"\"\n",
    "    print(\"\\n===== Testing preprocess_data() =====\")\n",
    "    \n",
    "    # Get the raw dataframe size\n",
    "    raw_df_size = len(vector_store.product_df)\n",
    "    \n",
    "    # Process the data\n",
    "    start_time = time.time()\n",
    "    processed_df = vector_store.preprocess_data()\n",
    "    process_time = time.time() - start_time\n",
    "    \n",
    "    # Check if processing worked\n",
    "    assert processed_df is not None, \"Processed DataFrame should not be None\"\n",
    "    \n",
    "    # Check for duplicates removal\n",
    "    if raw_df_size > len(processed_df):\n",
    "        print(f\"Removed {raw_df_size - len(processed_df)} duplicates\")\n",
    "    \n",
    "    # Check text normalization (sample a few rows)\n",
    "    if len(processed_df) > 0:\n",
    "        sample_row = processed_df.iloc[0]\n",
    "        if 'title' in sample_row:\n",
    "            title = sample_row['title']\n",
    "            assert title.lower() == title, \"Title should be lowercase\"\n",
    "            assert title == title.strip(), \"Title should be stripped of whitespace\"\n",
    "    \n",
    "    print(f\"Preprocessed {len(processed_df)} products in {process_time:.2f} seconds\")\n",
    "    return processed_df\n",
    "\n",
    "def test_create_embeddings(vector_store):\n",
    "    \"\"\"Test creation of embeddings.\"\"\"\n",
    "    print(\"\\n===== Testing create_embeddings() =====\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    start_time = time.time()\n",
    "    embeddings = vector_store.create_embeddings()\n",
    "    embed_time = time.time() - start_time\n",
    "    \n",
    "    # Check if embeddings were created\n",
    "    assert embeddings is not None, \"Embeddings should not be None\"\n",
    "    assert vector_store.embeddings is not None, \"Embeddings should be stored in vector_store\"\n",
    "    \n",
    "    # Check shape of embeddings\n",
    "    num_products = len(vector_store.product_df)\n",
    "    embed_dim = vector_store.model.get_sentence_embedding_dimension()\n",
    "    assert embeddings.shape == (num_products, embed_dim), f\"Embeddings shape should be ({num_products}, {embed_dim})\"\n",
    "    \n",
    "    print(f\"Created embeddings of shape {embeddings.shape} in {embed_time:.2f} seconds\")\n",
    "    return embeddings\n",
    "\n",
    "def test_build_index(vector_store):\n",
    "    \"\"\"Test building the FAISS index.\"\"\"\n",
    "    print(\"\\n===== Testing build_index() =====\")\n",
    "    \n",
    "    # Build index\n",
    "    start_time = time.time()\n",
    "    index = vector_store.build_index()\n",
    "    build_time = time.time() - start_time\n",
    "    \n",
    "    # Check if index was built\n",
    "    assert index is not None, \"Index should not be None\"\n",
    "    assert hasattr(index, 'ntotal'), \"Index should be a FAISS index\"\n",
    "    assert index.ntotal == len(vector_store.embeddings), \"Index should contain all embeddings\"\n",
    "    \n",
    "    print(f\"Built index with {index.ntotal} vectors in {build_time:.2f} seconds\")\n",
    "    return index\n",
    "\n",
    "def test_save_and_load_index(vector_store):\n",
    "    \"\"\"Test saving and loading the index.\"\"\"\n",
    "    print(\"\\n===== Testing save_index() and load_index() =====\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(\"services/product_service/test_index\", exist_ok=True)\n",
    "    \n",
    "    # Define paths\n",
    "    index_path = \"services/product_service/test_index/test_product_index.bin\"\n",
    "    data_path = \"services/product_service/test_index/test_product_data.pkl\"\n",
    "    \n",
    "    # Save index\n",
    "    start_time = time.time()\n",
    "    save_result = vector_store.save_index(index_path, data_path)\n",
    "    save_time = time.time() - start_time\n",
    "    \n",
    "    # Check if save was successful\n",
    "    assert save_result, \"Save operation should return True\"\n",
    "    assert os.path.exists(index_path), f\"Index file should exist at {index_path}\"\n",
    "    assert os.path.exists(data_path), f\"Data file should exist at {data_path}\"\n",
    "    \n",
    "    print(f\"Saved index and data in {save_time:.2f} seconds\")\n",
    "    \n",
    "    # Create a new vector store\n",
    "    new_vector_store = ProductVectorStore()\n",
    "    \n",
    "    # Load index\n",
    "    start_time = time.time()\n",
    "    load_result = new_vector_store.load_index(index_path, data_path)\n",
    "    load_time = time.time() - start_time\n",
    "    \n",
    "    # Check if load was successful\n",
    "    assert load_result, \"Load operation should return True\"\n",
    "    assert new_vector_store.index is not None, \"Index should not be None after loading\"\n",
    "    assert new_vector_store.product_df is not None, \"Product DataFrame should not be None after loading\"\n",
    "    assert len(new_vector_store.product_df) == len(vector_store.product_df), \"Loaded DataFrame should have same size\"\n",
    "    \n",
    "    print(f\"Loaded index and data in {load_time:.2f} seconds\")\n",
    "    return new_vector_store\n",
    "\n",
    "def test_search(vector_store):\n",
    "    \"\"\"Test search functionality.\"\"\"\n",
    "    print(\"\\n===== Testing search() =====\")\n",
    "    \n",
    "    # Define test queries\n",
    "    test_queries = [\n",
    "        \"acoustic guitar\",\n",
    "        \"high quality microphone\",\n",
    "        \"music equipment with good ratings\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        # Search for products\n",
    "        start_time = time.time()\n",
    "        results = vector_store.search(query, top_k=3)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        # Check if search returned results\n",
    "        assert isinstance(results, list), \"Search results should be a list\"\n",
    "        assert len(results) <= 3, \"Search should return at most top_k results\"\n",
    "        \n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"Found {len(results)} results in {search_time:.4f} seconds:\")\n",
    "        \n",
    "        # Display results\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"{i+1}. {result.get('title', 'No title')} (Score: {result.get('search_score', 0):.4f})\")\n",
    "\n",
    "def test_search_by_category(vector_store):\n",
    "    \"\"\"Test search by category functionality.\"\"\"\n",
    "    print(\"\\n===== Testing search_by_category() =====\")\n",
    "    \n",
    "    # Get a sample category from the data\n",
    "    if 'main_category' in vector_store.product_df.columns:\n",
    "        sample_category = vector_store.product_df['main_category'].iloc[0]\n",
    "        \n",
    "        # Search by category\n",
    "        query = \"high quality\"\n",
    "        start_time = time.time()\n",
    "        results = vector_store.search_by_category(query, sample_category, top_k=3)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        # Check if search returned results\n",
    "        assert isinstance(results, list), \"Search results should be a list\"\n",
    "        \n",
    "        print(f\"Query: '{query}' in category '{sample_category}'\")\n",
    "        print(f\"Found {len(results)} results in {search_time:.4f} seconds:\")\n",
    "        \n",
    "        # Display results\n",
    "        for i, result in enumerate(results):\n",
    "            if len(results) > 0:\n",
    "                print(f\"{i+1}. {result.get('title', 'No title')} (Score: {result.get('search_score', 0):.4f})\")\n",
    "    else:\n",
    "        print(\"Skipping category search test - no main_category column in data\")\n",
    "\n",
    "def test_get_product_by_id(vector_store):\n",
    "    \"\"\"Test retrieving a product by ID.\"\"\"\n",
    "    print(\"\\n===== Testing get_product_by_id() =====\")\n",
    "    \n",
    "    # Get a sample product ID\n",
    "    if len(vector_store.product_ids) > 0:\n",
    "        sample_id = vector_store.product_ids[0]\n",
    "        \n",
    "        # Get product by ID\n",
    "        start_time = time.time()\n",
    "        product = vector_store.get_product_by_id(sample_id)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        # Check if product was retrieved\n",
    "        assert product is not None, f\"Product with ID {sample_id} should be found\"\n",
    "        assert isinstance(product, dict), \"Retrieved product should be a dictionary\"\n",
    "        \n",
    "        print(f\"Retrieved product with ID {sample_id} in {retrieval_time:.4f} seconds:\")\n",
    "        print(f\"Title: {product.get('title', 'No title')}\")\n",
    "        print(f\"Price: ${product.get('price', 0):.2f}\")\n",
    "        print(f\"Rating: {product.get('average_rating', 0):.1f}\")\n",
    "    else:\n",
    "        print(\"Skipping product retrieval test - no product IDs available\")\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run all tests in sequence.\"\"\"\n",
    "    print(\"===== Running all ProductVectorStore tests =====\")\n",
    "    \n",
    "    # Test initialization\n",
    "    vector_store = test_init()\n",
    "    \n",
    "    # Test loading data\n",
    "    test_load_data(vector_store)\n",
    "    \n",
    "    # Test preprocessing\n",
    "    test_preprocess_data(vector_store)\n",
    "    \n",
    "    # Test embedding creation\n",
    "    test_create_embeddings(vector_store)\n",
    "    \n",
    "    # Test index building\n",
    "    test_build_index(vector_store)\n",
    "    \n",
    "    # Test saving and loading\n",
    "    loaded_vector_store = test_save_and_load_index(vector_store)\n",
    "    \n",
    "    # Test search functionality\n",
    "    test_search(loaded_vector_store)\n",
    "    \n",
    "    # Test category search\n",
    "    test_search_by_category(loaded_vector_store)\n",
    "    \n",
    "    # Test product retrieval\n",
    "    test_get_product_by_id(loaded_vector_store)\n",
    "    \n",
    "    print(\"\\n===== All tests completed successfully! =====\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a05178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee2180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genailabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
